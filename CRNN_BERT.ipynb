{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet.gluon.rnn.rnn_layer import LSTM\n",
    "\n",
    "\n",
    "class CRNN_mxnet:\n",
    "    def __init__(self, num_classes, dropout=0., rnn_hidden_size=100,\n",
    "                 inference=True, img_width=560, num_label=20):\n",
    "        '''\n",
    "        same as conv-lite-lstm in CnOcr\n",
    "        :param inference: boolean\n",
    "            indicates evaluation without training\n",
    "        '''\n",
    "        # 560 => 140 - 1 = 139\n",
    "        seq_len_cmpr_ratio = 4\n",
    "        self.seq_len = img_width // seq_len_cmpr_ratio - 1\n",
    "        self.dropout = dropout\n",
    "        self.inference = inference\n",
    "        self.num_classes = num_classes\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "\n",
    "    def convRelu(self, idx, input_data, kernel_size, layer_size, padding_size,\n",
    "                 batch_norm=True):\n",
    "        layer = mx.symbol.Convolution(\n",
    "            name='conv-%d' % idx,\n",
    "            data=input_data,\n",
    "            kernel=kernel_size,\n",
    "            pad=padding_size,\n",
    "            num_filter=layer_size,\n",
    "        )\n",
    "        if batch_norm:\n",
    "            layer = mx.sym.BatchNorm(data=layer, name='batchnorm-%d' % idx)\n",
    "        layer = mx.sym.LeakyReLU(data=layer, name='leakyrelu-%d' % idx)\n",
    "        return layer\n",
    "\n",
    "    def bottle_conv(self, idx, input_data, kernel_size, layer_size, padding_size,\n",
    "                    batch_norm=True):\n",
    "        bottle_channel = layer_size // 2\n",
    "        layer = mx.symbol.Convolution(\n",
    "            name='conv-%d-1-1x1' % idx,\n",
    "            data=input_data,\n",
    "            kernel=(1, 1),\n",
    "            pad=(0, 0),\n",
    "            num_filter=bottle_channel,\n",
    "        )\n",
    "        layer = mx.sym.LeakyReLU(data=layer, name='leakyrelu-%d-1' % idx)\n",
    "        layer = mx.symbol.Convolution(\n",
    "            name='conv-%d' % idx,\n",
    "            data=layer,\n",
    "            kernel=kernel_size,\n",
    "            pad=padding_size,\n",
    "            num_filter=bottle_channel,\n",
    "        )\n",
    "        layer = mx.sym.LeakyReLU(data=layer, name='leakyrelu-%d-2' % idx)\n",
    "        layer = mx.symbol.Convolution(\n",
    "            name='conv-%d-2-1x1' % idx,\n",
    "            data=layer,\n",
    "            kernel=(1, 1),\n",
    "            pad=(0, 0),\n",
    "            num_filter=layer_size,\n",
    "        )\n",
    "        if batch_norm:\n",
    "            layer = mx.sym.BatchNorm(data=layer, name='batchnorm-%d' % idx)\n",
    "        layer = mx.sym.LeakyReLU(data=layer, name='leakyrelu-%d' % idx)\n",
    "        return layer\n",
    "\n",
    "    def gen_network(self, data):\n",
    "        kernel_size = [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)]\n",
    "        padding_size = [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n",
    "        layer_size = [min(32 * 2 ** (i + 1), 512) for i in range(len(kernel_size))]\n",
    "\n",
    "        net = self.convRelu(\n",
    "            0, data, kernel_size[0], layer_size[0], padding_size[0]\n",
    "        )\n",
    "        net = self.convRelu(\n",
    "            1, net, kernel_size[1], layer_size[1], padding_size[1], True\n",
    "        )\n",
    "        net = mx.sym.Pooling(\n",
    "            data=net, name='pool-0', pool_type='max', kernel=(2, 2), stride=(2, 2)\n",
    "        )\n",
    "        net = self.convRelu(\n",
    "            2, net, kernel_size[2], layer_size[2], padding_size[2]\n",
    "        )\n",
    "        net = self.convRelu(\n",
    "            3, net, kernel_size[3], layer_size[3], padding_size[3], True\n",
    "        )\n",
    "        x = net = mx.sym.Pooling(\n",
    "            data=net, name='pool-1', pool_type='max', kernel=(2, 2), stride=(2, 2)\n",
    "        )\n",
    "        net = self.bottle_conv(4, net, kernel_size[4], layer_size[4], padding_size[4])\n",
    "        net = self.bottle_conv(5, net, kernel_size[5], layer_size[5], padding_size[5], True) + x\n",
    "        net = mx.symbol.Pooling(\n",
    "            data=net, name='pool-2', pool_type='max', kernel=(2, 2), stride=(2, 1)\n",
    "        )\n",
    "        net = self.bottle_conv(6, net, (4, 1), layer_size[5], (0, 0))\n",
    "        if self.dropout > 0.:\n",
    "            net = mx.symbol.Dropout(data=net, p=self.dropout)\n",
    "\n",
    "        # res: bz x emb_size x seq_len\n",
    "        net = mx.symbol.squeeze(net, axis=2)\n",
    "        net = mx.symbol.transpose(net, axes=(2, 0, 1))\n",
    "        seq_model = LSTM(self.rnn_hidden_size, 2, bidirectional=True)\n",
    "        hidden_concat = seq_model(net)\n",
    "        return hidden_concat\n",
    "\n",
    "    def get_network(self, data=None):\n",
    "        # placeholder of input data\n",
    "        self.data = mx.sym.Variable('data')\n",
    "        # Note that the name of label is `label` instead of the \\\n",
    "        # default `softmax_label` in mxnet\n",
    "        self.label = mx.sym.Variable('label')\n",
    "        output = self.gen_network(self.data)\n",
    "        # => (batch_size, seq_len=139, hidden_size=200)\n",
    "        output = mx.symbol.transpose(output, axes=(1, 0, 2))\n",
    "        # => (seq_len * batch_size, rnn_hidden_size)\n",
    "        output_reshape = mx.symbol.reshape(output, shape=(-3, -2))\n",
    "        # => ((seq_len * batch_size), num_classes)\n",
    "        pred = mx.sym.FullyConnected(data=output_reshape,\n",
    "                                     num_hidden=self.num_classes,\n",
    "                                     name='pred_fc')\n",
    "        if self.inference:\n",
    "            return mx.sym.Group([output, mx.sym.softmax(data=pred, name='softmax')])\n",
    "        else:\n",
    "            # training with CTC loss\n",
    "            # => (seq_len, batch_size, num_classes)\n",
    "            pred_ctc = mx.sym.Reshape(data=pred, shape=(-4, self.seq_len, -1, 0))\n",
    "            loss = mx.sym.contrib.ctc_loss(data=pred_ctc, label=self.label)\n",
    "            ctc_loss = mx.sym.MakeLoss(loss)\n",
    "            softmax_class = mx.symbol.SoftmaxActivation(data=pred)\n",
    "            softmax_loss = mx.sym.MakeLoss(softmax_class)\n",
    "            softmax_loss = mx.sym.BlockGrad(softmax_loss)\n",
    "            return mx.sym.Group([softmax_loss, ctc_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.onnx import operators\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from numpy.random import uniform\n",
    "from transformers import BertTokenizer, AlbertForMaskedLM\n",
    "\n",
    "\n",
    "# Helper funcs\n",
    "INCREMENTAL_STATE_INSTANCE_ID = defaultdict(lambda: 0)\n",
    "\n",
    "\n",
    "def fill_with_neg_inf(t):\n",
    "    \"\"\"FP16-compatible function that fills a tensor with -inf.\"\"\"\n",
    "    return t.float().fill_(float('-inf')).type_as(t)\n",
    "\n",
    "\n",
    "def _get_full_incremental_state_key(module_instance, key):\n",
    "    module_name = module_instance.__class__.__name__\n",
    "\n",
    "    # assign a unique ID to each module instance, so that incremental state is\n",
    "    # not shared across module instances\n",
    "    if not hasattr(module_instance, '_fairseq_instance_id'):\n",
    "        INCREMENTAL_STATE_INSTANCE_ID[module_name] += 1\n",
    "        module_instance._fairseq_instance_id = INCREMENTAL_STATE_INSTANCE_ID[module_name]\n",
    "\n",
    "    return '{}.{}.{}'.format(module_name, module_instance._fairseq_instance_id, key)\n",
    "\n",
    "\n",
    "def softmax(x, dim, onnx_trace=False):\n",
    "    if onnx_trace:\n",
    "        return F.softmax(x.float(), dim=dim)\n",
    "    else:\n",
    "        # noinspection PyTypeChecker\n",
    "        return F.softmax(x, dim=dim, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def get_incremental_state(module, incremental_state, key):\n",
    "    \"\"\"Helper for getting incremental state for an nn.Module.\"\"\"\n",
    "    full_key = _get_full_incremental_state_key(module, key)\n",
    "    if incremental_state is None or full_key not in incremental_state:\n",
    "        return None\n",
    "    return incremental_state[full_key]\n",
    "\n",
    "\n",
    "def set_incremental_state(module, incremental_state, key, value):\n",
    "    \"\"\"Helper for setting incremental state for an nn.Module.\"\"\"\n",
    "    if incremental_state is not None:\n",
    "        full_key = _get_full_incremental_state_key(module, key)\n",
    "        incremental_state[full_key] = value\n",
    "\n",
    "\n",
    "def make_positions(tensor, padding_idx, onnx_trace=False):\n",
    "    \"\"\"Replace non-padding symbols with their position numbers.\n",
    "\n",
    "    Position numbers begin at padding_idx+1. Padding symbols are ignored.\n",
    "    \"\"\"\n",
    "    mask = tensor.ne(padding_idx).long()\n",
    "    return torch.cumsum(mask, dim=1) * mask + padding_idx\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention.\n",
    "\n",
    "    See \"Attention Is All You Need\" for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, kdim=None, vdim=None, dropout=0., bias=True,\n",
    "                 add_bias_kv=False, add_zero_attn=False, self_attention=False,\n",
    "                 encoder_decoder_attention=False):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "\n",
    "        self.self_attention = self_attention\n",
    "        self.encoder_decoder_attention = encoder_decoder_attention\n",
    "\n",
    "        assert not self.self_attention or self.qkv_same_dim, 'Self-attention requires query, key and '                                                              'value to be of the same size'\n",
    "\n",
    "\n",
    "        if self.qkv_same_dim:\n",
    "            self.in_proj_weight = nn.Parameter(torch.Tensor(3 * embed_dim, embed_dim))\n",
    "        else:\n",
    "            self.k_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.kdim))\n",
    "            self.v_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.vdim))\n",
    "            self.q_proj_weight = nn.Parameter(torch.Tensor(embed_dim, embed_dim))\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = nn.Parameter(torch.Tensor(3 * embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = nn.Parameter(torch.Tensor(1, 1, embed_dim))\n",
    "            self.bias_v = nn.Parameter(torch.Tensor(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "        self.onnx_trace = False\n",
    "\n",
    "    def prepare_for_onnx_export_(self):\n",
    "        self.onnx_trace = True\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.qkv_same_dim:\n",
    "            nn.init.xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.k_proj_weight)\n",
    "            nn.init.xavier_uniform_(self.v_proj_weight)\n",
    "            nn.init.xavier_uniform_(self.q_proj_weight)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "        if self.in_proj_bias is not None:\n",
    "            nn.init.constant_(self.in_proj_bias, 0.)\n",
    "            nn.init.constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            nn.init.xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            nn.init.xavier_normal_(self.bias_v)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, incremental_state=None,\n",
    "                need_weights=True, static_kv=False, attn_mask=None):\n",
    "        \"\"\"Input shape: Time x Batch x Channel\n",
    "\n",
    "        Timesteps can be masked by supplying a T x T mask in the\n",
    "        `attn_mask` argument. Padding elements can be excluded from\n",
    "        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n",
    "        batch x src_len, where padding elements are indicated by 1s.\n",
    "        \"\"\"\n",
    "\n",
    "        tgt_len, bsz, embed_dim = query.size()\n",
    "        assert embed_dim == self.embed_dim\n",
    "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
    "\n",
    "        if incremental_state is not None:\n",
    "            saved_state = self._get_input_buffer(incremental_state)\n",
    "            if 'prev_key' in saved_state:\n",
    "                # previous time steps are cached - no need to recompute\n",
    "                # key and value if they are static\n",
    "                if static_kv:\n",
    "                    assert self.encoder_decoder_attention and not self.self_attention\n",
    "                    key = value = None\n",
    "        else:\n",
    "            saved_state = None\n",
    "\n",
    "        if self.self_attention:\n",
    "            # self-attention\n",
    "            q, k, v = self.in_proj_qkv(query)\n",
    "        elif self.encoder_decoder_attention:\n",
    "            # encoder-decoder attention\n",
    "            q = self.in_proj_q(query)\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = v = None\n",
    "            else:\n",
    "                k = self.in_proj_k(key)\n",
    "                v = self.in_proj_v(key)\n",
    "\n",
    "        else:\n",
    "            q = self.in_proj_q(query)\n",
    "            k = self.in_proj_k(key)\n",
    "            v = self.in_proj_v(value)\n",
    "        q *= self.scaling\n",
    "\n",
    "        if self.bias_k is not None:\n",
    "            assert self.bias_v is not None\n",
    "            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = torch.cat(\n",
    "                    [key_padding_mask, key_padding_mask.new_zeros(key_padding_mask.size(0), 1)],\n",
    "                    dim=1)\n",
    "\n",
    "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        if k is not None:\n",
    "            k = k.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        if v is not None:\n",
    "            v = v.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        if saved_state is not None:\n",
    "            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
    "            if 'prev_key' in saved_state:\n",
    "                prev_key = saved_state['prev_key'].view(bsz * self.num_heads, -1, self.head_dim)\n",
    "                if static_kv:\n",
    "                    k = prev_key\n",
    "                else:\n",
    "                    k = torch.cat((prev_key, k), dim=1)\n",
    "            if 'prev_value' in saved_state:\n",
    "                prev_value = saved_state['prev_value'].view(bsz * self.num_heads, -1, self.head_dim)\n",
    "                if static_kv:\n",
    "                    v = prev_value\n",
    "                else:\n",
    "                    v = torch.cat((prev_value, v), dim=1)\n",
    "            saved_state['prev_key'] = k.view(bsz, self.num_heads, -1, self.head_dim)\n",
    "            saved_state['prev_value'] = v.view(bsz, self.num_heads, -1, self.head_dim)\n",
    "\n",
    "            self._set_input_buffer(incremental_state, saved_state)\n",
    "\n",
    "        src_len = k.size(1)\n",
    "\n",
    "        # This is part of a workaround to get around fork/join parallelism\n",
    "        # not supporting Optional types.\n",
    "        if key_padding_mask is not None and key_padding_mask.shape == torch.Size([]):\n",
    "            key_padding_mask = None\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.size(0) == bsz\n",
    "            assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "        if self.add_zero_attn:\n",
    "            src_len += 1\n",
    "            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n",
    "            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = torch.cat(\n",
    "                    [key_padding_mask, torch.zeros(key_padding_mask.size(0), 1).type_as(key_padding_mask)],\n",
    "                    dim=1)\n",
    "\n",
    "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "            if self.onnx_trace:\n",
    "                attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)\n",
    "            attn_weights += attn_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            # don't attend to padding symbols\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            if self.onnx_trace:\n",
    "                attn_weights = torch.where(\n",
    "                    key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                    torch.Tensor([float(\"-Inf\")]),\n",
    "                    attn_weights.float()\n",
    "                ).type_as(attn_weights)\n",
    "            else:\n",
    "                attn_weights = attn_weights.masked_fill(\n",
    "                    key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                    float('-inf'),\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = softmax(\n",
    "            attn_weights, dim=-1, onnx_trace=self.onnx_trace,\n",
    "        ).type_as(attn_weights)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn = torch.bmm(attn_weights, v)\n",
    "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
    "        if (self.onnx_trace and attn.size(1) == 1):\n",
    "            # when ONNX tracing a single decoder step (sequence length == 1)\n",
    "            # the transpose is a no-op copy before view, thus unnecessary\n",
    "            attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n",
    "        else:\n",
    "            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "        attn = self.out_proj(attn)\n",
    "\n",
    "        if need_weights:\n",
    "            # average attention weights over heads\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights.sum(dim=1) / self.num_heads\n",
    "        else:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn, attn_weights\n",
    "\n",
    "    def in_proj_qkv(self, query):\n",
    "        return self._in_proj(query).chunk(3, dim=-1)\n",
    "\n",
    "    def in_proj_q(self, query):\n",
    "        if self.qkv_same_dim:\n",
    "            return self._in_proj(query, end=self.embed_dim)\n",
    "        else:\n",
    "            bias = self.in_proj_bias\n",
    "            if bias is not None:\n",
    "                bias = bias[:self.embed_dim]\n",
    "            return F.linear(query, self.q_proj_weight, bias)\n",
    "\n",
    "    def in_proj_k(self, key):\n",
    "        if self.qkv_same_dim:\n",
    "            return self._in_proj(key, start=self.embed_dim, end=2 * self.embed_dim)\n",
    "        else:\n",
    "            weight = self.k_proj_weight\n",
    "            bias = self.in_proj_bias\n",
    "            if bias is not None:\n",
    "                bias = bias[self.embed_dim:2 * self.embed_dim]\n",
    "            return F.linear(key, weight, bias)\n",
    "\n",
    "    def in_proj_v(self, value):\n",
    "        if self.qkv_same_dim:\n",
    "            return self._in_proj(value, start=2 * self.embed_dim)\n",
    "        else:\n",
    "            weight = self.v_proj_weight\n",
    "            bias = self.in_proj_bias\n",
    "            if bias is not None:\n",
    "                bias = bias[2 * self.embed_dim:]\n",
    "            return F.linear(value, weight, bias)\n",
    "\n",
    "    def _in_proj(self, input, start=0, end=None):\n",
    "        weight = self.in_proj_weight\n",
    "        bias = self.in_proj_bias\n",
    "        weight = weight[start:end, :]\n",
    "        if bias is not None:\n",
    "            bias = bias[start:end]\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "    def reorder_incremental_state(self, incremental_state, new_order):\n",
    "        \"\"\"Reorder buffered internal state (for incremental generation).\"\"\"\n",
    "        input_buffer = self._get_input_buffer(incremental_state)\n",
    "        if input_buffer is not None:\n",
    "            for k in input_buffer.keys():\n",
    "                input_buffer[k] = input_buffer[k].index_select(0, new_order)\n",
    "            self._set_input_buffer(incremental_state, input_buffer)\n",
    "\n",
    "    def _get_input_buffer(self, incremental_state):\n",
    "        return get_incremental_state(\n",
    "            self,\n",
    "            incremental_state,\n",
    "            'attn_state',\n",
    "        ) or {}\n",
    "\n",
    "    def _set_input_buffer(self, incremental_state, buffer):\n",
    "        set_incremental_state(\n",
    "            self,\n",
    "            incremental_state,\n",
    "            'attn_state',\n",
    "            buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):\n",
    "    if not export and torch.cuda.is_available():\n",
    "        try:\n",
    "            from apex.normalization import FusedLayerNorm\n",
    "            return FusedLayerNorm(normalized_shape, eps, elementwise_affine)\n",
    "        except ImportError:\n",
    "            pass\n",
    "    return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)\n",
    "\n",
    "\n",
    "def Linear(in_features, out_features, bias=True):\n",
    "    m = nn.Linear(in_features, out_features, bias)\n",
    "    nn.init.xavier_uniform_(m.weight)\n",
    "    if bias:\n",
    "        nn.init.constant_(m.bias, 0.)\n",
    "    return m\n",
    "\n",
    "\n",
    "# Modules\n",
    "class LearnedPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    This module learns positional embeddings up to a fixed maximum size.\n",
    "    Padding ids are ignored by either offsetting based on padding_idx\n",
    "    or by setting padding_idx to None and ensuring that the appropriate\n",
    "    position ids are passed to the forward function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            padding_idx: int,\n",
    "    ):\n",
    "        super().__init__(num_embeddings, embedding_dim, padding_idx)\n",
    "        self.onnx_trace = False\n",
    "\n",
    "    def forward(self, input, incremental_state=None, positions=None):\n",
    "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
    "        assert (\n",
    "            (positions is None) or (self.padding_idx is None)\n",
    "        ), \"If positions is pre-computed then padding_idx should not be set.\"\n",
    "\n",
    "        if positions is None:\n",
    "            if incremental_state is not None:\n",
    "                # positions is the same for every token when decoding a single step\n",
    "                positions = input.data.new(1, 1).fill_(self.padding_idx + input.size(1))\n",
    "            else:\n",
    "                positions = make_positions(\n",
    "                    input.data, self.padding_idx, onnx_trace=self.onnx_trace,\n",
    "                )\n",
    "        return super().forward(positions)\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum number of supported positions.\"\"\"\n",
    "        if self.padding_idx is not None:\n",
    "            return self.num_embeddings - self.padding_idx - 1\n",
    "        else:\n",
    "            return self.num_embeddings\n",
    "\n",
    "\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\n",
    "\n",
    "    Padding symbols are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, padding_idx, init_size=1024):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        self.weights = SinusoidalPositionalEmbedding.get_embedding(\n",
    "            init_size,\n",
    "            embedding_dim,\n",
    "            padding_idx,\n",
    "        )\n",
    "        self.onnx_trace = False\n",
    "        self.register_buffer('_float_tensor', torch.FloatTensor(1))\n",
    "\n",
    "    def prepare_for_onnx_export_(self):\n",
    "        self.onnx_trace = True\n",
    "\n",
    "    @staticmethod\n",
    "    def get_embedding(num_embeddings, embedding_dim, padding_idx=None):\n",
    "        \"\"\"Build sinusoidal embeddings.\n",
    "\n",
    "        This matches the implementation in tensor2tensor, but differs slightly\n",
    "        from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "        \"\"\"\n",
    "        half_dim = embedding_dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n",
    "        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)\n",
    "        if embedding_dim % 2 == 1:\n",
    "            # zero pad\n",
    "            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n",
    "        if padding_idx is not None:\n",
    "            emb[padding_idx, :] = 0\n",
    "        return emb\n",
    "\n",
    "    def forward(self, input, incremental_state=None, timestep=None, **kwargs):\n",
    "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
    "        bsz, seq_len = torch.onnx.operators.shape_as_tensor(input)\n",
    "        max_pos = self.padding_idx + 1 + seq_len\n",
    "        if self.weights is None or max_pos > self.weights.size(0):\n",
    "            # recompute/expand embeddings if needed\n",
    "            self.weights = SinusoidalPositionalEmbedding.get_embedding(\n",
    "                max_pos,\n",
    "                self.embedding_dim,\n",
    "                self.padding_idx,\n",
    "            )\n",
    "        self.weights = self.weights.to(self._float_tensor)\n",
    "\n",
    "        if incremental_state is not None:\n",
    "            # positions is the same for every token when decoding a single step\n",
    "            pos = (timestep.int() + 1).long() if timestep is not None else seq_len\n",
    "            if self.onnx_trace:\n",
    "                return self.weights[self.padding_idx + pos, :].unsqueeze(1).repeat(bsz, 1, 1)\n",
    "            return self.weights[self.padding_idx + pos, :].expand(bsz, 1, -1)\n",
    "\n",
    "        positions = make_positions(input, self.padding_idx, onnx_trace=self.onnx_trace)\n",
    "        if self.onnx_trace:\n",
    "            flat_embeddings = self.weights.detach().index_select(0, positions.view(-1))\n",
    "            embedding_shape = torch.cat((bsz.view(1), seq_len.view(1), torch.LongTensor([-1])))\n",
    "            embeddings = torch.onnx.operators.reshape_from_tensor_shape(flat_embeddings, embedding_shape)\n",
    "            return embeddings\n",
    "        return self.weights.index_select(0, positions.view(-1)).view(bsz, seq_len, -1).detach()\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum number of supported positions.\"\"\"\n",
    "        return int(1e5)  # an arbitrary large number\n",
    "\n",
    "\n",
    "def PositionalEmbedding(\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        padding_idx: int,\n",
    "        learned: bool = False,\n",
    "):\n",
    "    if learned:\n",
    "        # if padding_idx is specified then offset the embedding ids by\n",
    "        # this index and adjust num_embeddings appropriately\n",
    "        # TODO: The right place for this offset would be inside\n",
    "        # LearnedPositionalEmbedding. Move this there for a cleaner implementation.\n",
    "        if padding_idx is not None:\n",
    "            num_embeddings = num_embeddings + padding_idx + 1\n",
    "        m = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx)\n",
    "        nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n",
    "        if padding_idx is not None:\n",
    "            nn.init.constant_(m.weight[padding_idx], 0)\n",
    "    else:\n",
    "        m = SinusoidalPositionalEmbedding(\n",
    "            embedding_dim, padding_idx, init_size=num_embeddings + padding_idx + 1,\n",
    "        )\n",
    "    return m\n",
    "\n",
    "\n",
    "# Layers\n",
    "class BERTfusedEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, encoder_ffn_embed_dim,\n",
    "                 attention_dropout, dropout, bert_out_dim,\n",
    "                 encoder_attention_heads,\n",
    "                 encoder_ratio=0.5, bert_ratio=0.5,\n",
    "                 bert_gate=True,\n",
    "                 normalize_before=False,\n",
    "                 bert_dropnet=False,\n",
    "                 bert_dropnet_rate=0.25,\n",
    "                 bert_mixup=False,\n",
    "                 activation_dropout=0.,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        (bert_ratio, encoder_ratio) and dropnet are alternative\n",
    "        \"\"\"\n",
    "        super(BERTfusedEncoderLayer, self).__init__()\n",
    "        self.activation_fn = F.relu\n",
    "        self.dropout = dropout\n",
    "        self.activation_dropout = activation_dropout\n",
    "        self.normalize_before = normalize_before\n",
    "        self.embed_dim = embed_dim\n",
    "        self.self_attn = MultiheadAttention(\n",
    "            embed_dim, encoder_attention_heads,\n",
    "            dropout=attention_dropout, self_attention=True)\n",
    "        self.bert_attn = MultiheadAttention(\n",
    "            embed_dim=embed_dim, num_heads=encoder_attention_heads,\n",
    "            kdim=bert_out_dim, vdim=bert_out_dim,\n",
    "            dropout=attention_dropout,\n",
    "        )\n",
    "        self.self_attn_layer_norm = LayerNorm(embed_dim)\n",
    "        self.fc1 = Linear(embed_dim, encoder_ffn_embed_dim)\n",
    "        self.fc2 = Linear(encoder_ffn_embed_dim, embed_dim)\n",
    "        self.final_layer_norm = LayerNorm(embed_dim)\n",
    "        # bert-fused\n",
    "        self.encoder_ratio = encoder_ratio\n",
    "        self.bert_ratio = bert_ratio\n",
    "        self.bert_dropnet = bert_dropnet\n",
    "        self.bert_dropnet_rate = bert_dropnet_rate\n",
    "        assert 0. <= self.bert_dropnet_rate <= 0.5\n",
    "        self.bert_mixup = bert_mixup\n",
    "        if not bert_gate:\n",
    "            self.bert_ratio = 0.\n",
    "            self.bert_dropnet = False\n",
    "            self.bert_mixup = False\n",
    "\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "        \"\"\"\n",
    "        Rename layer norm states from `...layer_norms.0.weight` to\n",
    "        `...self_attn_layer_norm.weight` and `...layer_norms.1.weight` to\n",
    "        `...final_layer_norm.weight`\n",
    "        \"\"\"\n",
    "        layer_norm_map = {\n",
    "            '0': 'self_attn_layer_norm',\n",
    "            '1': 'final_layer_norm'\n",
    "        }\n",
    "        for old, new in layer_norm_map.items():\n",
    "            for m in ('weight', 'bias'):\n",
    "                k = '{}.layer_norms.{}.{}'.format(name, old, m)\n",
    "                if k in state_dict:\n",
    "                    state_dict[\n",
    "                        '{}.{}.{}'.format(name, new, m)\n",
    "                    ] = state_dict[k]\n",
    "                    del state_dict[k]\n",
    "\n",
    "    def forward(self, x, encoder_padding_mask, bert_encoder_out,\n",
    "                bert_encoder_padding_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n",
    "                `(batch, src_len)` where padding elements are indicated by ``1``.\n",
    "\n",
    "        Returns:\n",
    "            encoded output of shape `(batch, src_len, embed_dim)`\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.maybe_layer_norm(self.self_attn_layer_norm, x, before=True)\n",
    "        x1, _ = self.self_attn(\n",
    "            query=x, key=x, value=x, key_padding_mask=encoder_padding_mask)\n",
    "        x2, _ = self.bert_attn(\n",
    "            query=x, key=bert_encoder_out, value=bert_encoder_out,\n",
    "            key_padding_mask=bert_encoder_padding_mask)\n",
    "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
    "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
    "        # DCMMC: drop-net trick\n",
    "        ratios = self.get_ratio()\n",
    "        x = residual + ratios[0] * x1 + ratios[1] * x2\n",
    "        x = self.maybe_layer_norm(self.self_attn_layer_norm, x, after=True)\n",
    "\n",
    "        residual = x\n",
    "        x = self.maybe_layer_norm(self.final_layer_norm, x, before=True)\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = residual + x\n",
    "        x = self.maybe_layer_norm(self.final_layer_norm, x, after=True)\n",
    "        return x\n",
    "\n",
    "    def get_ratio(self):\n",
    "        if self.bert_dropnet:\n",
    "            frand = float(uniform(0, 1))\n",
    "            if self.bert_mixup and self.training:\n",
    "                return [frand, 1 - frand]\n",
    "            # dropnet trick\n",
    "            if frand < self.bert_dropnet_rate and self.training:\n",
    "                return [1, 0]\n",
    "            elif frand > 1 - self.bert_dropnet_rate and self.training:\n",
    "                return [0, 1]\n",
    "            else:\n",
    "                return [0.5, 0.5]\n",
    "        else:\n",
    "            return [self.encoder_ratio, self.bert_ratio]\n",
    "\n",
    "    def maybe_layer_norm(self, layer_norm, x, before=False, after=False):\n",
    "        assert before ^ after\n",
    "        if after ^ self.normalize_before:\n",
    "            return layer_norm(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class BERTfusedEncoder(nn.Module):\n",
    "    def __init__(self, dropout, encoder_layer, embed_dim,\n",
    "                 input_dim,\n",
    "                 bert_out_dim, encoder_ffn_embed_dim,\n",
    "                 encoder_attention_heads, attention_dropout,\n",
    "                 encoder_normalize_before=False,\n",
    "                 bert_dropnet=False,\n",
    "                 bert_dropnet_rate=0.25,\n",
    "                 bert_mixup=False,\n",
    "                 **kwargs):\n",
    "        super(BERTfusedEncoder, self).__init__()\n",
    "        # what if add position embed?\n",
    "        # self.embed_positions\n",
    "        self.dropout = dropout\n",
    "        self.bert_gates = [1, ] * encoder_layer\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.layers.extend([\n",
    "            BERTfusedEncoderLayer(\n",
    "                embed_dim=embed_dim,\n",
    "                encoder_ffn_embed_dim=encoder_ffn_embed_dim,\n",
    "                attention_dropout=attention_dropout,\n",
    "                dropout=dropout,\n",
    "                bert_out_dim=bert_out_dim,\n",
    "                encoder_attention_heads=encoder_attention_heads,\n",
    "                bert_gate=self.bert_gates[i],\n",
    "                normalize_before=encoder_normalize_before,\n",
    "                bert_dropnet=bert_dropnet,\n",
    "                bert_dropnet_rate=bert_dropnet_rate,\n",
    "                bert_mixup=bert_mixup,\n",
    "            )\n",
    "            for i in range(encoder_layer)\n",
    "        ])\n",
    "        self.project_in_dim = Linear(input_dim, embed_dim, bias=False) \\\n",
    "            if embed_dim != input_dim else None\n",
    "        if encoder_normalize_before:\n",
    "            self.layer_norm = LayerNorm(embed_dim)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "\n",
    "    def forward(self, source, src_lengths, encoder_padding_mask,\n",
    "                bert_encoder_out):\n",
    "        if self.project_in_dim is not None:\n",
    "            source = self.project_in_dim(source)\n",
    "        x = F.dropout(source, p=self.dropout, training=self.training)\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "        # encoder_padding_mask from CRNN\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x, encoder_padding_mask,\n",
    "                bert_encoder_out['bert_encoder_out'],\n",
    "                bert_encoder_out['bert_encoder_padding_mask'])\n",
    "        if self.layer_norm:\n",
    "            x = self.layer_norm(x)\n",
    "        return {\n",
    "            # T x B x C\n",
    "            'encoder_out': x,\n",
    "            # B x T\n",
    "            'encoder_padding_mask': encoder_padding_mask\n",
    "        }\n",
    "\n",
    "\n",
    "class BERTfusedDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, decoder_ffn_embed_dim,\n",
    "                 attention_dropout, dropout, bert_out_dim,\n",
    "                 decoder_attention_heads,\n",
    "                 encoder_ratio=0.5, bert_ratio=0.5,\n",
    "                 normalize_before=False,\n",
    "                 bert_dropnet=False,\n",
    "                 bert_dropnet_rate=0.25,\n",
    "                 bert_mixup=False,\n",
    "                 no_encoder_attn=False, add_bias_kv=False,\n",
    "                 add_zero_attn=False, bert_gate=True,\n",
    "                 char_inputs=False,\n",
    "                 activation_dropout=0.,\n",
    "                ):\n",
    "        super(BERTfusedDecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=decoder_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "            add_bias_kv=add_bias_kv,\n",
    "            add_zero_attn=add_zero_attn,\n",
    "            self_attention=True\n",
    "        )\n",
    "        self.dropout = dropout\n",
    "        self.activation_dropout = activation_dropout\n",
    "        self.activation_fn = F.relu\n",
    "        self.normalize_before = normalize_before\n",
    "        self.embed_dim = embed_dim\n",
    "        # dont know whats this\n",
    "        export = char_inputs\n",
    "        self.self_attn_layer_norm = LayerNorm(embed_dim, export=export)\n",
    "        if no_encoder_attn:\n",
    "            self.encoder_attn = None\n",
    "            self.encoder_attn_layer_norm = None\n",
    "        else:\n",
    "            self.encoder_attn = MultiheadAttention(\n",
    "                embed_dim, decoder_attention_heads,\n",
    "                dropout=attention_dropout, encoder_decoder_attention=True\n",
    "            )\n",
    "            self.bert_attn = MultiheadAttention(\n",
    "                self.embed_dim, decoder_attention_heads,\n",
    "                kdim=bert_out_dim, vdim=bert_out_dim,\n",
    "                dropout=attention_dropout, encoder_decoder_attention=True\n",
    "            )\n",
    "            self.encoder_attn_layer_norm = LayerNorm(embed_dim, export=export)\n",
    "        self.fc1 = Linear(self.embed_dim, decoder_ffn_embed_dim)\n",
    "        self.fc2 = Linear(decoder_ffn_embed_dim, self.embed_dim)\n",
    "        self.final_layer_norm = LayerNorm(self.embed_dim, export=export)\n",
    "        self.need_attn = True\n",
    "        self.onnx_trace = False\n",
    "        self.encoder_ratio = encoder_ratio\n",
    "        self.bert_ratio = bert_ratio\n",
    "        self.bert_dropnet = bert_dropnet\n",
    "        self.bert_dropnet_rate = bert_dropnet_rate\n",
    "        assert 0 <= self.bert_dropnet_rate <= 0.5\n",
    "        self.bert_mixup = bert_mixup\n",
    "        if not bert_gate:\n",
    "            self.bert_ratio = 0.\n",
    "            self.bert_dropnet = False\n",
    "            self.bert_mixup = False\n",
    "\n",
    "    def prepare_for_onnx_export_(self):\n",
    "        self.onnx_trace = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        encoder_out=None,\n",
    "        encoder_padding_mask=None,\n",
    "        bert_encoder_out=None,\n",
    "        bert_encoder_padding_mask=None,\n",
    "        incremental_state=None,\n",
    "        prev_self_attn_state=None,\n",
    "        prev_attn_state=None,\n",
    "        self_attn_mask=None,\n",
    "        self_attn_padding_mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n",
    "                `(batch, src_len)` where padding elements are indicated by ``True``.\n",
    "\n",
    "        Returns:\n",
    "            encoded output of shape `(batch, src_len, embed_dim)`\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.maybe_layer_norm(self.self_attn_layer_norm, x, before=True)\n",
    "        if prev_self_attn_state is not None:\n",
    "            if incremental_state is None:\n",
    "                incremental_state = {}\n",
    "            prev_key, prev_value = prev_self_attn_state\n",
    "            saved_state = {\"prev_key\": prev_key, \"prev_value\": prev_value}\n",
    "            self.self_attn._set_input_buffer(incremental_state, saved_state)\n",
    "        x, attn = self.self_attn(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            key_padding_mask=self_attn_padding_mask,\n",
    "            incremental_state=incremental_state,\n",
    "            need_weights=False,\n",
    "            attn_mask=self_attn_mask,\n",
    "        )\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = residual + x\n",
    "        x = self.maybe_layer_norm(self.self_attn_layer_norm, x, after=True)\n",
    "\n",
    "        if self.encoder_attn is not None:\n",
    "            residual = x\n",
    "            x = self.maybe_layer_norm(self.encoder_attn_layer_norm, x, before=True)\n",
    "            if prev_attn_state is not None:\n",
    "                if incremental_state is None:\n",
    "                    incremental_state = {}\n",
    "                prev_key, prev_value = prev_attn_state\n",
    "                saved_state = {\"prev_key\": prev_key, \"prev_value\": prev_value}\n",
    "                self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n",
    "            x1, attn = self.encoder_attn(\n",
    "                query=x,\n",
    "                key=encoder_out,\n",
    "                value=encoder_out,\n",
    "                key_padding_mask=encoder_padding_mask,\n",
    "                incremental_state=incremental_state,\n",
    "                static_kv=True,\n",
    "                need_weights=(not self.training and self.need_attn),\n",
    "            )\n",
    "            x2, _ = self.bert_attn(\n",
    "                query=x,\n",
    "                key=bert_encoder_out,\n",
    "                value=bert_encoder_out,\n",
    "                key_padding_mask=bert_encoder_padding_mask,\n",
    "                incremental_state=incremental_state,\n",
    "                static_kv=True,\n",
    "                need_weights=(not self.training and self.need_attn),\n",
    "            )\n",
    "            x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
    "            x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
    "            ratios = self.get_ratio()\n",
    "            x = residual + ratios[0] * x1 + ratios[1] * x2\n",
    "            x = self.maybe_layer_norm(self.encoder_attn_layer_norm, x, after=True)\n",
    "\n",
    "        residual = x\n",
    "        x = self.maybe_layer_norm(self.final_layer_norm, x, before=True)\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = residual + x\n",
    "        x = self.maybe_layer_norm(self.final_layer_norm, x, after=True)\n",
    "        if self.onnx_trace and incremental_state is not None:\n",
    "            saved_state = self.self_attn._get_input_buffer(incremental_state)\n",
    "            self_attn_state = saved_state[\"prev_key\"], saved_state[\"prev_value\"]\n",
    "            return x, attn, self_attn_state\n",
    "        return x, attn\n",
    "\n",
    "    def get_ratio(self):\n",
    "        if self.bert_dropnet:\n",
    "            frand = float(uniform(0, 1))\n",
    "            if self.bert_mixup and self.training:\n",
    "                return [frand, 1 - frand]\n",
    "            if frand < self.bert_dropnet_rate and self.training:\n",
    "                return [1, 0]\n",
    "            elif frand > 1 - self.bert_dropnet_rate and self.training:\n",
    "                return [0, 1]\n",
    "            else:\n",
    "                return [0.5, 0.5]\n",
    "        else:\n",
    "            return [self.encoder_ratio, self.bert_ratio]\n",
    "\n",
    "    def maybe_layer_norm(self, layer_norm, x, before=False, after=False):\n",
    "        assert before ^ after\n",
    "        if after ^ self.normalize_before:\n",
    "            return layer_norm(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def make_generation_fast_(self, need_attn=False, **kwargs):\n",
    "        self.need_attn = need_attn\n",
    "\n",
    "\n",
    "class BERTfusedDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_tgt_alphabet,\n",
    "                 dropout, decoder_layer, embed_dim,\n",
    "                 bert_out_dim, decoder_ffn_embed_dim,\n",
    "                 decoder_attention_heads, attention_dropout,\n",
    "                 embed_layer,\n",
    "                 normalize_before=False,\n",
    "                 bert_dropnet=False,\n",
    "                 bert_dropnet_rate=0.25,\n",
    "                 bert_mixup=False,\n",
    "                 max_target_positions=128,\n",
    "                 no_token_positional_embeddings=False,\n",
    "                 decoder_learned_pos=False,\n",
    "                 decoder_no_bert=False,\n",
    "                 no_encoder_attn=False):\n",
    "        super(BERTfusedDecoder, self).__init__()\n",
    "        bert_gates = [1, ] * decoder_layer\n",
    "        self.layers = nn.ModuleList([])\n",
    "        # TODO\n",
    "        # if decoder_no_bert:\n",
    "        self.layers.extend([\n",
    "            BERTfusedDecoderLayer(\n",
    "                embed_dim=embed_dim,\n",
    "                decoder_ffn_embed_dim=decoder_ffn_embed_dim,\n",
    "                attention_dropout=attention_dropout,\n",
    "                dropout=dropout,\n",
    "                bert_out_dim=bert_out_dim,\n",
    "                decoder_attention_heads=decoder_attention_heads,\n",
    "                normalize_before=normalize_before,\n",
    "                bert_dropnet=bert_dropnet,\n",
    "                bert_dropnet_rate=bert_dropnet_rate,\n",
    "                bert_mixup=bert_mixup,\n",
    "                bert_gate=bert_gates[i])\n",
    "            for i in range(decoder_layer)\n",
    "        ])\n",
    "        self.dropout = dropout\n",
    "        self.adaptive_softmax = None\n",
    "        self.embed_layer = embed_layer\n",
    "        self.project_in_dim = Linear(embed_layer.embedding_dim, embed_dim, bias=False) \\\n",
    "            if embed_dim != embed_layer.embedding_dim else None\n",
    "        self.embed_scale = math.sqrt(embed_dim)\n",
    "        out_embed_dim = self.embed_layer.embedding_dim\n",
    "        padding_idx = self.embed_layer.padding_idx\n",
    "        self.embed_positions = PositionalEmbedding(\n",
    "            max_target_positions, embed_dim, padding_idx,\n",
    "            learned=decoder_learned_pos,\n",
    "        ) if not no_token_positional_embeddings else None\n",
    "        self.project_out_dim = Linear(embed_dim, out_embed_dim, bias=False) \\\n",
    "            if embed_dim != out_embed_dim else None\n",
    "        self.embed_out = nn.Parameter(torch.Tensor(num_tgt_alphabet, out_embed_dim))\n",
    "        nn.init.normal_(self.embed_out, mean=0, std=out_embed_dim ** -0.5)\n",
    "        if normalize_before:\n",
    "            self.layer_norm = LayerNorm(embed_dim)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "\n",
    "    def forward(self, prev_output_tokens, encoder_out=None, bert_encoder_out=None,\n",
    "                incremental_state=None, **unused):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prev_output_tokens (LongTensor): previous decoder outputs of shape\n",
    "                `(batch, tgt_len)`, for input feeding/teacher forcing\n",
    "            encoder_out (Tensor, optional): output from the encoder, used for\n",
    "                encoder-side attention\n",
    "            incremental_state (dict): dictionary used for storing state during\n",
    "                :ref:`Incremental decoding`\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - the decoder's output of shape `(batch, tgt_len, vocab)`\n",
    "                - a dictionary with any model-specific outputs\n",
    "        \"\"\"\n",
    "        x, extra = self.extract_features(prev_output_tokens, encoder_out, bert_encoder_out,\n",
    "                                         incremental_state)\n",
    "        x = self.output_layer(x)\n",
    "#         return x, extra\n",
    "        return x\n",
    "\n",
    "    def extract_features(self, prev_output_tokens, encoder_out=None, bert_encoder_out=None,\n",
    "                         incremental_state=None, **unused):\n",
    "        \"\"\"\n",
    "        Similar to *forward* but only return features.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n",
    "                - a dictionary with any model-specific outputs\n",
    "        \"\"\"\n",
    "        # embed positions\n",
    "        positions = self.embed_positions(\n",
    "            prev_output_tokens,\n",
    "            incremental_state=incremental_state,\n",
    "        ) if self.embed_positions is not None else None\n",
    "        if incremental_state is not None:\n",
    "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
    "            if positions is not None:\n",
    "                positions = positions[:, -1:]\n",
    "        # embed tokens and positions\n",
    "        x = self.embed_scale * self.embed_layer(prev_output_tokens)\n",
    "        if self.project_in_dim is not None:\n",
    "            x = self.project_in_dim(x)\n",
    "        if positions is not None:\n",
    "            x += positions\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "        attn = None\n",
    "        inner_states = [x]\n",
    "        # decoder layers\n",
    "        for layer in self.layers:\n",
    "            x, attn = layer(\n",
    "                x,\n",
    "                encoder_out['encoder_out'] if encoder_out is not None else None,\n",
    "                encoder_out['encoder_padding_mask'] if encoder_out is not None else None,\n",
    "                bert_encoder_out['bert_encoder_out'],\n",
    "                bert_encoder_out['bert_encoder_padding_mask'],\n",
    "                incremental_state,\n",
    "                self_attn_mask=self.buffered_future_mask(x) if incremental_state is None else None,\n",
    "            )\n",
    "            inner_states.append(x)\n",
    "        if self.layer_norm:\n",
    "            x = self.layer_norm(x)\n",
    "        # T x B x C -> B x T x C\n",
    "        x = x.transpose(0, 1)\n",
    "        if self.project_out_dim is not None:\n",
    "            x = self.project_out_dim(x)\n",
    "        return x, {'attn': attn, 'inner_states': inner_states}\n",
    "\n",
    "    def buffered_future_mask(self, tensor):\n",
    "        dim = tensor.size(0)\n",
    "        if not hasattr(self, '_future_mask') or self._future_mask is None or \\\n",
    "                self._future_mask.device != tensor.device:\n",
    "            self._future_mask = torch.triu(fill_with_neg_inf(tensor.new(dim, dim)), 1)\n",
    "        if self._future_mask.size(0) < dim:\n",
    "            self._future_mask = torch.triu(fill_with_neg_inf(self._future_mask.resize_(\n",
    "                dim, dim)), 1)\n",
    "        return self._future_mask[:dim, :dim]\n",
    "\n",
    "    def output_layer(self, features, **kwargs):\n",
    "        \"\"\"Project features to the vocabulary size.\"\"\"\n",
    "        return F.linear(features, self.embed_out)\n",
    "\n",
    "\n",
    "class BERTfused(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        bert_model_name (str): default is 4-layer ALBERT\n",
    "    \"\"\"\n",
    "    def __init__(self, num_tgt_alphabet,\n",
    "                 input_dim,\n",
    "                 bert_model_name='voidful/albert_chinese_tiny',\n",
    "                 bert_output_layer=-1,\n",
    "                 bert_dropnet_rate=0.5,\n",
    "                 bert_dropnet=True,\n",
    "                 bert_mixup=False,\n",
    "                 dropout=0.3,\n",
    "                 attention_dropout=0.,\n",
    "                 normalize_before=False,\n",
    "                 decoder_no_bert=False,\n",
    "                 no_encoder_attn=False,\n",
    "                 encoder_layer=6, decoder_layer=6,\n",
    "                 encoder_embed_dim=512, encoder_ffn_embed_dim=1024,\n",
    "                 encoder_attention_heads=4, decoder_attention_heads=4,\n",
    "                 decoder_embed_dim=512, decoder_ffn_embed_dim=1024,\n",
    "                 **kwargs):\n",
    "        super(BERTfused, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert_encoder = AlbertForMaskedLM.from_pretrained(\n",
    "            bert_model_name,\n",
    "            output_hidden_states=True,\n",
    "            output_attentions=True)\n",
    "        for param in self.bert_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.bert_out_dim = self.bert_encoder.config.hidden_size\n",
    "        self.bert_output_layer = bert_output_layer\n",
    "        self.encoder = BERTfusedEncoder(\n",
    "            input_dim=input_dim,\n",
    "            dropout=self.dropout,\n",
    "            encoder_layer=encoder_layer,\n",
    "            embed_dim=encoder_embed_dim,\n",
    "            bert_out_dim=self.bert_out_dim,\n",
    "            encoder_ffn_embed_dim=encoder_ffn_embed_dim,\n",
    "            attention_dropout=attention_dropout,\n",
    "            encoder_attention_heads=encoder_attention_heads,\n",
    "            encoder_normalize_before=normalize_before,\n",
    "            bert_dropnet=bert_dropnet,\n",
    "            bert_dropnet_rate=bert_dropnet_rate,\n",
    "            bert_mixup=bert_mixup,\n",
    "        )\n",
    "        self.decoder_emb = self.build_embedding(num_tgt_alphabet, decoder_embed_dim)\n",
    "        self.decoder = BERTfusedDecoder(\n",
    "            num_tgt_alphabet=num_tgt_alphabet,\n",
    "            embed_layer=self.decoder_emb,\n",
    "            embed_dim=decoder_embed_dim,\n",
    "            decoder_layer=decoder_layer,\n",
    "            dropout=dropout,\n",
    "            bert_out_dim=self.bert_out_dim,\n",
    "            decoder_ffn_embed_dim=decoder_ffn_embed_dim,\n",
    "            attention_dropout=attention_dropout,\n",
    "            decoder_attention_heads=decoder_attention_heads,\n",
    "            normalize_before=normalize_before,\n",
    "            bert_dropnet=bert_dropnet,\n",
    "            bert_dropnet_rate=bert_dropnet_rate,\n",
    "            bert_mixup=bert_mixup,\n",
    "            decoder_no_bert=decoder_no_bert,\n",
    "            no_encoder_attn=no_encoder_attn,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_embedding(num_emb, embed_dim, padding_idx=0):\n",
    "        emb = nn.Embedding(num_emb, embed_dim, padding_idx=padding_idx)\n",
    "        nn.init.normal_(emb.weight, mean=0, std=embed_dim ** -0.5)\n",
    "        nn.init.constant_(emb.weight[padding_idx], 0)\n",
    "        return emb\n",
    "\n",
    "    def forward(self, source, prev_output_tokens, bert_input,\n",
    "                encoder_padding_mask=None, src_lengths=None,\n",
    "                **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source (LongTensor): hidden feature outputed from RNN in CRNN\n",
    "                `(batch, src_len, hidden_size)`\n",
    "            prev_output_tokens (LongTensor): previous decoder outputs of shape\n",
    "                `(batch, tgt_len)`, for input feeding/teacher forcing\n",
    "            bert_input (list of str): output string of CRNN\n",
    "            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n",
    "        \"\"\"\n",
    "        bert_input = self.bert_tokenizer.batch_encode_plus(\n",
    "            bert_input, pad_to_max_length=True, add_special_tokens=True)\n",
    "        # gpu context?\n",
    "        # In huggingface transformers, padding mask is 0 instead of 1\n",
    "        bert_encoder_padding_mask = (torch.tensor(bert_input['attention_mask']) == 0)\n",
    "        bert_encoder_out =  self.bert_encoder(\n",
    "            torch.tensor(bert_input['input_ids']),\n",
    "            attention_mask=bert_encoder_padding_mask)[-2]\n",
    "        bert_encoder_out = bert_encoder_out[self.bert_output_layer]\n",
    "        bert_encoder_out = {\n",
    "            # => (T, B, C)\n",
    "            'bert_encoder_out': bert_encoder_out.permute(1,0,2).contiguous(),\n",
    "            'bert_encoder_padding_mask': bert_encoder_padding_mask\n",
    "        }\n",
    "        if type(src_lengths) == type(None):\n",
    "            # default is no padding in source\n",
    "            # TODO for now, we only support that source without padding\n",
    "            src_lengths = torch.LongTensor([source.shape[1], ] * source.shape[0])\n",
    "        # TODO encoder_padding_mask seems duplicated with src_lengths\n",
    "        if type(encoder_padding_mask) == type(None):\n",
    "            encoder_padding_mask = torch.zeros(source.shape[:2], dtype=torch.bool)\n",
    "        encoder_out = self.encoder(\n",
    "            source, src_lengths=src_lengths,\n",
    "            encoder_padding_mask=encoder_padding_mask,\n",
    "            bert_encoder_out=bert_encoder_out)\n",
    "        decoder_out = self.decoder(\n",
    "            prev_output_tokens, encoder_out=encoder_out,\n",
    "            bert_encoder_out=bert_encoder_out, **kwargs)\n",
    "        return decoder_out.view(-1, decoder_out.shape[-1])\n",
    "#         return decoder_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thinc.api import MXNetWrapper, prefer_gpu\n",
    "from thinc.api import Model, PyTorchWrapper\n",
    "from thinc.api import xp2torch, torch2xp, Adam, MXNetShim\n",
    "from thinc.util import xp2mxnet, mxnet2xp, convert_recursive, is_xp_array, is_mxnet_array\n",
    "from thinc.types import ArgsKwargs\n",
    "from thinc.loss import Loss\n",
    "import cupy\n",
    "\n",
    "class NMTfusedCRNN(Model):\n",
    "    def __init__(self, crnn, alphabet, input_dim=200, is_gpu=False):\n",
    "        super(NMTfusedCRNN, self).__init__('NMTfusedCRNN', forward=self._forward_impl)\n",
    "        self.crnn = crnn\n",
    "        self.alphabet = alphabet\n",
    "        # pad, bos, eos + alphabet\n",
    "        num_tgt_alphabet = len(alphabet)\n",
    "        nmt = BERTfused(num_tgt_alphabet=num_tgt_alphabet,\n",
    "                        input_dim=input_dim, encoder_layer=4, decoder_layer=4,\n",
    "                        encoder_embed_dim=256, encoder_ffn_embed_dim=512,\n",
    "                        encoder_attention_heads=4, decoder_attention_heads=4,\n",
    "                        decoder_embed_dim=256, decoder_ffn_embed_dim=512)\n",
    "        for p in nmt.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform(p)\n",
    "        if is_gpu:\n",
    "            nmt = nmt.cuda()\n",
    "        self.nmt = PyTorchWrapper(nmt)\n",
    "        self._layers = [self.crnn, self.nmt]\n",
    "\n",
    "    @staticmethod\n",
    "    def _forward_impl(model, inputs, is_train=True):\n",
    "        img, prev_output_tokens = inputs\n",
    "        prev_output_tokens = xp2torch(prev_output_tokens, requires_grad=False)\n",
    "        if is_train:\n",
    "            (rnn_output, pred), backprop_crnn = model.crnn.begin_update(img)\n",
    "        else:\n",
    "            rnn_output, pred = model.crnn.predict(img)\n",
    "        batch_size = img.shape[0]\n",
    "        Yh = cupy.asnumpy(pred)\n",
    "        prob = np.reshape(Yh, (-1, batch_size, Yh.shape[1]))\n",
    "        res_crnn = []\n",
    "        for i in range(batch_size):\n",
    "            lp = np.argmax(prob[:, i, :], axis=-1)\n",
    "            res_crnn.append(''.join([\n",
    "                model.alphabet[ele] for idx, ele in enumerate(\n",
    "                    lp) if (lp[idx] and (idx == 0 or (lp[idx] != lp[idx - 1])))\n",
    "            ]))\n",
    "        if is_train:\n",
    "            word_probs, backprop_nmt = model.nmt.begin_update(\n",
    "                (rnn_output, prev_output_tokens, res_crnn))\n",
    "        else:\n",
    "            word_probs = model.nmt.predict(\n",
    "                (rnn_output, prev_output_tokens, res_crnn))\n",
    "        # normalized\n",
    "        word_probs = torch2xp(F.log_softmax(xp2torch(word_probs), dim=-1))\n",
    "\n",
    "        def finish_update(d_word_probs):\n",
    "            d_source, d_prev_output_tokens, _ = backprop_nmt(d_word_probs)\n",
    "            d_img = backprop_crnn(d_source)\n",
    "            return (d_img, d_prev_output_tokens)\n",
    "\n",
    "        return word_probs, finish_update\n",
    "\n",
    "\n",
    "class LabelSmoothedCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1, padding_idx=0):\n",
    "        super(LabelSmoothedCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "    def forward(self, lprobs, target):\n",
    "        lprobs = lprobs.view(-1, lprobs.size(-1))\n",
    "        target = target.requires_grad_(False).view(-1, 1).to(dtype=torch.long)\n",
    "        non_pad_mask = target.ne(self.padding_idx)\n",
    "        nll_loss = -lprobs.gather(dim=-1, index=target)[non_pad_mask]\n",
    "        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)[non_pad_mask]\n",
    "        nll_loss = nll_loss.sum()\n",
    "        smooth_loss = smooth_loss.sum()\n",
    "        eps_i = self.smoothing / lprobs.size(-1)\n",
    "        loss = (1. - self.smoothing) * nll_loss + eps_i * smooth_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "class LabelSmoothedCrossEntropyLoss(Loss):\n",
    "    def __init__(self, smoothing=0.1, padding_idx=0):\n",
    "        super(LabelSmoothedCrossEntropyLoss, self).__init__()\n",
    "        self.loss = PyTorchWrapper(\n",
    "            LabelSmoothedCrossEntropy(smoothing, padding_idx))\n",
    "\n",
    "    def get_loss(self, guesses, truths):\n",
    "        return self.loss.predict((guesses, truths))\n",
    "\n",
    "    def get_grad(self, guesses, truths):\n",
    "        truths = truths.astype('float32')\n",
    "        return self.loss.begin_update(\n",
    "            (guesses, truths))[1]([torch.tensor(1.)])\n",
    "\n",
    "    def __call__(self, guesses, truths):\n",
    "        truths = truths.astype('float32')\n",
    "        loss, grad_fn = self.loss.begin_update((guesses, truths))\n",
    "        grad = grad_fn([torch.tensor(1.)])\n",
    "        return grad, loss\n",
    "    \n",
    "def hacked_MXNetWrapper():\n",
    "    def MXNetWrapper(\n",
    "        mxnet_model,\n",
    "        convert_inputs=None,\n",
    "        convert_outputs=None,\n",
    "        model_class=Model,\n",
    "        model_name=\"mxnet\",\n",
    "    ):\n",
    "        if convert_inputs is None:\n",
    "            convert_inputs = convert_mxnet_default_inputs\n",
    "        if convert_outputs is None:\n",
    "            convert_outputs = convert_mxnet_default_outputs\n",
    "        return model_class(\n",
    "            model_name,\n",
    "            forward,\n",
    "            attrs={\"convert_inputs\": convert_inputs, \"convert_outputs\": convert_outputs},\n",
    "            shims=[MXNetShim(mxnet_model)],\n",
    "        )\n",
    "\n",
    "    def forward(model, X, is_train):\n",
    "        convert_inputs = model.attrs[\"convert_inputs\"]\n",
    "        convert_outputs = model.attrs[\"convert_outputs\"]\n",
    "        Xmxnet, get_dX = convert_inputs(model, X, is_train)\n",
    "        Ymxnet, mxnet_backprop = model.shims[0](Xmxnet, is_train)\n",
    "        Y, get_dYmxnet = convert_outputs(model, (X, Ymxnet), is_train)\n",
    "        def backprop(dY):\n",
    "            dYmxnet = get_dYmxnet(dY)\n",
    "            # hack! we only need d_rnn_output, dont need d_pred\n",
    "            dYmxnet.args = tuple(tuple([dYmxnet.args[0][0][0]]))\n",
    "            dXmxnet = mxnet_backprop(dYmxnet)\n",
    "            dX = get_dX(dXmxnet)\n",
    "            return dX\n",
    "        return Y, backprop\n",
    "\n",
    "    def convert_mxnet_default_inputs(\n",
    "        model, X, is_train\n",
    "    ):\n",
    "        xp2mxnet_ = lambda x: xp2mxnet(x, requires_grad=is_train)\n",
    "        converted = convert_recursive(is_xp_array, xp2mxnet_, X)\n",
    "        if isinstance(converted, ArgsKwargs):\n",
    "            def reverse_conversion(dXmxnet):\n",
    "                return convert_recursive(is_mxnet_array, mxnet2xp, dXmxnet)\n",
    "            return converted, reverse_conversion\n",
    "        elif isinstance(converted, dict):\n",
    "            def reverse_conversion(dXmxnet):\n",
    "                dX = convert_recursive(is_mxnet_array, mxnet2xp, dXmxnet)\n",
    "                return dX.kwargs\n",
    "            return ArgsKwargs(args=tuple(), kwargs=converted), reverse_conversion\n",
    "        elif isinstance(converted, (tuple, list)):\n",
    "            def reverse_conversion(dXmxnet):\n",
    "                dX = convert_recursive(is_mxnet_array, mxnet2xp, dXmxnet)\n",
    "                return dX.args\n",
    "            return ArgsKwargs(args=tuple(converted), kwargs={}), reverse_conversion\n",
    "        else:\n",
    "            def reverse_conversion(dXmxnet):\n",
    "                dX = convert_recursive(is_mxnet_array, mxnet2xp, dXmxnet)\n",
    "                return dX.args[0]\n",
    "            return ArgsKwargs(args=(converted,), kwargs={}), reverse_conversion\n",
    "\n",
    "    def convert_mxnet_default_outputs(model, X_Ymxnet, is_train):\n",
    "        X, Ymxnet = X_Ymxnet\n",
    "        Y = convert_recursive(is_mxnet_array, mxnet2xp, Ymxnet)\n",
    "        def reverse_conversion(dY):\n",
    "            dYmxnet = convert_recursive(is_xp_array, xp2mxnet, dY)\n",
    "            return ArgsKwargs(args=((Ymxnet,),), kwargs={\"head_grads\": dYmxnet})\n",
    "        return Y, reverse_conversion\n",
    "\n",
    "    return MXNetWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user -U prefetch_generator\n",
    "from thinc.types import SizedGenerator\n",
    "import random\n",
    "from prefetch_generator import background\n",
    "from math import ceil\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "import h5py\n",
    "from mxnet.gluon import SymbolBlock\n",
    "\n",
    "\n",
    "def get_alphabet():\n",
    "    # source text (i.e. CRNN output) is not need to add bos and eos tokens,\n",
    "    # however, target text need.\n",
    "    alphabet = {(idx + 1): tok.strip() for idx, tok in enumerate(\n",
    "        open('./cnocr/examples/label_cn.txt').readlines())}\n",
    "    # space token\n",
    "    alphabet[len(alphabet)] = ' '\n",
    "    pad_idx, bos_idx, eos_idx = 0, len(alphabet) + 1, len(alphabet) + 2\n",
    "    # the bos_idx and eos_idx is different from NMT where bos=2 and eos=3\n",
    "    alphabet[bos_idx] = '<s>'\n",
    "    alphabet[eos_idx] = '</s>'\n",
    "    # blank token\n",
    "    alphabet[0] = '#'\n",
    "    print(f'#alphabet:{len(alphabet)}, bos:{bos_idx}, eos:{eos_idx}')\n",
    "    return {'alphabet': alphabet, 'pad_idx': pad_idx, 'bos_idx': bos_idx,\n",
    "            'eos_idx': eos_idx}\n",
    "\n",
    "@background(max_prefetch=2)\n",
    "def hdf5Dataset(batch_size, asarray, alphabet, mode='train', train_ratio=0.8,\n",
    "                shuffle=True):\n",
    "    label_width = 20\n",
    "    alphabet_inv = {v: k for k, v in alphabet['alphabet'].items()}\n",
    "    pad_idx = alphabet['pad_idx']\n",
    "    bos_idx = alphabet['bos_idx']\n",
    "    eos_idx = alphabet['eos_idx']\n",
    "    dataset = h5py.File('./data_generated/dataset_fonts.h5', 'r')\n",
    "    indices = list(range(len(dataset)))\n",
    "    if shuffle:\n",
    "        random.shuffle(indices)\n",
    "    num_train = int(len(dataset) * train_ratio)\n",
    "    num_sample = num_train if mode == 'train' else len(dataset) - num_train\n",
    "    offset = 0 if mode == 'train' else num_train\n",
    "    num_batch = ceil(num_sample / batch_size)\n",
    "    cursor = 0\n",
    "    print(f'num_batch={num_batch}, batch_size={batch_size}, mode={mode}')\n",
    "    \n",
    "    for batch_idx in range(num_batch):\n",
    "        imgs = []\n",
    "        golds = []\n",
    "        for idx in range(batch_size * batch_idx, min(\n",
    "                len(indices), (batch_size * (batch_idx + 1)))):\n",
    "            idx = str(indices[idx])\n",
    "            imgs.append(dataset[idx]['img'][...] / 255.)\n",
    "            label = str(dataset[idx]['y'][...])\n",
    "            golds.append(label)\n",
    "        imgs = np.expand_dims(np.array(imgs, dtype=np.float32), axis=1)\n",
    "        # mx_model.crnn.ops.\n",
    "        imgs = np.array(imgs, dtype=np.float32)\n",
    "        golds_ids = [([alphabet_inv[c] for c in g] + [eos_idx, ]) for g in golds]\n",
    "        # In NMT, target usually uses right padding, but source text usually uses left padding.\n",
    "        golds_ids = [g + [pad_idx] * (label_width - len(g)) for g in golds_ids]\n",
    "        # used for Transformer's decoder\n",
    "        prev_output_tokens = [(g[-1:] + [bos_idx, ] + g[1:-1]) for g in golds_ids]\n",
    "        golds_ids = asarray(golds_ids, dtype='long').reshape(-1)\n",
    "        prev_output_tokens = asarray(prev_output_tokens, dtype='long')\n",
    "        yield imgs, prev_output_tokens, golds_ids\n",
    "        \n",
    "def inverse_sqrt_lr(lr=0.0005, warmup_updates=4000, warmup_init_lr=1e-7):\n",
    "    num_updates = 0\n",
    "    warmup_end_lr = lr\n",
    "    lr_step = (warmup_end_lr - warmup_init_lr) / warmup_updates\n",
    "    decay_factor = warmup_end_lr * warmup_updates ** 0.5\n",
    "    while True:\n",
    "        num_updates += 1\n",
    "        if num_updates <= warmup_updates:\n",
    "            yield warmup_init_lr + num_updates * lr_step\n",
    "        else:\n",
    "            yield decay_factor * num_updates ** -0.5\n",
    "\n",
    "def gen_CRNN_network(gpus, context):\n",
    "    num_classes = 6426\n",
    "    crnn_instance = CRNN_mxnet(num_classes, inference=True)\n",
    "    rnn_output, pred = crnn_instance.get_network()\n",
    "    input_symbol = crnn_instance.data\n",
    "    prefix = '/data/xiaowentao/.cnocr/1.1.0/conv-lite-lstm/cnocr-v1.1.0-conv-lite-lstm'\n",
    "    epoch = 47\n",
    "    data_shape = [('data', (batch_size, 1, 32, 560))]\n",
    "    # pred_fc = network.get_internals()['pred_fc_output']\n",
    "    # It seems that Thinc Shim only support mxnet Gluon.\n",
    "    # Therefore, we first wrap the Symbol network into Gluon Block.\n",
    "    network = SymbolBlock(outputs=[rnn_output, pred], inputs=input_symbol)\n",
    "    # load the parameetrs!\n",
    "    network.collect_params().load('%s-%04d' % (prefix, epoch) + '.params',\n",
    "                                  ctx=context)\n",
    "    # Yet another way!\n",
    "    # with open('/tmp/crnn.json', 'w') as f:\n",
    "    #     f.write(sym.tojson())\n",
    "    # network = SymbolBlock.imports('/tmp/crnn.json', ['data'],\n",
    "    #                               '%s-%04d' % (prefix, epoch) + '.params',\n",
    "    #                               ctx=context)\n",
    "    # network.hybridize(static_alloc=True, static_shape=True)\n",
    "    \n",
    "    # MXNet doesn't provide a Softmax layer but a .softmax() operation/method for \\\n",
    "    # prediction and it integrates an internal softmax during training. So to be able\\\n",
    "    # to integrate it with the rest of the components, you combine it with a Softmax() \\\n",
    "    # Thinc layer using the chain combinator.\n",
    "    wrapper_mxnet_crnn = hacked_MXNetWrapper()(network)\n",
    "    return wrapper_mxnet_crnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n",
      "#alphabet:6428, bos:6426, eos:6427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiaowentao/.local/lib/python3.7/site-packages/mxnet/gluon/block.py:1159: UserWarning: Cannot decide type for the following arguments. Consider providing them as input:\n",
      "\tdata: None\n",
      "  input_sym_arg_type = in_param.infer_type()[0]\n",
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:23: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "538a7e20555c4f40a0d6f3e031a5b89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_batch=27284, batch_size=48, mode=train\n",
      "loss: 494.56204\n",
      "loss: 494.21924\n",
      "loss: 493.29956\n",
      "loss: 493.15155\n",
      "loss: 493.1659\n",
      "loss: 493.19928\n",
      "loss: 493.1542\n",
      "loss: 493.1704\n",
      "loss: 493.18665\n",
      "loss: 493.1778\n",
      "loss: 493.17078\n",
      "loss: 493.18085\n",
      "loss: 493.16962\n",
      "loss: 493.16208\n",
      "loss: 493.1787\n",
      "loss: 493.15646\n",
      "loss: 493.11813\n",
      "loss: 493.15054\n",
      "loss: 493.19003\n",
      "loss: 493.14655\n",
      "loss: 493.16876\n",
      "loss: 493.1794\n",
      "loss: 493.16858\n",
      "loss: 493.11163\n",
      "loss: 493.19012\n",
      "loss: 493.19788\n",
      "loss: 493.10425\n",
      "loss: 493.18106\n",
      "loss: 493.14645\n",
      "loss: 493.1684\n",
      "loss: 493.23215\n",
      "loss: 493.15994\n",
      "loss: 493.13275\n",
      "loss: 493.27728\n",
      "loss: 493.27322\n",
      "loss: 493.31348\n",
      "loss: 493.2324\n",
      "loss: 493.35187\n",
      "loss: 493.13535\n",
      "loss: 493.1858\n",
      "loss: 493.26715\n",
      "loss: 493.30798\n",
      "loss: 493.3249\n",
      "loss: 493.26575\n",
      "loss: 493.26065\n",
      "loss: 493.44604\n",
      "loss: 492.907\n",
      "loss: 492.8549\n",
      "loss: 493.75787\n",
      "loss: 492.98773\n",
      "loss: 493.19177\n",
      "loss: 493.408\n",
      "loss: 493.4703\n",
      "loss: 493.56787\n",
      "loss: 493.51807\n",
      "loss: 493.638\n",
      "loss: 493.5653\n",
      "loss: 493.75922\n",
      "loss: 493.50046\n",
      "loss: 493.79456\n",
      "loss: 493.6812\n",
      "loss: 493.82233\n",
      "loss: 493.53333\n",
      "loss: 493.35452\n",
      "loss: 494.62967\n",
      "loss: 493.47095\n",
      "loss: 493.92087\n",
      "loss: 494.19415\n",
      "loss: 495.31012\n",
      "loss: 495.5285\n",
      "loss: 493.89896\n",
      "loss: 495.49606\n",
      "loss: 495.20935\n",
      "loss: 494.28638\n",
      "loss: 495.08014\n",
      "loss: 495.0904\n",
      "loss: 495.71637\n",
      "loss: 495.0246\n",
      "loss: 494.9647\n",
      "loss: 495.9233\n",
      "loss: 495.40182\n",
      "loss: 495.2708\n",
      "loss: 496.09225\n",
      "loss: 495.55466\n",
      "loss: 496.14343\n",
      "loss: 495.60983\n",
      "loss: 496.30835\n",
      "loss: 496.40826\n",
      "loss: 495.83875\n",
      "loss: 496.3182\n",
      "loss: 495.37482\n",
      "loss: 499.7396\n",
      "loss: 496.0819\n",
      "loss: 497.781\n",
      "loss: 496.1155\n",
      "loss: 499.13202\n",
      "loss: 498.2292\n",
      "loss: 498.67407\n",
      "loss: 500.29114\n",
      "loss: 498.46625\n",
      "loss: 498.67923\n",
      "loss: 501.8799\n",
      "loss: 499.76703\n",
      "loss: 501.7887\n",
      "loss: 502.37634\n",
      "loss: 500.83612\n",
      "loss: 500.86926\n",
      "loss: 503.26593\n",
      "loss: 502.08765\n",
      "loss: 503.93716\n",
      "loss: 502.7619\n",
      "loss: 503.90204\n",
      "loss: 503.77203\n",
      "loss: 504.80023\n",
      "loss: 504.87958\n",
      "loss: 503.29315\n",
      "loss: 504.22354\n",
      "loss: 504.13568\n",
      "loss: 505.4707\n",
      "loss: 506.15552\n",
      "loss: 504.99133\n",
      "loss: 505.80682\n",
      "loss: 506.20517\n",
      "loss: 506.32285\n",
      "loss: 506.70148\n",
      "loss: 506.87762\n",
      "loss: 507.29303\n",
      "loss: 508.88654\n",
      "loss: 512.5276\n",
      "loss: 515.04004\n",
      "loss: 516.0012\n",
      "loss: 517.10046\n",
      "loss: 518.2842\n",
      "loss: 528.1457\n",
      "loss: 522.2355\n",
      "loss: 529.40186\n",
      "loss: 524.0192\n",
      "loss: 530.296\n",
      "loss: 542.4991\n",
      "loss: 532.9669\n",
      "loss: 527.07855\n",
      "loss: 533.3722\n",
      "loss: 534.1861\n",
      "loss: 542.34656\n",
      "loss: 541.15283\n",
      "loss: 540.67474\n",
      "loss: 542.685\n",
      "loss: 550.95703\n",
      "loss: 546.2009\n",
      "loss: 555.0503\n",
      "loss: 552.8497\n",
      "loss: 553.7241\n",
      "loss: 567.99176\n",
      "loss: 560.549\n",
      "loss: 557.1377\n",
      "loss: 566.3503\n",
      "loss: 562.4877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1340.353\n",
      "loss: 1340.2203\n",
      "loss: 1352.271\n",
      "loss: 1356.247\n",
      "loss: 1287.1561\n",
      "loss: 1287.4409\n",
      "loss: 1322.363\n",
      "loss: 1353.0859\n",
      "loss: 1327.7112\n",
      "loss: 1482.0801\n",
      "loss: 1547.3828\n",
      "loss: 1637.3746\n",
      "loss: 1835.3049\n",
      "loss: 1547.8501\n",
      "loss: 1633.9609\n",
      "loss: 1718.1595\n",
      "loss: 1766.6489\n",
      "loss: 1669.5093\n",
      "loss: 1702.822\n",
      "loss: 1702.6619\n",
      "loss: 1770.7563\n",
      "loss: 1794.373\n",
      "loss: 2218.4978\n",
      "loss: 1850.5905\n",
      "loss: 1682.0522\n",
      "loss: 1735.2982\n",
      "loss: 1785.3168\n",
      "loss: 1895.9229\n",
      "loss: 1868.0051\n",
      "loss: 1944.0579\n",
      "loss: 2077.4492\n",
      "loss: 1778.6157\n",
      "loss: 1876.3407\n",
      "loss: 2114.1785\n",
      "loss: 2073.5173\n",
      "loss: 2085.0266\n",
      "loss: 2081.043\n",
      "loss: 2178.6958\n",
      "loss: 1967.9966\n",
      "loss: 2140.8108\n",
      "loss: 2173.282\n",
      "loss: 2179.0361\n",
      "loss: 2140.067\n",
      "loss: 2060.0027\n",
      "loss: 2291.9658\n",
      "loss: 2174.7375\n",
      "loss: 2268.2585\n",
      "loss: 2167.6199\n",
      "loss: 2451.1396\n",
      "loss: 2265.1213\n",
      "loss: 2165.8135\n",
      "loss: 2375.1973\n",
      "loss: 2340.7449\n",
      "loss: 2448.8882\n",
      "loss: 2416.288\n",
      "loss: 2480.5771\n",
      "loss: 2132.0684\n",
      "loss: 2172.5808\n",
      "loss: 2217.5754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6779.0547\n",
      "loss: 8219.103\n",
      "loss: 7059.747\n",
      "loss: 10911.826\n",
      "loss: 14135.941\n",
      "loss: 9736.236\n",
      "loss: 7103.616\n",
      "loss: 8164.9565\n",
      "loss: 17639.613\n",
      "loss: 7658.214\n",
      "loss: 8769.686\n",
      "loss: 7837.916\n",
      "loss: 8411.257\n",
      "loss: 7922.759\n",
      "loss: 12030.773\n",
      "loss: 7204.952\n",
      "loss: 8270.755\n",
      "loss: 8870.283\n",
      "loss: 9040.252\n",
      "loss: 13911.309\n",
      "loss: 7865.7856\n",
      "loss: 6954.8364\n",
      "loss: 6651.3525\n",
      "loss: 11229.988\n",
      "loss: 7575.913\n",
      "loss: 8176.501\n",
      "loss: 7925.246\n",
      "loss: 10389.621\n",
      "loss: 7680.882\n",
      "loss: 8044.4326\n",
      "loss: 10713.864\n",
      "loss: 12758.393\n",
      "loss: 8055.855\n",
      "loss: 8267.338\n",
      "loss: 7227.3413\n",
      "loss: 7813.1953\n",
      "loss: 10320.696\n",
      "loss: 10931.827\n",
      "loss: 8152.5225\n",
      "loss: 7147.914\n",
      "loss: 10226.256\n",
      "loss: 8470.756\n",
      "loss: 7490.4785\n",
      "loss: 12792.059\n",
      "loss: 14378.679\n",
      "loss: 8078.9443\n",
      "loss: 8734.293\n",
      "loss: 10283.65\n",
      "loss: 8066.9844\n",
      "loss: 8961.441\n",
      "loss: 9387.342\n",
      "loss: 6703.5283\n",
      "loss: 9648.069\n",
      "loss: 10121.566\n",
      "loss: 7420.4023\n",
      "loss: 8154.655\n",
      "loss: 8049.9365\n",
      "loss: 8889.294\n",
      "loss: 8268.088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-eae2316863fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mcnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mYh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmx_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_shifted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;31m#         print('Yh:', Yh.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mY_debug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxp2torch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/thinc/model.py\u001b[0m in \u001b[0;36mbegin_update\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mrespect\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[0;32m--> 291\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mInT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOutT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-df3b21f668c3>\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(model, inputs, is_train)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mrnn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mYh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcupy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mYh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mres_crnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/cupy/__init__.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(a, stream, order)\u001b[0m\n\u001b[1;32m    731\u001b[0m     \"\"\"\n\u001b[1;32m    732\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# thinc>=8.0.0a0\n",
    "# !pip install --user -U git+git://github.com/DCMMC/thinc.git\n",
    "from thinc.api import Adam, prefer_gpu, CategoricalCrossentropy\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "gpus = '3'\n",
    "batch_size = 48\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "is_gpu = len(gpus) > 0\n",
    "if is_gpu:\n",
    "    is_gpu = prefer_gpu()\n",
    "    print(\"GPU:\", is_gpu)\n",
    "\n",
    "context = [mx.context.gpu(i) for i in range(len(gpus))] if len(gpus) else \\\n",
    "        [mx.context.cpu()]\n",
    "alphabet = get_alphabet()\n",
    "wrapper_mxnet_crnn = gen_CRNN_network(gpus, context)\n",
    "mx_model = NMTfusedCRNN(wrapper_mxnet_crnn, alphabet['alphabet'], is_gpu=is_gpu)\n",
    "train_dataset = hdf5Dataset(batch_size, mx_model.crnn.ops.asarray, alphabet,\n",
    "                            mode='train', shuffle=True)\n",
    "optimizer = Adam(\n",
    "    beta1=0.9,\n",
    "    beta2=0.98,\n",
    "    eps=1e-10,\n",
    "#     learn_rate=inverse_sqrt_lr(),\n",
    "    learn_rate=0.01,\n",
    "#         L2=1e-6,\n",
    "#         grad_clip=1.0,\n",
    "#         use_averages=True,\n",
    "#         L2_is_weight_decay=True\n",
    ")\n",
    "# calculate_loss = LabelSmoothedCrossEntropyLoss()\n",
    "calculate_loss = CategoricalCrossentropy()\n",
    "\n",
    "res = []\n",
    "ground = []\n",
    "log_cp = 50\n",
    "cnt = 0\n",
    "with mx.Context('gpu', 0):\n",
    "    for X, tgt_shifted, Y in tqdm(train_dataset, leave=True):\n",
    "        cnt += 1\n",
    "        X = mx.nd.from_numpy(X).copyto(context[0])\n",
    "        Yh, backprop = mx_model.begin_update((X, tgt_shifted))\n",
    "#         print('Yh:', Yh.shape)\n",
    "        Y_debug = np.argmax(np.array(xp2torch(Yh).tolist()), axis=-1)\n",
    "        grad, loss = calculate_loss(Yh, Y)\n",
    "        if cnt % log_cp == 0:\n",
    "            print('loss:', loss)\n",
    "        # grad: [d_guesses, d_truths]\n",
    "#         grad = grad[0]\n",
    "        backprop(grad)\n",
    "        mx_model.finish_update(optimizer)\n",
    "        optimizer.step_schedules()\n",
    "#     # hidden: (batch_size, seq_len=139, hidden_size=200)\n",
    "#     hidden, Yh = mx_model.predict(X)\n",
    "#     assert Yh.shape == (batch_size * 139, num_classes)\n",
    "#     Yh = cupy.asnumpy(Yh)\n",
    "#     Y = cupy.asnumpy(Y[0])\n",
    "#     prob = np.reshape(Yh, (-1, batch_size, Yh.shape[1]))\n",
    "#     for i in range(batch_size):\n",
    "#         lp = np.argmax(prob[:, i, :], axis=-1)\n",
    "# #         print(lp[:10])\n",
    "#         res.append(''.join([\n",
    "#             alphabet[ele] for idx, ele in enumerate(\n",
    "#                 lp) if (lp[idx] and (idx == 0 or (lp[idx] != lp[idx - 1])))\n",
    "#         ]))\n",
    "#         ground.append(''.join([alphabet[c] for c in Y[i]]))\n",
    "# print(res[:4], '\\n', ground[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun May  3 11:58:31 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 430.14       Driver Version: 430.14       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:18:00.0  On |                  N/A |\r\n",
      "| 22%   33C    P2    61W / 250W |   1868MiB / 11019MiB |      3%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:3B:00.0  On |                  N/A |\r\n",
      "| 22%   25C    P8    12W / 250W |     27MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:86:00.0  On |                  N/A |\r\n",
      "| 22%   26C    P8    21W / 250W |     27MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:AF:00.0  On |                  N/A |\r\n",
      "| 26%   41C    P2    63W / 250W |  10588MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0    231292      C   python                                      1817MiB |\r\n",
      "|    0    243184      G   /usr/lib/xorg/Xorg                            39MiB |\r\n",
      "|    1    243184      G   /usr/lib/xorg/Xorg                            15MiB |\r\n",
      "|    2    243184      G   /usr/lib/xorg/Xorg                            15MiB |\r\n",
      "|    3    192787      C   /data/xiaowentao/.anaconda3/bin/python     10561MiB |\r\n",
      "|    3    243184      G   /usr/lib/xorg/Xorg                            15MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ConvReluBlock(nn.Module):\n",
    "    '''\n",
    "    2-layer Conv2d + (batch norm) + LeakyBlock\n",
    "    :param res_conn: boolean\n",
    "        if add residual connection\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes, padding_sizes,\n",
    "                 res_conn=False, bottle_conv=False, **kwargs):\n",
    "        super(ConvReluBlock, self).__init__(**kwargs)\n",
    "        self.res_conn = res_conn\n",
    "        layers = []\n",
    "        # Layer 1\n",
    "        if bottle_conv:\n",
    "            # conv 1x1 with same input and output channels\n",
    "            layers += [\n",
    "                nn.Conv2d(in_channels[0], out_channels[0] // 2, (1, 1),\n",
    "                      stride=1, padding=(0, 0)),\n",
    "                nn.LeakyReLU()\n",
    "            ]\n",
    "            in_channels[0] = out_channels[0] // 2\n",
    "            out_channels[0] = out_channels[0] // 2\n",
    "        layers += [\n",
    "            nn.Conv2d(in_channels[0], out_channels[0], kernel_sizes[0],\n",
    "                      stride=1, padding=padding_sizes[0]),\n",
    "            nn.LeakyReLU()\n",
    "        ]\n",
    "        if bottle_conv:\n",
    "            layers += [\n",
    "                nn.Conv2d(out_channels[0], out_channels[0] * 2, (1, 1),\n",
    "                      stride=1, padding=(0, 0)),\n",
    "                nn.LeakyReLU()\n",
    "            ]\n",
    "        if len(in_channels) > 1:\n",
    "            if bottle_conv:\n",
    "                layers += [\n",
    "                    nn.Conv2d(in_channels[1], out_channels[1] // 2, (1, 1),\n",
    "                          stride=1, padding=(0, 0)),\n",
    "                    nn.LeakyReLU()\n",
    "                ]\n",
    "                in_channels[1] = out_channels[1] // 2\n",
    "                out_channels[1] = out_channels[1] // 2\n",
    "            # Layer 2 with batch norm\n",
    "            layers += [\n",
    "                nn.Conv2d(in_channels[1], out_channels[1], kernel_sizes[1],\n",
    "                      stride=1, padding=padding_sizes[1]),\n",
    "                nn.BatchNorm2d(out_channels[1]),\n",
    "                nn.LeakyReLU(),\n",
    "            ]\n",
    "            if bottle_conv:\n",
    "                layers += [\n",
    "                    nn.Conv2d(out_channels[1], out_channels[1] * 2, (1, 1),\n",
    "                          stride=1, padding=(0, 0)),\n",
    "                    nn.LeakyReLU()\n",
    "                ]\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.block(x)\n",
    "        # residual connection\n",
    "        if self.res_conn:\n",
    "            out += x\n",
    "        return out\n",
    "\n",
    "    \n",
    "class CRNN(nn.Module):\n",
    "    '''\n",
    "    Same as cnocr/symbols/crnn.py/crnn_lstm_lite.\n",
    "    7-layer CNN + 2-layer LSTM\n",
    "    CNN reduce the width of image to width // 4 - 1, e.g. 560 => 139\n",
    "    '''\n",
    "    def __init__(self, dropout=0., rnn_hidden_size=100,\n",
    "                 batch_first=False, **kwargs):\n",
    "        super(CRNN, self).__init__(**kwargs)\n",
    "        self.batch_first = batch_first\n",
    "        # 7-layer Conv + 3-layer Pool\n",
    "        conv_kernel = [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (4, 1)]\n",
    "        conv_padding = [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (0, 0)]\n",
    "        conv_channel = [64, 128, 256, 512, 512, 512, 512]\n",
    "        # layer number of Pooling\n",
    "        pool_kernel = [(2, 2)] * 3\n",
    "        pool_stride = [(2, 2)] * 2 + [(2, 1)]\n",
    "        # Suppose input shape (N, C, H, W) is: (N, 1, 32, 560)\n",
    "        self.cnn = nn.Sequential(OrderedDict([\n",
    "            # => (N, 128, 32, 560)\n",
    "            ('ConvBlock-0', ConvReluBlock(\n",
    "                [1, conv_channel[0]], conv_channel[0:2],\n",
    "                conv_kernel[0:2], conv_padding[0:2])),\n",
    "            # => (N, 128, 16, 280)\n",
    "            ('Pool-0', nn.MaxPool2d(pool_kernel[0], pool_stride[0])),\n",
    "            # => (N, 512, 16, 280)\n",
    "            ('ConvBlock-1', ConvReluBlock(\n",
    "                conv_channel[1:3], conv_channel[2:4],\n",
    "                conv_kernel[2:4], conv_padding[2:4])),\n",
    "            # => (N, 512, 8, 140)\n",
    "            ('Pool-1', nn.MaxPool2d(pool_kernel[1], pool_stride[1])),\n",
    "            # => (N, 512, 8, 140)\n",
    "            ('BottleBlock-0', ConvReluBlock(\n",
    "                conv_channel[3:5], conv_channel[4:6],\n",
    "                conv_kernel[4:6], conv_padding[4:6],\n",
    "                bottle_conv=True, res_conn=True)),\n",
    "            # => (N, 512, 4, 139)\n",
    "            ('Pool-2', nn.MaxPool2d(pool_kernel[2], pool_stride[2])),\n",
    "            # => (N, 512, 1, 139)\n",
    "            ('BottleBlock-1', ConvReluBlock(\n",
    "                conv_channel[5:6], conv_channel[6:],\n",
    "                conv_kernel[6:], conv_padding[6:],\n",
    "                bottle_conv=True)),\n",
    "        ]))\n",
    "        if dropout > 0:\n",
    "            self.cnn.add_module('Dropout-0', nn.Dropout(dropout))\n",
    "        # 2-layer Bi-LSTM\n",
    "        self.rnn = nn.LSTM(input_size=conv_channel[-1],\n",
    "                           hidden_size=rnn_hidden_size,\n",
    "                           num_layers=2,\n",
    "                           batch_first=batch_first,\n",
    "                           bidirectional=True\n",
    "                          )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        conv = self.cnn(x)\n",
    "        # h == 1\n",
    "        b, c, h, w = conv.size()\n",
    "        assert h == 1, f'the output height of conv must be 1 instead of {h}'\n",
    "        if self.batch_first:\n",
    "            # => (B, width, channel) a.k.a. (B, seq_len, input_size)\n",
    "            # e.g. (N, 139, 512)\n",
    "            conv = conv.squeeze(2).permute(0, 2, 1)\n",
    "        else:\n",
    "            conv = conv.squeeze(2).permute(2, 0, 1)\n",
    "        # => (N, 139, 200) / (139, N, 200)\n",
    "        rnn_output, _ = self.rnn(conv)\n",
    "        return rnn_output\n",
    "    \n",
    "    \n",
    "class OCR(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0., rnn_hidden_size=100,\n",
    "                 batch_first=False, **kwargs):\n",
    "        super(OCR, self).__init__(**kwargs)\n",
    "        self.batch_first = batch_first\n",
    "        self.crnn = CRNN(dropout=dropout,\n",
    "                         rnn_hidden_size=rnn_hidden_size,\n",
    "                         batch_first=batch_first)\n",
    "        self.proj = nn.Linear(2 * rnn_hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        rnn_output = self.crnn(x)\n",
    "        if self.batch_first:\n",
    "            b, t, h = rnn_output.size()\n",
    "            rnn_output = rnn_output.contiguous()\n",
    "        else:\n",
    "            t, b, h = rnn_output.size()\n",
    "        # rnn with batch_first must .contiguous() before view()\n",
    "        # .contiguous() add extra cost.\n",
    "        output = self.proj(rnn_output.view(b * t, h))\n",
    "        if self.batch_first:\n",
    "            output = output.view(b, t, -1)\n",
    "        else:\n",
    "            output = output.view(t, b, -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#trainable parameters: 5520712\n",
      "OCR(\n",
      "  (crnn): CRNN(\n",
      "    (cnn): Sequential(\n",
      "      (ConvBlock-0): ConvReluBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): LeakyReLU(negative_slope=0.01)\n",
      "        )\n",
      "      )\n",
      "      (Pool-0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (ConvBlock-1): ConvReluBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): LeakyReLU(negative_slope=0.01)\n",
      "        )\n",
      "      )\n",
      "      (Pool-1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (BottleBlock-0): ConvReluBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): LeakyReLU(negative_slope=0.01)\n",
      "          (4): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (5): LeakyReLU(negative_slope=0.01)\n",
      "          (6): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (7): LeakyReLU(negative_slope=0.01)\n",
      "          (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (10): LeakyReLU(negative_slope=0.01)\n",
      "          (11): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (12): LeakyReLU(negative_slope=0.01)\n",
      "        )\n",
      "      )\n",
      "      (Pool-2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "      (BottleBlock-1): ConvReluBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Conv2d(256, 256, kernel_size=(4, 1), stride=(1, 1))\n",
      "          (3): LeakyReLU(negative_slope=0.01)\n",
      "          (4): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (5): LeakyReLU(negative_slope=0.01)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(512, 100, num_layers=2, bidirectional=True)\n",
      "  )\n",
      "  (proj): Linear(in_features=200, out_features=5000, bias=True)\n",
      ") \n",
      " ================================================== \n",
      "\n",
      "crnn\n",
      "CPU times: user 43.9 s, sys: 0 ns, total: 43.9 s\n",
      "Wall time: 3.15 s\n",
      "crnn_bf\n",
      "CPU times: user 20.1 s, sys: 2.38 s, total: 22.5 s\n",
      "Wall time: 1.95 s\n",
      "torch.Size([139, 8, 5000]) torch.Size([8, 139, 5000])\n"
     ]
    }
   ],
   "source": [
    "# Test CRNN-pytorch\n",
    "# sometimes crnn_bf even faster than crnn\n",
    "crnn_bf = OCR(5000, batch_first=True)\n",
    "crnn = OCR(5000, batch_first=False)\n",
    "print('#trainable parameters:', sum(p.numel() for p in crnn.parameters() if p.requires_grad))\n",
    "print(crnn, '\\n', '='*50, '\\n')\n",
    "inp = torch.randn(8, 1, 32, 560)\n",
    "print('crnn')\n",
    "%time out = crnn(inp)\n",
    "print('crnn_bf')\n",
    "%time out_bf = crnn_bf(inp)\n",
    "print(out.shape, out_bf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 32, 560)\n",
      "['', '.']\n"
     ]
    }
   ],
   "source": [
    "# Test CRNN\n",
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class SimpleBatch(object):\n",
    "    def __init__(self, data_names, data, label_names=list(), label=list()):\n",
    "        self._data = data\n",
    "        self._label = label\n",
    "        self._data_names = data_names\n",
    "        self._label_names = label_names\n",
    "\n",
    "        self.pad = 0\n",
    "        self.index = None  # TODO: what is index?\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    @property\n",
    "    def label(self):\n",
    "        return self._label\n",
    "\n",
    "    @property\n",
    "    def data_names(self):\n",
    "        return self._data_names\n",
    "\n",
    "    @property\n",
    "    def label_names(self):\n",
    "        return self._label_names\n",
    "\n",
    "    @property\n",
    "    def provide_data(self):\n",
    "        return [(n, x.shape) for n, x in zip(self._data_names, self._data)]\n",
    "\n",
    "    @property\n",
    "    def provide_label(self):\n",
    "        return [(n, x.shape) for n, x in zip(self._label_names, self._label)]\n",
    "\n",
    "\n",
    "def inference(samples, alphabet, batch_size,\n",
    "              network, data_shape, context, prefix, epoch):\n",
    "    '''\n",
    "    alphabet contains num_classes - 1 elements,\n",
    "    because it does not contain black token\n",
    "    '''\n",
    "    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)\n",
    "    if network is not None:\n",
    "        sym = network\n",
    "    # DCMMC: useless codes\n",
    "#     pred_fc = sym.get_internals()['pred_fc_output']\n",
    "#     sym = mx.sym.softmax(data=pred_fc)\n",
    "    mod = mx.mod.Module(\n",
    "        symbol=sym, context=context, data_names=['data', ], label_names=None\n",
    "    )\n",
    "    mod.bind(for_training=False, data_shapes=data_shape)\n",
    "    # DCMMC: in jupyter environment, you need restart juputer kernel\n",
    "    # to release all the models before you create new model instances.\n",
    "    mod.set_params(arg_params, aux_params, allow_missing=False)\n",
    "    \n",
    "    mod.forward(samples)\n",
    "    prob = mod.get_outputs()[0].asnumpy()\n",
    "    # => (seq_len, batch_size, num_classes)\n",
    "    prob = np.reshape(prob, (-1, batch_size, prob.shape[1]))\n",
    "    res = []\n",
    "    for i in range(batch_size):\n",
    "        lp = np.argmax(prob[:, i, :], axis=-1)\n",
    "        res.append(''.join([\n",
    "            alphabet[ele - 1] for idx, ele in enumerate(\n",
    "                lp) if (lp[idx] and (idx == 0 or (lp[idx] != lp[idx - 1])))\n",
    "        ]))\n",
    "    return res\n",
    "\n",
    "# cpu mode:\n",
    "# gpus = ''\n",
    "gpus = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "context = [mx.context.gpu(i) for i in range(len(gpus))] if len(gpus) else \\\n",
    "    [mx.context.cpu()]\n",
    "num_classes = 6426\n",
    "batch_size = 2\n",
    "img_size = (32, 560)\n",
    "network = CRNN_mxnet(num_classes, inference=True).get_network()\n",
    "data_shape = [('data', (batch_size, 1) + img_size)]\n",
    "prefix = '/data/xiaowentao/.cnocr/1.1.0/conv-lite-lstm/cnocr-v1.1.0-conv-lite-lstm'\n",
    "alphabet = {idx: v.strip() for idx, v in enumerate(\n",
    "    open('./cnocr/examples/label_cn.txt').readlines())}\n",
    "alphabet[len(alphabet) - 1] = alphabet[len(alphabet) - 1].replace('<space>', ' ')\n",
    "assert all([len(v) for v in alphabet.values()])\n",
    "# original version of cnocr is 45\n",
    "# epoch = 45\n",
    "epoch = 47\n",
    "\n",
    "imgs = [\n",
    "    np.array(\n",
    "        ImageOps.expand(\n",
    "            Image.open('./cnocr/examples/chn-00199981.jpg').convert('L'), (0, 0, 280, 0)),\n",
    "        dtype='float32'),\n",
    "    np.array(\n",
    "        ImageOps.expand(\n",
    "            Image.open('./cnocr/examples/chn-00199985.jpg').convert('L'), (0, 0, 280, 0)),\n",
    "        dtype='float32')\n",
    "]\n",
    "imgs = [img / 255. for img in imgs]\n",
    "imgs = mx.nd.expand_dims(mx.nd.array(imgs), 1)\n",
    "print(imgs.shape)\n",
    "samples = SimpleBatch(data_names=['data'], data=[imgs])\n",
    "res = inference(samples, alphabet, batch_size,\n",
    "                network, data_shape, context, prefix, epoch)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTfused(\n",
      "  (bert_encoder): AlbertForMaskedLM(\n",
      "    (albert): AlbertModel(\n",
      "      (embeddings): AlbertEmbeddings(\n",
      "        (word_embeddings): Embedding(21128, 128, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 128)\n",
      "        (token_type_embeddings): Embedding(2, 128)\n",
      "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (encoder): AlbertTransformer(\n",
      "        (embedding_hidden_mapping_in): Linear(in_features=128, out_features=312, bias=True)\n",
      "        (albert_layer_groups): ModuleList(\n",
      "          (0): AlbertLayerGroup(\n",
      "            (albert_layers): ModuleList(\n",
      "              (0): AlbertLayer(\n",
      "                (full_layer_layer_norm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "                (attention): AlbertAttention(\n",
      "                  (query): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (key): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (value): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "                )\n",
      "                (ffn): Linear(in_features=312, out_features=1248, bias=True)\n",
      "                (ffn_output): Linear(in_features=1248, out_features=312, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): Linear(in_features=312, out_features=312, bias=True)\n",
      "      (pooler_activation): Tanh()\n",
      "    )\n",
      "    (predictions): AlbertMLMHead(\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dense): Linear(in_features=312, out_features=128, bias=True)\n",
      "      (decoder): Linear(in_features=128, out_features=21128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (encoder): BERTfusedEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): BERTfusedEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (final_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): BERTfusedEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (final_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (project_in_dim): Linear(in_features=20, out_features=32, bias=False)\n",
      "  )\n",
      "  (decoder_emb): Embedding(50, 32, padding_idx=0)\n",
      "  (decoder): BERTfusedDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): BERTfusedDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (final_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): BERTfusedDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (final_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (embed_layer): Embedding(50, 32, padding_idx=0)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "  )\n",
      ")\n",
      "Trainable parameters: 135168, total parameters: 4277136\n",
      "torch.Size([3, 40, 50])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "gpus = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "# Test BERT-fused NMT\n",
    "model = BERTfused(num_tgt_alphabet=50, input_dim=20,\n",
    "                      encoder_layer=2, decoder_layer=2,\n",
    "                      encoder_embed_dim=32, encoder_ffn_embed_dim=64,\n",
    "                      encoder_attention_heads=2, decoder_attention_heads=2,\n",
    "                      decoder_embed_dim=32, decoder_ffn_embed_dim=64\n",
    "                      )\n",
    "if len(gpus) > 0:\n",
    "    model = model.cuda()\n",
    "print(model)\n",
    "print('Trainable parameters: {}, total parameters: {}'.format(\n",
    "    sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    sum(p.numel() for p in model.parameters())\n",
    "))\n",
    "# (batch_size, seq_length, hidden_size)\n",
    "source = torch.rand(3, 20, 20)\n",
    "# src_lengths = torch.Tensor([18, 17, 16])\n",
    "src_lengths = None\n",
    "# encoder_padding_mask = (torch.zeros([3, 20]) == 1)\n",
    "encoder_padding_mask = None\n",
    "prev_output_tokens = torch.randint(1, 48, size=(3, 40), dtype=torch.long)\n",
    "bert_input = ['1', '22', '333']\n",
    "y_hat, _ = model(source, prev_output_tokens, bert_input, encoder_padding_mask,\n",
    "                 src_lengths)\n",
    "print(y_hat.shape)\n",
    "assert list(y_hat.shape) == [3, 40, 50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
