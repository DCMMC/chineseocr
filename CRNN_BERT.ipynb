{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import OrderedDict\n",
    "\n",
    "class ConvReluBlock(nn.Module):\n",
    "    '''\n",
    "    2-layer Conv2d + (batch norm) + LeakyBlock\n",
    "    :param res_conn: boolean\n",
    "        if add residual connection\n",
    "    '''\n",
    "    def __init__(self, in_channels, out_channels, kernel_sizes, padding_sizes,\n",
    "                 res_conn=False, bottle_conv=False, **kwargs):\n",
    "        super(ConvReluBlock, self).__init__(**kwargs)\n",
    "        self.res_conn = res_conn\n",
    "        layers = []\n",
    "        # Layer 1\n",
    "        if bottle_conv:\n",
    "            # conv 1x1 with same input and output channels\n",
    "            layers += [\n",
    "                nn.Conv2d(in_channels[0], out_channels[0] // 2, (1, 1),\n",
    "                      stride=1, padding=(0, 0)),\n",
    "                nn.LeakyReLU()\n",
    "            ]\n",
    "            in_channels[0] = out_channels[0] // 2\n",
    "            out_channels[0] = out_channels[0] // 2\n",
    "        layers += [\n",
    "            nn.Conv2d(in_channels[0], out_channels[0], kernel_sizes[0],\n",
    "                      stride=1, padding=padding_sizes[0]),\n",
    "            nn.LeakyReLU()\n",
    "        ]\n",
    "        if bottle_conv:\n",
    "            layers += [\n",
    "                nn.Conv2d(out_channels[0], out_channels[0] * 2, (1, 1),\n",
    "                      stride=1, padding=(0, 0)),\n",
    "                nn.LeakyReLU()\n",
    "            ]\n",
    "        if len(in_channels) > 1:\n",
    "            if bottle_conv:\n",
    "                layers += [\n",
    "                    nn.Conv2d(in_channels[1], out_channels[1] // 2, (1, 1),\n",
    "                          stride=1, padding=(0, 0)),\n",
    "                    nn.LeakyReLU()\n",
    "                ]\n",
    "                in_channels[1] = out_channels[1] // 2\n",
    "                out_channels[1] = out_channels[1] // 2\n",
    "            # Layer 2 with batch norm\n",
    "            layers += [\n",
    "                nn.Conv2d(in_channels[1], out_channels[1], kernel_sizes[1],\n",
    "                      stride=1, padding=padding_sizes[1]),\n",
    "                nn.BatchNorm2d(out_channels[1]),\n",
    "                nn.LeakyReLU(),\n",
    "            ]\n",
    "            if bottle_conv:\n",
    "                layers += [\n",
    "                    nn.Conv2d(out_channels[1], out_channels[1] * 2, (1, 1),\n",
    "                          stride=1, padding=(0, 0)),\n",
    "                    nn.LeakyReLU()\n",
    "                ]\n",
    "        self.block = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.block(x)\n",
    "        # residual connection\n",
    "        if self.res_conn:\n",
    "            out += x\n",
    "        return out\n",
    "\n",
    "    \n",
    "class CRNN(nn.Module):\n",
    "    '''\n",
    "    Same as cnocr/symbols/crnn.py/crnn_lstm_lite.\n",
    "    7-layer CNN + 2-layer LSTM\n",
    "    CNN reduce the width of image to width // 4 - 1, e.g. 560 => 139\n",
    "    '''\n",
    "    def __init__(self, dropout=0., rnn_hidden_size=100,\n",
    "                 batch_first=False, **kwargs):\n",
    "        super(CRNN, self).__init__(**kwargs)\n",
    "        self.batch_first = batch_first\n",
    "        # 7-layer Conv + 3-layer Pool\n",
    "        conv_kernel = [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (4, 1)]\n",
    "        conv_padding = [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (0, 0)]\n",
    "        conv_channel = [64, 128, 256, 512, 512, 512, 512]\n",
    "        # layer number of Pooling\n",
    "        pool_kernel = [(2, 2)] * 3\n",
    "        pool_stride = [(2, 2)] * 2 + [(2, 1)]\n",
    "        # Suppose input shape (N, C, H, W) is: (N, 1, 32, 560)\n",
    "        self.cnn = nn.Sequential(OrderedDict([\n",
    "            # => (N, 128, 32, 560)\n",
    "            ('ConvBlock-0', ConvReluBlock(\n",
    "                [1, conv_channel[0]], conv_channel[0:2],\n",
    "                conv_kernel[0:2], conv_padding[0:2])),\n",
    "            # => (N, 128, 16, 280)\n",
    "            ('Pool-0', nn.MaxPool2d(pool_kernel[0], pool_stride[0])),\n",
    "            # => (N, 512, 16, 280)\n",
    "            ('ConvBlock-1', ConvReluBlock(\n",
    "                conv_channel[1:3], conv_channel[2:4],\n",
    "                conv_kernel[2:4], conv_padding[2:4])),\n",
    "            # => (N, 512, 8, 140)\n",
    "            ('Pool-1', nn.MaxPool2d(pool_kernel[1], pool_stride[1])),\n",
    "            # => (N, 512, 8, 140)\n",
    "            ('BottleBlock-0', ConvReluBlock(\n",
    "                conv_channel[3:5], conv_channel[4:6],\n",
    "                conv_kernel[4:6], conv_padding[4:6],\n",
    "                bottle_conv=True, res_conn=True)),\n",
    "            # => (N, 512, 4, 139)\n",
    "            ('Pool-2', nn.MaxPool2d(pool_kernel[2], pool_stride[2])),\n",
    "            # => (N, 512, 1, 139)\n",
    "            ('BottleBlock-1', ConvReluBlock(\n",
    "                conv_channel[5:6], conv_channel[6:],\n",
    "                conv_kernel[6:], conv_padding[6:],\n",
    "                bottle_conv=True)),\n",
    "        ]))\n",
    "        if dropout > 0:\n",
    "            self.cnn.add_module('Dropout-0', nn.Dropout(dropout))\n",
    "        # 2-layer Bi-LSTM\n",
    "        self.rnn = nn.LSTM(input_size=conv_channel[-1],\n",
    "                           hidden_size=rnn_hidden_size,\n",
    "                           num_layers=2,\n",
    "                           batch_first=batch_first,\n",
    "                           bidirectional=True\n",
    "                          )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        conv = self.cnn(x)\n",
    "        # h == 1\n",
    "        b, c, h, w = conv.size()\n",
    "        assert h == 1, f'the output height of conv must be 1 instead of {h}'\n",
    "        if self.batch_first:\n",
    "            # => (B, width, channel) a.k.a. (B, seq_len, input_size)\n",
    "            # e.g. (N, 139, 512)\n",
    "            conv = conv.squeeze(2).permute(0, 2, 1)\n",
    "        else:\n",
    "            conv = conv.squeeze(2).permute(2, 0, 1)\n",
    "        # => (N, 139, 200) / (139, N, 200)\n",
    "        rnn_output, _ = self.rnn(conv)\n",
    "        return rnn_output\n",
    "    \n",
    "    \n",
    "class OCR(nn.Module):\n",
    "    def __init__(self, num_classes, dropout=0., rnn_hidden_size=100,\n",
    "                 batch_first=False, **kwargs):\n",
    "        super(OCR, self).__init__(**kwargs)\n",
    "        self.batch_first = batch_first\n",
    "        self.crnn = CRNN(dropout=dropout,\n",
    "                         rnn_hidden_size=rnn_hidden_size,\n",
    "                         batch_first=batch_first)\n",
    "        self.proj = nn.Linear(2 * rnn_hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        rnn_output = self.crnn(x)\n",
    "        if self.batch_first:\n",
    "            b, t, h = rnn_output.size()\n",
    "            rnn_output = rnn_output.contiguous()\n",
    "        else:\n",
    "            t, b, h = rnn_output.size()\n",
    "        # rnn with batch_first must .contiguous() before view()\n",
    "        # .contiguous() add extra cost.\n",
    "        output = self.proj(rnn_output.view(b * t, h))\n",
    "        if self.batch_first:\n",
    "            output = output.view(b, t, -1)\n",
    "        else:\n",
    "            output = output.view(t, b, -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#trainable parameters: 5520712\n",
      "OCR(\n",
      "  (crnn): CRNN(\n",
      "    (cnn): Sequential(\n",
      "      (ConvBlock-0): ConvReluBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): LeakyReLU(negative_slope=0.01)\n",
      "        )\n",
      "      )\n",
      "      (Pool-0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (ConvBlock-1): ConvReluBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (4): LeakyReLU(negative_slope=0.01)\n",
      "        )\n",
      "      )\n",
      "      (Pool-1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "      (BottleBlock-0): ConvReluBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (3): LeakyReLU(negative_slope=0.01)\n",
      "          (4): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (5): LeakyReLU(negative_slope=0.01)\n",
      "          (6): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (7): LeakyReLU(negative_slope=0.01)\n",
      "          (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (10): LeakyReLU(negative_slope=0.01)\n",
      "          (11): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (12): LeakyReLU(negative_slope=0.01)\n",
      "        )\n",
      "      )\n",
      "      (Pool-2): MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
      "      (BottleBlock-1): ConvReluBlock(\n",
      "        (block): Sequential(\n",
      "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (1): LeakyReLU(negative_slope=0.01)\n",
      "          (2): Conv2d(256, 256, kernel_size=(4, 1), stride=(1, 1))\n",
      "          (3): LeakyReLU(negative_slope=0.01)\n",
      "          (4): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "          (5): LeakyReLU(negative_slope=0.01)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(512, 100, num_layers=2, bidirectional=True)\n",
      "  )\n",
      "  (proj): Linear(in_features=200, out_features=5000, bias=True)\n",
      ") \n",
      " ================================================== \n",
      "\n",
      "crnn\n",
      "CPU times: user 24 ms, sys: 4 ms, total: 28 ms\n",
      "Wall time: 580 ms\n",
      "crnn_bf\n",
      "CPU times: user 16 ms, sys: 0 ns, total: 16 ms\n",
      "Wall time: 14.9 ms\n",
      "torch.Size([139, 8, 5000]) torch.Size([8, 139, 5000])\n"
     ]
    }
   ],
   "source": [
    "# sometimes crnn_bf even faster than crnn\n",
    "crnn_bf = OCR(5000, batch_first=True)\n",
    "crnn = OCR(5000, batch_first=False)\n",
    "print('#trainable parameters:', sum(p.numel() for p in crnn.parameters() if p.requires_grad))\n",
    "print(crnn, '\\n', '='*50, '\\n')\n",
    "inp = torch.randn(8, 1, 32, 560)\n",
    "print('crnn')\n",
    "%time out = crnn(inp)\n",
    "print('crnn_bf')\n",
    "%time out_bf = crnn_bf(inp)\n",
    "print(out.shape, out_bf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mxnet as mx\n",
    "from mxnet.gluon.rnn.rnn_layer import LSTM\n",
    "\n",
    "\n",
    "class CRNN_mxnet:\n",
    "    def __init__(self, num_classes, dropout=0., rnn_hidden_size=100,\n",
    "                 inference=True, img_width=560, num_label=20):\n",
    "        '''\n",
    "        same as conv-lite-lstm in CnOcr\n",
    "        :param inference: boolean\n",
    "            indicates evaluation without training\n",
    "        '''\n",
    "        # 560 => 140 - 1 = 139\n",
    "        seq_len_cmpr_ratio = 4\n",
    "        self.seq_len = img_width // seq_len_cmpr_ratio - 1\n",
    "        self.dropout = dropout\n",
    "        self.inference = inference\n",
    "        self.num_classes = num_classes\n",
    "        self.rnn_hidden_size = rnn_hidden_size\n",
    "    \n",
    "    def convRelu(self, idx, input_data, kernel_size, layer_size, padding_size,\n",
    "                 batch_norm=True):\n",
    "        layer = mx.symbol.Convolution(\n",
    "            name='conv-%d' % idx,\n",
    "            data=input_data,\n",
    "            kernel=kernel_size,\n",
    "            pad=padding_size,\n",
    "            num_filter=layer_size,\n",
    "        )\n",
    "        if batch_norm:\n",
    "            layer = mx.sym.BatchNorm(data=layer, name='batchnorm-%d' % idx)\n",
    "        layer = mx.sym.LeakyReLU(data=layer, name='leakyrelu-%d' % idx)\n",
    "        return layer\n",
    "    \n",
    "    def bottle_conv(self, idx, input_data, kernel_size, layer_size, padding_size,\n",
    "                    batch_norm=True):\n",
    "        bottle_channel = layer_size // 2\n",
    "        layer = mx.symbol.Convolution(\n",
    "            name='conv-%d-1-1x1' % idx,\n",
    "            data=input_data,\n",
    "            kernel=(1, 1),\n",
    "            pad=(0, 0),\n",
    "            num_filter=bottle_channel,\n",
    "        )\n",
    "        layer = mx.sym.LeakyReLU(data=layer, name='leakyrelu-%d-1' % idx)\n",
    "        layer = mx.symbol.Convolution(\n",
    "            name='conv-%d' % idx,\n",
    "            data=layer,\n",
    "            kernel=kernel_size,\n",
    "            pad=padding_size,\n",
    "            num_filter=bottle_channel,\n",
    "        )\n",
    "        layer = mx.sym.LeakyReLU(data=layer, name='leakyrelu-%d-2' % idx)\n",
    "        layer = mx.symbol.Convolution(\n",
    "            name='conv-%d-2-1x1' % idx,\n",
    "            data=layer,\n",
    "            kernel=(1, 1),\n",
    "            pad=(0, 0),\n",
    "            num_filter=layer_size,\n",
    "        )\n",
    "        if batch_norm:\n",
    "            layer = mx.sym.BatchNorm(data=layer, name='batchnorm-%d' % idx)\n",
    "        layer = mx.sym.LeakyReLU(data=layer, name='leakyrelu-%d' % idx)\n",
    "        return layer\n",
    "\n",
    "    def gen_network(self, data):\n",
    "        kernel_size = [(3, 3), (3, 3), (3, 3), (3, 3), (3, 3), (3, 3)]\n",
    "        padding_size = [(1, 1), (1, 1), (1, 1), (1, 1), (1, 1), (1, 1)]\n",
    "        layer_size = [min(32 * 2 ** (i + 1), 512) for i in range(len(kernel_size))]\n",
    "        \n",
    "        net = self.convRelu(\n",
    "            0, data, kernel_size[0], layer_size[0], padding_size[0]\n",
    "        )\n",
    "        net = self.convRelu(\n",
    "            1, net, kernel_size[1], layer_size[1], padding_size[1], True\n",
    "        )\n",
    "        net = mx.sym.Pooling(\n",
    "            data=net, name='pool-0', pool_type='max', kernel=(2, 2), stride=(2, 2)\n",
    "        )\n",
    "        net = self.convRelu(\n",
    "            2, net, kernel_size[2], layer_size[2], padding_size[2]\n",
    "        )\n",
    "        net = self.convRelu(\n",
    "            3, net, kernel_size[3], layer_size[3], padding_size[3], True\n",
    "        )\n",
    "        x = net = mx.sym.Pooling(\n",
    "            data=net, name='pool-1', pool_type='max', kernel=(2, 2), stride=(2, 2)\n",
    "        )\n",
    "        net = self.bottle_conv(4, net, kernel_size[4], layer_size[4], padding_size[4])\n",
    "        net = self.bottle_conv(5, net, kernel_size[5], layer_size[5], padding_size[5], True) + x\n",
    "        net = mx.symbol.Pooling(\n",
    "            data=net, name='pool-2', pool_type='max', kernel=(2, 2), stride=(2, 1)\n",
    "        )\n",
    "        net = self.bottle_conv(6, net, (4, 1), layer_size[5], (0, 0))\n",
    "        if self.dropout > 0.:\n",
    "            net = mx.symbol.Dropout(data=net, p=self.dropout)\n",
    "\n",
    "        # res: bz x emb_size x seq_len\n",
    "        net = mx.symbol.squeeze(net, axis=2)  \n",
    "        net = mx.symbol.transpose(net, axes=(2, 0, 1))\n",
    "        seq_model = LSTM(self.rnn_hidden_size, 2, bidirectional=True)\n",
    "        hidden_concat = seq_model(net)\n",
    "        return hidden_concat\n",
    "\n",
    "    def get_network(self, data=None):\n",
    "        # placeholder of input data\n",
    "        self.data = mx.sym.Variable('data')\n",
    "        # Note that the name of label is `label` instead of the \\\n",
    "        # default `softmax_label` in mxnet\n",
    "        self.label = mx.sym.Variable('label')\n",
    "        output = self.gen_network(self.data)\n",
    "        # => (seq_len * batch_size, rnn_hidden_size)\n",
    "        output = mx.symbol.reshape(output, shape=(-3, -2))\n",
    "        # => ((seq_len * batch_size), num_classes)\n",
    "        pred = mx.sym.FullyConnected(data=output,\n",
    "                                     num_hidden=self.num_classes,\n",
    "                                     name='pred_fc')\n",
    "        if self.inference:\n",
    "            return mx.sym.softmax(data=pred, name='softmax')\n",
    "        else:\n",
    "            # training with CTC loss\n",
    "            # => (seq_len, batch_size, num_classes)\n",
    "            pred_ctc = mx.sym.Reshape(data=pred, shape=(-4, self.seq_len, -1, 0))\n",
    "            loss = mx.sym.contrib.ctc_loss(data=pred_ctc, label=self.label)\n",
    "            ctc_loss = mx.sym.MakeLoss(loss)\n",
    "            softmax_class = mx.symbol.SoftmaxActivation(data=pred)\n",
    "            softmax_loss = mx.sym.MakeLoss(softmax_class)\n",
    "            softmax_loss = mx.sym.BlockGrad(softmax_loss)\n",
    "            return mx.sym.Group([softmax_loss, ctc_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def inference(samples, alphabet, batch_size,\n",
    "              network, data_shape, context, prefix, epoch):\n",
    "    '''\n",
    "    alphabet contains num_classes - 1 elements,\n",
    "    because it does not contain black token\n",
    "    '''\n",
    "    sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)\n",
    "    if network is not None:\n",
    "        sym = network\n",
    "    # DCMMC: useless codes\n",
    "#     pred_fc = sym.get_internals()['pred_fc_output']\n",
    "#     sym = mx.sym.softmax(data=pred_fc)\n",
    "    mod = mx.mod.Module(\n",
    "        symbol=sym, context=context, data_names=['data', ], label_names=None\n",
    "    )\n",
    "    mod.bind(for_training=False, data_shapes=data_shape)\n",
    "    # DCMMC: in jupyter environment, you need restart juputer kernel\n",
    "    # to release all the models before you create new model instances.\n",
    "    mod.set_params(arg_params, aux_params, allow_missing=False)\n",
    "    \n",
    "    mod.forward(samples)\n",
    "    prob = mod.get_outputs()[0].asnumpy()\n",
    "    # => (seq_len, batch_size, num_classes)\n",
    "    prob = np.reshape(prob, (-1, batch_size, prob.shape[1]))\n",
    "    res = []\n",
    "    for i in range(batch_size):\n",
    "        lp = np.argmax(prob[:, i, :], axis=-1)\n",
    "        res.append(''.join([\n",
    "            alphabet[ele - 1] for idx, ele in enumerate(\n",
    "                lp) if (lp[idx] and (idx == 0 or (lp[idx] != lp[idx - 1])))\n",
    "        ]))\n",
    "    return res\n",
    "\n",
    "\n",
    "class SimpleBatch(object):\n",
    "    def __init__(self, data_names, data, label_names=list(), label=list()):\n",
    "        self._data = data\n",
    "        self._label = label\n",
    "        self._data_names = data_names\n",
    "        self._label_names = label_names\n",
    "\n",
    "        self.pad = 0\n",
    "        self.index = None  # TODO: what is index?\n",
    "\n",
    "    @property\n",
    "    def data(self):\n",
    "        return self._data\n",
    "\n",
    "    @property\n",
    "    def label(self):\n",
    "        return self._label\n",
    "\n",
    "    @property\n",
    "    def data_names(self):\n",
    "        return self._data_names\n",
    "\n",
    "    @property\n",
    "    def label_names(self):\n",
    "        return self._label_names\n",
    "\n",
    "    @property\n",
    "    def provide_data(self):\n",
    "        return [(n, x.shape) for n, x in zip(self._data_names, self._data)]\n",
    "\n",
    "    @property\n",
    "    def provide_label(self):\n",
    "        return [(n, x.shape) for n, x in zip(self._label_names, self._label)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 1, 32, 560)\n",
      "['掉比悟厉。谌查门蠕坑', '.马靼蘑熨距颖猬要藕等']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "\n",
    "# cpu mode:\n",
    "# gpus = ''\n",
    "gpus = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "context = [mx.context.gpu(i) for i in range(len(gpus))] if len(gpus) else \\\n",
    "    [mx.context.cpu()]\n",
    "num_classes = 6426\n",
    "batch_size = 2\n",
    "img_size = (32, 560)\n",
    "network = CRNN_mxnet(num_classes, inference=True).get_network()\n",
    "data_shape = [('data', (batch_size, 1) + img_size)]\n",
    "prefix = '/data/xiaowentao/.cnocr/1.1.0/conv-lite-lstm/cnocr-v1.1.0-conv-lite-lstm'\n",
    "alphabet = {idx: v.strip() for idx, v in enumerate(\n",
    "    open('./cnocr/examples/label_cn.txt').readlines())}\n",
    "alphabet[len(alphabet) - 1] = alphabet[len(alphabet) - 1].replace('<space>', ' ')\n",
    "assert all([len(v) for v in alphabet.values()])\n",
    "# original version of cnocr is 45\n",
    "# epoch = 45\n",
    "epoch = 47\n",
    "\n",
    "imgs = [\n",
    "    np.array(\n",
    "        ImageOps.expand(\n",
    "            Image.open('./cnocr/examples/chn-00199981.jpg').convert('L'), (0, 0, 280, 0)),\n",
    "        dtype='float32'),\n",
    "    np.array(\n",
    "        ImageOps.expand(\n",
    "            Image.open('./cnocr/examples/chn-00199985.jpg').convert('L'), (0, 0, 280, 0)),\n",
    "        dtype='float32')\n",
    "]\n",
    "imgs = [img / 255. for img in imgs]\n",
    "imgs = mx.nd.expand_dims(mx.nd.array(imgs), 1)\n",
    "print(imgs.shape)\n",
    "samples = SimpleBatch(data_names=['data'], data=[imgs])\n",
    "res = inference(samples, alphabet, batch_size,\n",
    "                network, data_shape, context, prefix, epoch)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/xiaowentao/.local/lib/python3.7/site-packages/mxnet/gluon/block.py:1159: UserWarning: Cannot decide type for the following arguments. Consider providing them as input:\n",
      "\tdata: None\n",
      "  input_sym_arg_type = in_param.infer_type()[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs: (128, 1, 32, 560)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['让客户由衷的体验奢侈之旅，尤其是万元以上', '0821:39:02)专家[warren', '离开区足的很多教因，其中包括因在凶春因泰', '一边与身旁的夏煊泽开玩笑，要分一半奖品给'] \n",
      " ['让客户由衷的体验奢侈之旅，尤其是万元以上', '0821:39:02)专家[warren', '离开辽足的很多教练，其中包括现在长春亚泰', '一边与身旁的夏煊泽开玩笑，要分一半奖品给']\n"
     ]
    }
   ],
   "source": [
    "from thinc.api import MXNetWrapper, chain, Softmax, prefer_gpu\n",
    "import mxnet as mx\n",
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm.notebook import tqdm\n",
    "from mxnet.gluon import SymbolBlock\n",
    "import cupy\n",
    "\n",
    "gpus = '2'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "is_gpu = prefer_gpu()\n",
    "print(\"GPU:\", is_gpu)\n",
    "context = [mx.context.gpu(i) for i in range(len(gpus))] if len(gpus) else \\\n",
    "    [mx.context.cpu()]\n",
    "num_classes = 6426\n",
    "crnn_instance = CRNN_mxnet(num_classes, inference=True)\n",
    "network = crnn_instance.get_network()\n",
    "input_symbol = crnn_instance.data\n",
    "prefix = '/data/xiaowentao/.cnocr/1.1.0/conv-lite-lstm/cnocr-v1.1.0-conv-lite-lstm'\n",
    "epoch = 47\n",
    "batch_size = 32\n",
    "data_shape = [('data', (batch_size, 1, 32, 560))]\n",
    "label_width = 20\n",
    "\n",
    "pred_fc = network.get_internals()['pred_fc_output']\n",
    "sym = mx.sym.softmax(data=pred_fc)\n",
    "# It seems that Thinc Shim only support mxnet Gluon.\n",
    "# Therefore, we first wrap the Symbol network into Gluon Block.\n",
    "network = SymbolBlock(outputs=sym, inputs=input_symbol)\n",
    "# load the parameetrs!\n",
    "network.collect_params().load('%s-%04d' % (prefix, epoch) + '.params',\n",
    "                              ctx=context)\n",
    "# Yet another way!\n",
    "# with open('/tmp/crnn.json', 'w') as f:\n",
    "#     f.write(sym.tojson())\n",
    "# network = SymbolBlock.imports('/tmp/crnn.json', ['data'],\n",
    "#                               '%s-%04d' % (prefix, epoch) + '.params',\n",
    "#                               ctx=context)\n",
    "network.hybridize(static_alloc=True, static_shape=True)\n",
    "\n",
    "# MXNet doesn't provide a Softmax layer but a .softmax() operation/method for \\\n",
    "# prediction and it integrates an internal softmax during training. So to be able\\\n",
    "# to integrate it with the rest of the components, you combine it with a Softmax() \\\n",
    "# Thinc layer using the chain combinator.\n",
    "wrapper_mxnet_crnn = MXNetWrapper(network)\n",
    "# mx_model = chain(wrapper_mxnet_crnn, Softmax())\n",
    "mx_model = wrapper_mxnet_crnn\n",
    "\n",
    "imgs = []\n",
    "golds = []\n",
    "num_batch = 4\n",
    "with h5py.File('./data_generated/dataset_fonts.h5', 'r') as d:\n",
    "    for idx in range(batch_size * num_batch):\n",
    "        idx = str(idx)\n",
    "        imgs.append(d[idx]['img'][...] / 255.)\n",
    "        label = str(d[idx]['y'][...])\n",
    "        golds.append(label)\n",
    "imgs = np.expand_dims(np.array(imgs, dtype=np.float32), axis=1)\n",
    "print('imgs:', imgs.shape)\n",
    "imgs = mx_model.ops.asarray(imgs, dtype='float32')\n",
    "alphabet = {(idx+1): tok.strip() for idx, tok in enumerate(\n",
    "    open('./cnocr/examples/label_cn.txt').readlines())}\n",
    "alphabet[len(alphabet)] = ' '\n",
    "# blank token\n",
    "alphabet[0] = '#'\n",
    "assert [len(tok) == 1 for tok in alphabet.values()]\n",
    "alphabet_inv = {v: k for k, v in alphabet.items()}\n",
    "golds_ids = [[alphabet_inv[c] for c in g] for g in golds]\n",
    "golds_ids = [g + [0] * (label_width - len(g)) for g in golds_ids]\n",
    "golds_ids = np.array(golds_ids, dtype=np.int32)\n",
    "golds_ids = mx_model.ops.asarray(golds_ids, dtype='float32')\n",
    "\n",
    "# mx_model.initialize(X=imgs[:4], Y=np.ones([4 * 139, num_classes], dtype=np.float32)[:4])\n",
    "batches = mx_model.ops.multibatch(batch_size, imgs, golds_ids, shuffle=True)\n",
    "\n",
    "res = []\n",
    "ground = []\n",
    "for X, Y in tqdm(batches, leave=False):\n",
    "    Yh = mx_model.predict(X)\n",
    "    assert Yh.shape == (batch_size * 139, num_classes)\n",
    "    Yh = cupy.asnumpy(Yh)\n",
    "    Y = cupy.asnumpy(Y)\n",
    "    prob = np.reshape(Yh, (-1, batch_size, Yh.shape[1]))\n",
    "    for i in range(batch_size):\n",
    "        lp = np.argmax(prob[:, i, :], axis=-1)\n",
    "#         print(lp[:10])\n",
    "        res.append(''.join([\n",
    "            alphabet[ele] for idx, ele in enumerate(\n",
    "                lp) if (lp[idx] and (idx == 0 or (lp[idx] != lp[idx - 1])))\n",
    "        ]))\n",
    "        ground.append(''.join([alphabet[c] for c in Y[i]]))\n",
    "print(res[:4], '\\n', ground[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 26 00:31:26 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 430.14       Driver Version: 430.14       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:18:00.0  On |                  N/A |\n",
      "| 22%   32C    P2    61W / 250W |   2272MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:3B:00.0  On |                  N/A |\n",
      "| 22%   37C    P2    59W / 250W |   4287MiB / 11019MiB |      2%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:86:00.0  On |                  N/A |\n",
      "| 22%   30C    P8    21W / 250W |   2902MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:AF:00.0  On |                  N/A |\n",
      "| 25%   45C    P2    87W / 250W |   8680MiB / 11019MiB |      3%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0    155629      C   python                                      2221MiB |\n",
      "|    0    243184      G   /usr/lib/xorg/Xorg                            39MiB |\n",
      "|    1    239219      C   python                                      2467MiB |\n",
      "|    1    243184      G   /usr/lib/xorg/Xorg                            15MiB |\n",
      "|    1    256138      C   python                                      1793MiB |\n",
      "|    2    243184      G   /usr/lib/xorg/Xorg                            15MiB |\n",
      "|    2    256939      C   /data/xiaowentao/.anaconda3/bin/python      2875MiB |\n",
      "|    3    104849      C   python                                      8653MiB |\n",
      "|    3    243184      G   /usr/lib/xorg/Xorg                            15MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
