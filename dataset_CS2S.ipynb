{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr\n",
      "100%|███████████████████████████████████████████| 20/20 [00:01<00:00, 14.07it/s]\n"
     ]
    }
   ],
   "source": [
    "%cd /amax/data/xiaowentao/chineseocr/\n",
    "# 20 images: zh, wikipedia, horizontal, random length with max height 20\n",
    "!trdg --output_dir data_generated/test --count 20 --language cn --length 20 \\\n",
    "    --format 40 --thread_count 20 --skew_angle 1 --random_skew \\\n",
    "    --background 0 --distorsion 3 --space_width 0 \\\n",
    "    --text_color '#000000,#888888' --name_format 2 --character_spacing 6 \\\n",
    "    --input_file THUCNews/娱乐/224227.txt --fit -fd ./data_generated/font_files \\\n",
    "    -ft `ls ./data_generated/font_files | sort -R | head -n 1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'cn-fonts-2'...\n",
      "remote: Enumerating objects: 33, done.\u001b[K\n",
      "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
      "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
      "remote: Total 33 (delta 0), reused 31 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (33/33), done.\n"
     ]
    }
   ],
   "source": [
    "!data_generated/fonts && \\\n",
    "    git clone --depth=1 --recursive https://github.com/yuleshow/chinese-fonts && \\\n",
    "    git clone --depth=1 --recursive https://github.com/Apollys/chinese-fonts cn-fonts-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'fonts/chinese-fonts/居中標點的秀月/TH-SiuNiu-Bold.ttf' -> 'font_files/TH-SiuNiu-Bold.ttf'\r\n",
      "'fonts/chinese-fonts/居中標點的秀月/TH-SiuNiu-Regular.ttf' -> 'font_files/TH-SiuNiu-Regular.ttf'\r\n"
     ]
    }
   ],
   "source": [
    "!cd data_generated && find fonts -name '*.*tf' -exec cp -vf -t font_files {} +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\r\n",
      "GenYoGothicTW-Medium.ttf\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data_generated/font_files | wc -l && ls ./data_generated/font_files | sort -R | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: trdg [-h] [--output_dir [OUTPUT_DIR]] [-i [INPUT_FILE]] [-l [LANGUAGE]]\r\n",
      "            -c [COUNT] [-rs] [-let] [-num] [-sym] [-w [LENGTH]] [-r]\r\n",
      "            [-f [FORMAT]] [-t [THREAD_COUNT]] [-e [EXTENSION]]\r\n",
      "            [-k [SKEW_ANGLE]] [-rk] [-wk] [-bl [BLUR]] [-rbl]\r\n",
      "            [-b [BACKGROUND]] [-hw] [-na NAME_FORMAT] [-om OUTPUT_MASK]\r\n",
      "            [-d [DISTORSION]] [-do [DISTORSION_ORIENTATION]] [-wd [WIDTH]]\r\n",
      "            [-al [ALIGNMENT]] [-or [ORIENTATION]] [-tc [TEXT_COLOR]]\r\n",
      "            [-sw [SPACE_WIDTH]] [-cs [CHARACTER_SPACING]] [-m [MARGINS]] [-fi]\r\n",
      "            [-ft [FONT]] [-fd [FONT_DIR]] [-ca [CASE]] [-dt [DICT]] [-ws]\r\n",
      "\r\n",
      "Generate synthetic text data for text recognition.\r\n",
      "\r\n",
      "optional arguments:\r\n",
      "  -h, --help            show this help message and exit\r\n",
      "  --output_dir [OUTPUT_DIR]\r\n",
      "                        The output directory\r\n",
      "  -i [INPUT_FILE], --input_file [INPUT_FILE]\r\n",
      "                        When set, this argument uses a specified text file as\r\n",
      "                        source for the text\r\n",
      "  -l [LANGUAGE], --language [LANGUAGE]\r\n",
      "                        The language to use, should be fr (French), en\r\n",
      "                        (English), es (Spanish), de (German), or cn (Chinese).\r\n",
      "  -c [COUNT], --count [COUNT]\r\n",
      "                        The number of images to be created.\r\n",
      "  -rs, --random_sequences\r\n",
      "                        Use random sequences as the source text for the\r\n",
      "                        generation. Set '-let','-num','-sym' to use\r\n",
      "                        letters/numbers/symbols. If none specified, using all\r\n",
      "                        three.\r\n",
      "  -let, --include_letters\r\n",
      "                        Define if random sequences should contain letters.\r\n",
      "                        Only works with -rs\r\n",
      "  -num, --include_numbers\r\n",
      "                        Define if random sequences should contain numbers.\r\n",
      "                        Only works with -rs\r\n",
      "  -sym, --include_symbols\r\n",
      "                        Define if random sequences should contain symbols.\r\n",
      "                        Only works with -rs\r\n",
      "  -w [LENGTH], --length [LENGTH]\r\n",
      "                        Define how many words should be included in each\r\n",
      "                        generated sample. If the text source is Wikipedia,\r\n",
      "                        this is the MINIMUM length\r\n",
      "  -r, --random          Define if the produced string will have variable word\r\n",
      "                        count (with --length being the maximum)\r\n",
      "  -f [FORMAT], --format [FORMAT]\r\n",
      "                        Define the height of the produced images if\r\n",
      "                        horizontal, else the width\r\n",
      "  -t [THREAD_COUNT], --thread_count [THREAD_COUNT]\r\n",
      "                        Define the number of thread to use for image\r\n",
      "                        generation\r\n",
      "  -e [EXTENSION], --extension [EXTENSION]\r\n",
      "                        Define the extension to save the image with\r\n",
      "  -k [SKEW_ANGLE], --skew_angle [SKEW_ANGLE]\r\n",
      "                        Define skewing angle of the generated text. In\r\n",
      "                        positive degrees\r\n",
      "  -rk, --random_skew    When set, the skew angle will be randomized between\r\n",
      "                        the value set with -k and it's opposite\r\n",
      "  -wk, --use_wikipedia  Use Wikipedia as the source text for the generation,\r\n",
      "                        using this paremeter ignores -r, -n, -s\r\n",
      "  -bl [BLUR], --blur [BLUR]\r\n",
      "                        Apply gaussian blur to the resulting sample. Should be\r\n",
      "                        an integer defining the blur radius\r\n",
      "  -rbl, --random_blur   When set, the blur radius will be randomized between 0\r\n",
      "                        and -bl.\r\n",
      "  -b [BACKGROUND], --background [BACKGROUND]\r\n",
      "                        Define what kind of background to use. 0: Gaussian\r\n",
      "                        Noise, 1: Plain white, 2: Quasicrystal, 3: Pictures\r\n",
      "  -hw, --handwritten    Define if the data will be \"handwritten\" by an RNN\r\n",
      "  -na NAME_FORMAT, --name_format NAME_FORMAT\r\n",
      "                        Define how the produced files will be named. 0:\r\n",
      "                        [TEXT]_[ID].[EXT], 1: [ID]_[TEXT].[EXT] 2: [ID].[EXT]\r\n",
      "                        + one file labels.txt containing id-to-label mappings\r\n",
      "  -om OUTPUT_MASK, --output_mask OUTPUT_MASK\r\n",
      "                        Define if the generator will return masks for the text\r\n",
      "  -d [DISTORSION], --distorsion [DISTORSION]\r\n",
      "                        Define a distorsion applied to the resulting image. 0:\r\n",
      "                        None (Default), 1: Sine wave, 2: Cosine wave, 3:\r\n",
      "                        Random\r\n",
      "  -do [DISTORSION_ORIENTATION], --distorsion_orientation [DISTORSION_ORIENTATION]\r\n",
      "                        Define the distorsion's orientation. Only used if -d\r\n",
      "                        is specified. 0: Vertical (Up and down), 1: Horizontal\r\n",
      "                        (Left and Right), 2: Both\r\n",
      "  -wd [WIDTH], --width [WIDTH]\r\n",
      "                        Define the width of the resulting image. If not set it\r\n",
      "                        will be the width of the text + 10. If the width of\r\n",
      "                        the generated text is bigger that number will be used\r\n",
      "  -al [ALIGNMENT], --alignment [ALIGNMENT]\r\n",
      "                        Define the alignment of the text in the image. Only\r\n",
      "                        used if the width parameter is set. 0: left, 1:\r\n",
      "                        center, 2: right\r\n",
      "  -or [ORIENTATION], --orientation [ORIENTATION]\r\n",
      "                        Define the orientation of the text. 0: Horizontal, 1:\r\n",
      "                        Vertical\r\n",
      "  -tc [TEXT_COLOR], --text_color [TEXT_COLOR]\r\n",
      "                        Define the text's color, should be either a single hex\r\n",
      "                        color or a range in the ?,? format.\r\n",
      "  -sw [SPACE_WIDTH], --space_width [SPACE_WIDTH]\r\n",
      "                        Define the width of the spaces between words. 2.0\r\n",
      "                        means twice the normal space width\r\n",
      "  -cs [CHARACTER_SPACING], --character_spacing [CHARACTER_SPACING]\r\n",
      "                        Define the width of the spaces between characters. 2\r\n",
      "                        means two pixels\r\n",
      "  -m [MARGINS], --margins [MARGINS]\r\n",
      "                        Define the margins around the text when rendered. In\r\n",
      "                        pixels\r\n",
      "  -fi, --fit            Apply a tight crop around the rendered text\r\n",
      "  -ft [FONT], --font [FONT]\r\n",
      "                        Define font to be used\r\n",
      "  -fd [FONT_DIR], --font_dir [FONT_DIR]\r\n",
      "                        Define a font directory to be used\r\n",
      "  -ca [CASE], --case [CASE]\r\n",
      "                        Generate upper or lowercase only. arguments: upper or\r\n",
      "                        lower. Example: --case upper\r\n",
      "  -dt [DICT], --dict [DICT]\r\n",
      "                        Define the dictionary to be used\r\n",
      "  -ws, --word_split     Split on words instead of on characters (preserves\r\n",
      "                        ligatures, no character spacing)\r\n"
     ]
    }
   ],
   "source": [
    "!trdg --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 7, done.\u001b[K\n",
      "remote: Counting objects: 100% (7/7), done.\u001b[K\n",
      "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
      "remote: Total 4 (delta 3), reused 4 (delta 3), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (4/4), done.\n",
      "From https://github.com/DCMMC/TextRecognitionDataGenerator\n",
      "   94b92f7..29ac8e2  master     -> origin/master\n",
      "Updating 94b92f7..29ac8e2\n",
      "Fast-forward\n",
      " trdg/string_generator.py | 3 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n",
      " 1 file changed, 2 insertions(+), 1 deletion(-)\n",
      "Processing /amax/data/xiaowentao/chineseocr/TextRecognitionDataGenerator\n",
      "Requirement already satisfied: pillow==7.0.0 in /amax/data/xiaowentao/.anaconda3/lib/python3.7/site-packages (from trdg==1.5.0->-r requirements.txt (line 1)) (7.0.0)\n",
      "Requirement already satisfied: numpy<1.17,>=1.16.4 in /amax/data/xiaowentao/.local/lib/python3.7/site-packages (from trdg==1.5.0->-r requirements.txt (line 1)) (1.16.6)\n",
      "Requirement already satisfied: requests>=2.20.0 in /amax/data/xiaowentao/.anaconda3/lib/python3.7/site-packages (from trdg==1.5.0->-r requirements.txt (line 1)) (2.23.0)\n",
      "Requirement already satisfied: opencv-python>=4.2.0.32 in /amax/data/xiaowentao/.local/lib/python3.7/site-packages (from trdg==1.5.0->-r requirements.txt (line 1)) (4.2.0.34)\n",
      "Requirement already satisfied: tqdm>=4.23.0 in /amax/data/xiaowentao/.local/lib/python3.7/site-packages (from trdg==1.5.0->-r requirements.txt (line 1)) (4.29.1)\n",
      "Requirement already satisfied: beautifulsoup4>=4.6.0 in /amax/data/xiaowentao/.anaconda3/lib/python3.7/site-packages (from trdg==1.5.0->-r requirements.txt (line 1)) (4.8.2)\n",
      "Requirement already satisfied: diffimg==0.2.3 in /amax/data/xiaowentao/.local/lib/python3.7/site-packages (from trdg==1.5.0->-r requirements.txt (line 1)) (0.2.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /amax/data/xiaowentao/.anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->trdg==1.5.0->-r requirements.txt (line 1)) (2.9)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /amax/data/xiaowentao/.anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->trdg==1.5.0->-r requirements.txt (line 1)) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /amax/data/xiaowentao/.anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->trdg==1.5.0->-r requirements.txt (line 1)) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /amax/data/xiaowentao/.anaconda3/lib/python3.7/site-packages (from requests>=2.20.0->trdg==1.5.0->-r requirements.txt (line 1)) (2019.11.28)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /amax/data/xiaowentao/.anaconda3/lib/python3.7/site-packages (from beautifulsoup4>=4.6.0->trdg==1.5.0->-r requirements.txt (line 1)) (2.0)\n",
      "Building wheels for collected packages: trdg\n",
      "  Building wheel for trdg (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for trdg: filename=trdg-1.5.0-py3-none-any.whl size=47598340 sha256=6fb2656876df5b592d8d079dc62775782faf95f58b9c28a220e815375873c422\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-49c0_epg/wheels/b7/e7/23/4b522edc293cfb372d0b33cd68b22502c8078a4ba3157e339f\n",
      "Successfully built trdg\n",
      "Installing collected packages: trdg\n",
      "  Attempting uninstall: trdg\n",
      "    Found existing installation: trdg 1.5.0\n",
      "    Uninstalling trdg-1.5.0:\n",
      "      Successfully uninstalled trdg-1.5.0\n",
      "Successfully installed trdg-1.5.0\n"
     ]
    }
   ],
   "source": [
    "!cd TextRecognitionDataGenerator/ && git pull && pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "张杰新歌《第一夫人》曝光 唱出爱情宣言(组图)\r\n",
      "　　>>点此试听：张杰《第一夫人》\r\n",
      "　　新浪娱乐讯 9月26日，张杰(微博)在自己与谢娜(微博)的婚礼上，首度演唱了新专辑第二主打歌《第一夫人》。《第一夫人》由王力宏(微博)作曲、姚若龙作词、陈子鸿制作。据悉，这首歌正是王力宏获悉张杰即将迈入人生全新阶段后灵感涌现创作而成的。\r\n",
      "　　王力宏表示，“这个世界上每一位为生活和事业打拼，为名利和地位奔波，为理想和未来忙碌的男人，只要他找到了那个无论贫富贵贱都愿意和他牵手陪伴他一生的女人，他就是这个世界上比总统还要成功的男人。”\r\n",
      "　　张杰则表示，这首歌不但是对自己爱情的一个鼓励，同时也是代表天下所有男人，对与自己“执子之手，与子偕老”的那个女孩，表达的一份发自生命的感谢。\r\n",
      "　　歌词：\r\n",
      "　　第一夫人\r\n",
      "　　作  曲：王力宏  \r\n",
      "　　作  词：姚若龙 王力宏\r\n",
      "　　制作人：陈子鸿\r\n",
      "　　演  唱：张  杰\r\n",
      "　　Hey, you know what you are my first lady\r\n",
      "　　我爱你\r\n",
      "　　透过生活进入生命\r\n",
      "　　爱你\r\n",
      "　　不是说说而已\r\n",
      "　　我爱你\r\n",
      "　　每个明天都会更感人\r\n",
      "　　我的第一夫人\r\n",
      "　　再也不要飘在人海里\r\n",
      "　　一个人流离\r\n",
      "　　再也不要怕恶梦来袭\r\n",
      "　　一个人惊醒\r\n",
      "　　清干净\r\n",
      "　　旧伤心\r\n",
      "　　美满将要造访并定居\r\n",
      "　　我爱你\r\n",
      "　　透过生活进入生命\r\n",
      "　　爱你\r\n",
      "　　不是说说而已\r\n",
      "　　我爱你\r\n",
      "　　每个明天都会更感人\r\n",
      "　　我的第一夫人\r\n",
      "　　再也不用一个人旅行\r\n",
      "　　浪漫的空虚\r\n",
      "　　再也不用一个人委屈\r\n",
      "　　心酸的独立\r\n",
      "　　就放心\r\n",
      "　　被疼惜\r\n",
      "　　信任酿的幸福最浓郁\r\n",
      "　　我爱你\r\n",
      "　　透过生活进入生命\r\n",
      "　　爱你\r\n",
      "　　不是说说而已\r\n",
      "　　我爱你\r\n",
      "　　每个明天都会更感人\r\n",
      "　　我的第一夫人\r\n",
      "　　我爱你\r\n",
      "　　透过生活进入生命\r\n",
      "　　爱你\r\n",
      "　　不是说说而已\r\n",
      "　　我爱你\r\n",
      "　　每个明天都会更感人\r\n",
      "　　我的第一夫人\r\n",
      "　　我的第一夫人\r\n",
      "　　我的第一夫人\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!cd THUCNews/娱乐 && cat 224227.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#classes=14\n",
      "体育 : 131604\n",
      "娱乐 : 92632\n",
      "家居 : 32586\n",
      "彩票 : 7588\n",
      "房产 : 20050\n",
      "教育 : 41936\n",
      "时尚 : 13368\n",
      "时政 : 63086\n",
      "星座 : 3578\n",
      "游戏 : 24373\n",
      "社会 : 50849\n",
      "科技 : 162929\n",
      "股票 : 154398\n",
      "财经 : 37098\n",
      "{'体育': {'file': 131604, 'line': 1551607}, '娱乐': {'file': 92632, 'line': 932170}, '家居': {'file': 32586, 'line': 303249}, '彩票': {'file': 7588, 'line': 119156}, '房产': {'file': 20050, 'line': 270363}, '教育': {'file': 41936, 'line': 761328}, '时尚': {'file': 13368, 'line': 155608}, '时政': {'file': 63086, 'line': 615980}, '星座': {'file': 3578, 'line': 47894}, '游戏': {'file': 24373, 'line': 342098}, '社会': {'file': 50849, 'line': 832729}, '科技': {'file': 162929, 'line': 1962311}, '股票': {'file': 154398, 'line': 1908180}, '财经': {'file': 37098, 'line': 548956}} time: 168.60633087158203\n"
     ]
    }
   ],
   "source": [
    "# statistical\n",
    "import os\n",
    "from itertools import (takewhile,repeat)\n",
    "from time import time\n",
    "\n",
    "def count_lines(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    bufgen = takewhile(lambda x: x, (f.raw.read(1024*1024) for _ in repeat(None)))\n",
    "    return sum(buf.count(b'\\n') for buf in bufgen)\n",
    "\n",
    "base = 'THUCNews'\n",
    "s_t = time()\n",
    "classes = os.listdir(base)\n",
    "print('#classes={}'.format(len(classes)))\n",
    "classes_stat = {c: {'file': 0, 'line': 0} for c in classes}\n",
    "for cls in classes:\n",
    "    fs = os.listdir(base + '/' + cls)\n",
    "    print(cls, ':', len(fs))\n",
    "    for f in fs:\n",
    "        classes_stat[cls]['line'] += count_lines(base + '/' + cls + '/' + f)\n",
    "        classes_stat[cls]['file'] += 1\n",
    "print(classes_stat, 'time:', time() - s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file need: {'体育': 2544, '娱乐': 2981, '家居': 3223, '彩票': 1910, '房产': 2224, '教育': 1652, '时尚': 2577, '时政': 3072, '星座': 2241, '游戏': 2137, '社会': 1831, '科技': 2490, '股票': 2427, '财经': 2027}\n",
      "#classes=14\n",
      "体育 : 131604\n",
      "娱乐 : 92632\n",
      "家居 : 32586\n",
      "彩票 : 7588\n",
      "房产 : 20050\n",
      "教育 : 41936\n",
      "时尚 : 13368\n",
      "时政 : 63086\n",
      "星座 : 3578\n",
      "游戏 : 24373\n",
      "社会 : 50849\n",
      "科技 : 162929\n",
      "股票 : 154398\n",
      "财经 : 37098\n",
      "{'体育': {'file': 2544, 'line': 28511}, '娱乐': {'file': 2981, 'line': 37139}, '家居': {'file': 3223, 'line': 13211}, '彩票': {'file': 1910, 'line': 26526}, '房产': {'file': 2224, 'line': 44615}, '教育': {'file': 1652, 'line': 26757}, '时尚': {'file': 2577, 'line': 26885}, '时政': {'file': 3072, 'line': 29251}, '星座': {'file': 2241, 'line': 19222}, '游戏': {'file': 2137, 'line': 27915}, '社会': {'file': 1831, 'line': 21279}, '科技': {'file': 2490, 'line': 33378}, '股票': {'file': 2427, 'line': 30318}, '财经': {'file': 2027, 'line': 36654}} time: 1.0347533226013184\n"
     ]
    }
   ],
   "source": [
    "file_need = {k: int(v['file'] * min(30000 / v['line'], 1)) for k, v in classes_stat.items()}\n",
    "print('file need:', file_need)\n",
    "\n",
    "# statistical\n",
    "import os\n",
    "from itertools import (takewhile,repeat)\n",
    "from time import time\n",
    "\n",
    "def count_lines(filename):\n",
    "    f = open(filename, 'rb')\n",
    "    bufgen = takewhile(lambda x: x, (f.raw.read(1024*1024) for _ in repeat(None)))\n",
    "    return sum(buf.count(b'\\n') for buf in bufgen)\n",
    "\n",
    "base = 'THUCNews'\n",
    "s_t = time()\n",
    "classes = os.listdir(base)\n",
    "print('#classes={}'.format(len(classes)))\n",
    "classes_stat_new = {c: {'file': 0, 'line': 0} for c in classes}\n",
    "for cls in classes:\n",
    "    fs = os.listdir(base + '/' + cls)\n",
    "    print(cls, ':', len(fs))\n",
    "    for f in fs:\n",
    "        classes_stat_new[cls]['line'] += count_lines(base + '/' + cls + '/' + f)\n",
    "        classes_stat_new[cls]['file'] += 1\n",
    "        if classes_stat_new[cls]['file'] >= file_need[cls]:\n",
    "            break\n",
    "print(classes_stat_new, 'time:', time() - s_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples: 401661\n"
     ]
    }
   ],
   "source": [
    "print('total samples:', sum([v['line'] for v in classes_stat_new.values()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7624175548553467s, #samples=4498\n",
      "100%|██████████| 4498/4498 [00:07<00:00, 614.64it/s]\n",
      "\n",
      "fold 0 took 11.218741655349731s, #samples=4498\n",
      "store to h5 file took 10.450234174728394s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7658288478851318s, #samples=4550\n",
      "100%|██████████| 4550/4550 [00:07<00:00, 620.74it/s]\n",
      "\n",
      "fold 1 took 11.284970998764038s, #samples=4550\n",
      "store to h5 file took 10.455467939376831s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7115452289581299s, #samples=4103\n",
      "100%|██████████| 4103/4103 [00:06<00:00, 617.65it/s]\n",
      "\n",
      "fold 2 took 10.524644374847412s, #samples=4103\n",
      "store to h5 file took 9.370426177978516s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4711592197418213s, #samples=5462\n",
      "100%|██████████| 5462/5462 [00:08<00:00, 619.34it/s]\n",
      "\n",
      "fold 3 took 12.510026454925537s, #samples=5462\n",
      "store to h5 file took 12.386886596679688s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2851388454437256s, #samples=5876\n",
      "100%|██████████| 5876/5876 [00:09<00:00, 632.04it/s]\n",
      "\n",
      "fold 4 took 13.346719026565552s, #samples=5876\n",
      "store to h5 file took 13.239306449890137s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.31249547004699707s, #samples=6595\n",
      "100%|██████████| 6595/6595 [00:10<00:00, 624.05it/s]\n",
      "\n",
      "fold 5 took 14.031310796737671s, #samples=6595\n",
      "store to h5 file took 14.162179708480835s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.519960880279541s, #samples=5043\n",
      "100%|██████████| 5043/5043 [00:07<00:00, 634.98it/s]\n",
      "\n",
      "fold 6 took 11.60740065574646s, #samples=5043\n",
      "store to h5 file took 11.93859314918518s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.1527693271636963s, #samples=5512\n",
      "100%|██████████| 5512/5512 [00:08<00:00, 630.24it/s]\n",
      "\n",
      "fold 7 took 12.338794231414795s, #samples=5512\n",
      "store to h5 file took 12.61595630645752s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.28673481941223145s, #samples=5618\n",
      "100%|██████████| 5618/5618 [00:09<00:00, 615.09it/s]\n",
      "\n",
      "fold 8 took 12.609614133834839s, #samples=5618\n",
      "store to h5 file took 12.989195108413696s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.11540675163269043s, #samples=5082\n",
      "100%|██████████| 5082/5082 [00:08<00:00, 600.80it/s]\n",
      "\n",
      "fold 9 took 12.004398822784424s, #samples=5082\n",
      "store to h5 file took 12.553675889968872s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.18877601623535156s, #samples=4955\n",
      "100%|██████████| 4955/4955 [00:08<00:00, 614.80it/s]\n",
      "\n",
      "fold 10 took 11.591034173965454s, #samples=4955\n",
      "store to h5 file took 11.41966462135315s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.20976471900939941s, #samples=4188\n",
      "100%|██████████| 4188/4188 [00:06<00:00, 618.32it/s]\n",
      "\n",
      "fold 11 took 10.503248929977417s, #samples=4188\n",
      "store to h5 file took 10.372246026992798s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3024415969848633s, #samples=5543\n",
      "100%|██████████| 5543/5543 [00:09<00:00, 577.89it/s]\n",
      "\n",
      "fold 12 took 13.175301313400269s, #samples=5543\n",
      "store to h5 file took 12.550524234771729s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2663118839263916s, #samples=5323\n",
      "100%|██████████| 5323/5323 [00:08<00:00, 632.09it/s]\n",
      "\n",
      "fold 13 took 12.121493577957153s, #samples=5323\n",
      "store to h5 file took 11.960870265960693s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2943429946899414s, #samples=3989\n",
      "100%|██████████| 3989/3989 [00:06<00:00, 609.39it/s]\n",
      "\n",
      "fold 14 took 10.053729772567749s, #samples=3989\n",
      "store to h5 file took 9.492578744888306s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.47164416313171387s, #samples=5110\n",
      "100%|██████████| 5110/5110 [00:08<00:00, 622.50it/s]\n",
      "\n",
      "fold 15 took 11.595072031021118s, #samples=5110\n",
      "store to h5 file took 11.562011957168579s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.15875959396362305s, #samples=4903\n",
      "100%|██████████| 4903/4903 [00:07<00:00, 628.09it/s]\n",
      "\n",
      "fold 16 took 11.506710529327393s, #samples=4903\n",
      "store to h5 file took 11.530436515808105s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2604691982269287s, #samples=5020\n",
      "100%|██████████| 5020/5020 [00:07<00:00, 669.86it/s]\n",
      "\n",
      "fold 17 took 11.282196283340454s, #samples=5020\n",
      "store to h5 file took 11.143302202224731s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7260308265686035s, #samples=4572\n",
      "100%|██████████| 4572/4572 [00:07<00:00, 600.51it/s]\n",
      "\n",
      "fold 18 took 11.316248416900635s, #samples=4572\n",
      "store to h5 file took 11.327911853790283s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.1631166934967041s, #samples=4544\n",
      "100%|██████████| 4544/4544 [00:07<00:00, 598.60it/s]\n",
      "\n",
      "fold 19 took 11.393677949905396s, #samples=4544\n",
      "store to h5 file took 10.210641384124756s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.28142523765563965s, #samples=4561\n",
      "100%|██████████| 4561/4561 [00:07<00:00, 617.95it/s]\n",
      "\n",
      "fold 20 took 11.319067001342773s, #samples=4561\n",
      "store to h5 file took 10.395281791687012s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2785773277282715s, #samples=5124\n",
      "100%|██████████| 5124/5124 [00:08<00:00, 626.94it/s]\n",
      "\n",
      "fold 21 took 12.01873517036438s, #samples=5124\n",
      "store to h5 file took 11.876676797866821s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.1825096607208252s, #samples=5504\n",
      "100%|██████████| 5504/5504 [00:08<00:00, 631.23it/s]\n",
      "\n",
      "fold 22 took 12.481380224227905s, #samples=5504\n",
      "store to h5 file took 13.14668869972229s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.38329124450683594s, #samples=4858\n",
      "100%|██████████| 4858/4858 [00:07<00:00, 663.24it/s]\n",
      "\n",
      "fold 23 took 10.983610391616821s, #samples=4858\n",
      "store to h5 file took 11.204598188400269s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3705410957336426s, #samples=4828\n",
      "100%|██████████| 4828/4828 [00:07<00:00, 640.83it/s]\n",
      "\n",
      "fold 24 took 11.059098958969116s, #samples=4828\n",
      "store to h5 file took 11.055397987365723s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7561488151550293s, #samples=4960\n",
      "100%|██████████| 4960/4960 [00:08<00:00, 577.46it/s]\n",
      "\n",
      "fold 25 took 12.266541481018066s, #samples=4960\n",
      "store to h5 file took 11.031383752822876s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6571850776672363s, #samples=4087\n",
      "100%|██████████| 4087/4087 [00:06<00:00, 654.41it/s]\n",
      "\n",
      "fold 26 took 10.08641767501831s, #samples=4087\n",
      "store to h5 file took 9.14607572555542s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5484263896942139s, #samples=4148\n",
      "100%|██████████| 4148/4148 [00:06<00:00, 648.28it/s]\n",
      "\n",
      "fold 27 took 10.059197187423706s, #samples=4148\n",
      "store to h5 file took 9.448191165924072s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.49149560928344727s, #samples=4413\n",
      "100%|██████████| 4413/4413 [00:07<00:00, 588.87it/s]\n",
      "\n",
      "fold 28 took 11.499095439910889s, #samples=4413\n",
      "store to h5 file took 9.996922731399536s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2845335006713867s, #samples=4614\n",
      "100%|██████████| 4614/4614 [00:07<00:00, 643.00it/s]\n",
      "\n",
      "fold 29 took 10.646000385284424s, #samples=4614\n",
      "store to h5 file took 10.67864203453064s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.12893986701965332s, #samples=4741\n",
      "100%|██████████| 4741/4741 [00:07<00:00, 614.68it/s]\n",
      "\n",
      "fold 30 took 11.191858291625977s, #samples=4741\n",
      "store to h5 file took 10.390310049057007s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.15107417106628418s, #samples=5655\n",
      "100%|██████████| 5655/5655 [00:09<00:00, 579.12it/s]\n",
      "\n",
      "fold 31 took 13.169093608856201s, #samples=5655\n",
      "store to h5 file took 12.94498896598816s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.21294426918029785s, #samples=4047\n",
      "100%|██████████| 4047/4047 [00:06<00:00, 650.19it/s]\n",
      "\n",
      "fold 32 took 9.968519687652588s, #samples=4047\n",
      "store to h5 file took 9.099348068237305s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.42418646812438965s, #samples=3840\n",
      "100%|██████████| 3840/3840 [00:06<00:00, 590.35it/s]\n",
      "\n",
      "fold 33 took 10.367549896240234s, #samples=3840\n",
      "store to h5 file took 8.323631525039673s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constrcuting file list took 0.1944584846496582s, #samples=4245\n",
      "100%|██████████| 4245/4245 [00:06<00:00, 636.30it/s]\n",
      "\n",
      "fold 34 took 10.156221389770508s, #samples=4245\n",
      "store to h5 file took 9.079285621643066s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.09691190719604492s, #samples=3697\n",
      "100%|██████████| 3697/3697 [00:06<00:00, 603.13it/s]\n",
      "\n",
      "fold 35 took 9.522407531738281s, #samples=3697\n",
      "store to h5 file took 8.67100977897644s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.08181428909301758s, #samples=3726\n",
      "100%|██████████| 3726/3726 [00:05<00:00, 621.89it/s]\n",
      "\n",
      "fold 36 took 9.486078977584839s, #samples=3726\n",
      "store to h5 file took 8.227418661117554s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.15622448921203613s, #samples=3513\n",
      "100%|██████████| 3513/3513 [00:05<00:00, 619.47it/s]\n",
      "\n",
      "fold 37 took 8.986856698989868s, #samples=3513\n",
      "store to h5 file took 7.715167999267578s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2516787052154541s, #samples=3608\n",
      "100%|██████████| 3608/3608 [00:06<00:00, 580.10it/s]\n",
      "\n",
      "fold 38 took 9.631502151489258s, #samples=3608\n",
      "store to h5 file took 8.033544301986694s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.12815046310424805s, #samples=3768\n",
      "100%|██████████| 3768/3768 [00:05<00:00, 639.79it/s]\n",
      "\n",
      "fold 39 took 9.397074460983276s, #samples=3768\n",
      "store to h5 file took 8.600060224533081s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.17081189155578613s, #samples=4173\n",
      "100%|██████████| 4173/4173 [00:06<00:00, 668.42it/s]\n",
      "\n",
      "fold 40 took 10.011669158935547s, #samples=4173\n",
      "store to h5 file took 9.328412771224976s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.34755778312683105s, #samples=4609\n",
      "100%|██████████| 4609/4609 [00:08<00:00, 567.52it/s]\n",
      "\n",
      "fold 41 took 11.520750284194946s, #samples=4609\n",
      "store to h5 file took 10.470231533050537s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.12733983993530273s, #samples=4135\n",
      "100%|██████████| 4135/4135 [00:06<00:00, 631.52it/s]\n",
      "\n",
      "fold 42 took 9.934261560440063s, #samples=4135\n",
      "store to h5 file took 9.406041860580444s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.3687098026275635s, #samples=3866\n",
      "100%|██████████| 3866/3866 [00:06<00:00, 609.78it/s]\n",
      "\n",
      "fold 43 took 9.9590163230896s, #samples=3866\n",
      "store to h5 file took 8.41248345375061s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.21605753898620605s, #samples=4388\n",
      "100%|██████████| 4388/4388 [00:07<00:00, 607.41it/s]\n",
      "\n",
      "fold 44 took 10.83165431022644s, #samples=4388\n",
      "store to h5 file took 10.146427392959595s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.23163962364196777s, #samples=3849\n",
      "100%|██████████| 3849/3849 [00:05<00:00, 673.08it/s]\n",
      "\n",
      "fold 45 took 9.298331022262573s, #samples=3849\n",
      "store to h5 file took 8.681208372116089s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3345975875854492s, #samples=3734\n",
      "100%|██████████| 3734/3734 [00:06<00:00, 580.26it/s]\n",
      "\n",
      "fold 46 took 9.916267395019531s, #samples=3734\n",
      "store to h5 file took 8.451588153839111s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.19386553764343262s, #samples=4076\n",
      "100%|██████████| 4076/4076 [00:06<00:00, 629.86it/s]\n",
      "\n",
      "fold 47 took 10.06941294670105s, #samples=4076\n",
      "store to h5 file took 8.910509586334229s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.13667869567871094s, #samples=3849\n",
      "100%|██████████| 3849/3849 [00:05<00:00, 644.07it/s]\n",
      "\n",
      "fold 48 took 9.488381147384644s, #samples=3849\n",
      "store to h5 file took 8.566427946090698s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.0763101577758789s, #samples=5168\n",
      "100%|██████████| 5168/5168 [00:07<00:00, 647.47it/s]\n",
      "\n",
      "fold 49 took 11.369908571243286s, #samples=5168\n",
      "store to h5 file took 11.854472637176514s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.286884069442749s, #samples=4200\n",
      "100%|██████████| 4200/4200 [00:07<00:00, 570.15it/s]\n",
      "\n",
      "fold 50 took 11.322981119155884s, #samples=4200\n",
      "store to h5 file took 8.847453117370605s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.19163107872009277s, #samples=6035\n",
      "100%|██████████| 6035/6035 [00:09<00:00, 639.97it/s]\n",
      "\n",
      "fold 51 took 12.783409595489502s, #samples=6035\n",
      "store to h5 file took 13.697298288345337s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3221275806427002s, #samples=3926\n",
      "100%|██████████| 3926/3926 [00:06<00:00, 599.22it/s]\n",
      "\n",
      "fold 52 took 10.513960838317871s, #samples=3926\n",
      "store to h5 file took 8.873090267181396s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6406650543212891s, #samples=3815\n",
      "100%|██████████| 3815/3815 [00:07<00:00, 534.85it/s]\n",
      "\n",
      "fold 53 took 11.102024555206299s, #samples=3815\n",
      "store to h5 file took 8.217445373535156s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.41846346855163574s, #samples=4765\n",
      "100%|██████████| 4765/4765 [00:07<00:00, 604.67it/s]\n",
      "\n",
      "fold 54 took 11.213577270507812s, #samples=4765\n",
      "store to h5 file took 11.061714172363281s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.20926523208618164s, #samples=4753\n",
      "100%|██████████| 4753/4753 [00:08<00:00, 593.88it/s]\n",
      "\n",
      "fold 55 took 12.273435831069946s, #samples=4753\n",
      "store to h5 file took 10.472505331039429s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6832530498504639s, #samples=4518\n",
      "100%|██████████| 4518/4518 [00:06<00:00, 661.45it/s]\n",
      "\n",
      "fold 56 took 10.986357688903809s, #samples=4518\n",
      "store to h5 file took 11.22648549079895s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6486072540283203s, #samples=5170\n",
      "100%|██████████| 5170/5170 [00:08<00:00, 634.20it/s]\n",
      "\n",
      "fold 57 took 11.715215921401978s, #samples=5170\n",
      "store to h5 file took 12.842348337173462s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6526827812194824s, #samples=4068\n",
      "100%|██████████| 4068/4068 [00:06<00:00, 581.45it/s]\n",
      "\n",
      "fold 58 took 10.57816481590271s, #samples=4068\n",
      "store to h5 file took 8.860694885253906s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.31615638732910156s, #samples=3848\n",
      "100%|██████████| 3848/3848 [00:05<00:00, 657.60it/s]\n",
      "\n",
      "fold 59 took 9.53894591331482s, #samples=3848\n",
      "store to h5 file took 8.675743818283081s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3640573024749756s, #samples=4375\n",
      "100%|██████████| 4375/4375 [00:07<00:00, 614.54it/s]\n",
      "\n",
      "fold 60 took 10.334174633026123s, #samples=4375\n",
      "store to h5 file took 9.867975950241089s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.1022188663482666s, #samples=3978\n",
      "100%|██████████| 3978/3978 [00:06<00:00, 580.73it/s]\n",
      "\n",
      "fold 61 took 10.401403188705444s, #samples=3978\n",
      "store to h5 file took 9.707499742507935s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4454977512359619s, #samples=4089\n",
      "100%|██████████| 4089/4089 [00:06<00:00, 600.14it/s]\n",
      "\n",
      "fold 62 took 10.168241262435913s, #samples=4089\n",
      "store to h5 file took 9.614047288894653s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.38587045669555664s, #samples=3237\n",
      "100%|██████████| 3237/3237 [00:05<00:00, 551.60it/s]\n",
      "\n",
      "fold 63 took 9.45198678970337s, #samples=3237\n",
      "store to h5 file took 7.661205291748047s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3466677665710449s, #samples=3794\n",
      "100%|██████████| 3794/3794 [00:06<00:00, 607.08it/s]\n",
      "\n",
      "fold 64 took 9.912651538848877s, #samples=3794\n",
      "store to h5 file took 9.021827936172485s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4445321559906006s, #samples=5234\n",
      "100%|██████████| 5234/5234 [00:08<00:00, 630.95it/s]\n",
      "\n",
      "fold 65 took 11.941923379898071s, #samples=5234\n",
      "store to h5 file took 11.95499563217163s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.28238749504089355s, #samples=4507\n",
      "100%|██████████| 4507/4507 [00:08<00:00, 551.86it/s]\n",
      "\n",
      "fold 66 took 11.675377607345581s, #samples=4507\n",
      "store to h5 file took 11.177042484283447s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5332579612731934s, #samples=4596\n",
      "100%|██████████| 4596/4596 [00:07<00:00, 618.38it/s]\n",
      "\n",
      "fold 67 took 11.190581798553467s, #samples=4596\n",
      "store to h5 file took 11.006850242614746s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constrcuting file list took 0.4954962730407715s, #samples=3149\n",
      "100%|██████████| 3149/3149 [00:05<00:00, 597.67it/s]\n",
      "\n",
      "fold 68 took 8.735904216766357s, #samples=3149\n",
      "store to h5 file took 7.295408010482788s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3084585666656494s, #samples=4959\n",
      "100%|██████████| 4959/4959 [00:08<00:00, 580.32it/s]\n",
      "\n",
      "fold 69 took 12.129473447799683s, #samples=4959\n",
      "store to h5 file took 11.736326456069946s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.30355119705200195s, #samples=4388\n",
      "100%|██████████| 4388/4388 [00:07<00:00, 563.98it/s]\n",
      "\n",
      "fold 70 took 11.333688974380493s, #samples=4388\n",
      "store to h5 file took 9.783356189727783s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6623191833496094s, #samples=5240\n",
      "100%|██████████| 5240/5240 [00:08<00:00, 641.40it/s]\n",
      "\n",
      "fold 71 took 11.552765130996704s, #samples=5240\n",
      "store to h5 file took 12.004887819290161s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5812714099884033s, #samples=3515\n",
      "100%|██████████| 3515/3515 [00:05<00:00, 616.91it/s]\n",
      "\n",
      "fold 72 took 9.460187435150146s, #samples=3515\n",
      "store to h5 file took 7.323230504989624s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.33383607864379883s, #samples=3847\n",
      "100%|██████████| 3847/3847 [00:05<00:00, 668.04it/s]\n",
      "\n",
      "fold 73 took 9.388690710067749s, #samples=3847\n",
      "store to h5 file took 9.330454111099243s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2845265865325928s, #samples=3235\n",
      "100%|██████████| 3235/3235 [00:05<00:00, 591.84it/s]\n",
      "\n",
      "fold 74 took 8.924428939819336s, #samples=3235\n",
      "store to h5 file took 7.69548225402832s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3387930393218994s, #samples=5338\n",
      "100%|██████████| 5338/5338 [00:08<00:00, 654.96it/s]\n",
      "\n",
      "fold 75 took 11.883488655090332s, #samples=5338\n",
      "store to h5 file took 12.589146614074707s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.36568593978881836s, #samples=4253\n",
      "100%|██████████| 4253/4253 [00:06<00:00, 616.65it/s]\n",
      "\n",
      "fold 76 took 10.458248615264893s, #samples=4253\n",
      "store to h5 file took 9.97313642501831s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2958707809448242s, #samples=4734\n",
      "100%|██████████| 4734/4734 [00:07<00:00, 656.62it/s]\n",
      "\n",
      "fold 77 took 10.38890528678894s, #samples=4734\n",
      "store to h5 file took 11.074346780776978s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.26977992057800293s, #samples=3654\n",
      "100%|██████████| 3654/3654 [00:05<00:00, 611.30it/s]\n",
      "\n",
      "fold 78 took 9.678523778915405s, #samples=3654\n",
      "store to h5 file took 8.709141492843628s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6401553153991699s, #samples=3417\n",
      "100%|██████████| 3417/3417 [00:06<00:00, 547.68it/s]\n",
      "\n",
      "fold 79 took 9.720080137252808s, #samples=3417\n",
      "store to h5 file took 8.807299613952637s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2590603828430176s, #samples=3469\n",
      "100%|██████████| 3469/3469 [00:05<00:00, 588.43it/s]\n",
      "\n",
      "fold 80 took 9.587243556976318s, #samples=3469\n",
      "store to h5 file took 7.905523061752319s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2874469757080078s, #samples=4125\n",
      "100%|██████████| 4125/4125 [00:06<00:00, 628.19it/s]\n",
      "\n",
      "fold 81 took 10.05370569229126s, #samples=4125\n",
      "store to h5 file took 10.551756143569946s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.24380898475646973s, #samples=4885\n",
      "100%|██████████| 4885/4885 [00:07<00:00, 642.88it/s]\n",
      "\n",
      "fold 82 took 10.881346225738525s, #samples=4885\n",
      "store to h5 file took 11.649875164031982s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6447763442993164s, #samples=4174\n",
      "100%|██████████| 4174/4174 [00:06<00:00, 630.39it/s]\n",
      "\n",
      "fold 83 took 10.223979711532593s, #samples=4174\n",
      "store to h5 file took 10.049090385437012s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3846004009246826s, #samples=3432\n",
      "100%|██████████| 3432/3432 [00:05<00:00, 656.93it/s]\n",
      "\n",
      "fold 84 took 8.312952756881714s, #samples=3432\n",
      "store to h5 file took 7.765284538269043s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.21618437767028809s, #samples=4015\n",
      "100%|██████████| 4015/4015 [00:07<00:00, 562.04it/s]\n",
      "\n",
      "fold 85 took 10.508709192276001s, #samples=4015\n",
      "store to h5 file took 9.614362955093384s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.34500670433044434s, #samples=4025\n",
      "100%|██████████| 4025/4025 [00:06<00:00, 610.30it/s]\n",
      "\n",
      "fold 86 took 10.1157808303833s, #samples=4025\n",
      "store to h5 file took 9.412256240844727s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4680202007293701s, #samples=4006\n",
      "100%|██████████| 4006/4006 [00:06<00:00, 613.37it/s]\n",
      "\n",
      "fold 87 took 10.287185907363892s, #samples=4006\n",
      "store to h5 file took 8.941060066223145s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.20644211769104004s, #samples=3810\n",
      "100%|██████████| 3810/3810 [00:06<00:00, 624.35it/s]\n",
      "\n",
      "fold 88 took 9.339212894439697s, #samples=3810\n",
      "store to h5 file took 8.768732070922852s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.37887048721313477s, #samples=4476\n",
      "100%|██████████| 4476/4476 [00:07<00:00, 629.82it/s]\n",
      "\n",
      "fold 89 took 10.350549221038818s, #samples=4476\n",
      "store to h5 file took 9.89651870727539s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.298750638961792s, #samples=6127\n",
      "100%|██████████| 6127/6127 [00:10<00:00, 610.57it/s]\n",
      "\n",
      "fold 90 took 13.792255401611328s, #samples=6127\n",
      "store to h5 file took 14.082468509674072s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.36554694175720215s, #samples=4742\n",
      "100%|██████████| 4742/4742 [00:07<00:00, 627.41it/s]\n",
      "\n",
      "fold 91 took 11.293019533157349s, #samples=4742\n",
      "store to h5 file took 11.907379150390625s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.30391597747802734s, #samples=4109\n",
      "100%|██████████| 4109/4109 [00:07<00:00, 583.07it/s]\n",
      "\n",
      "fold 92 took 10.422920227050781s, #samples=4109\n",
      "store to h5 file took 9.50318455696106s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.637794017791748s, #samples=4224\n",
      "100%|██████████| 4224/4224 [00:06<00:00, 626.40it/s]\n",
      "\n",
      "fold 93 took 10.008580207824707s, #samples=4224\n",
      "store to h5 file took 9.66495943069458s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2339332103729248s, #samples=4010\n",
      "100%|██████████| 4010/4010 [00:06<00:00, 640.53it/s]\n",
      "\n",
      "fold 94 took 9.822889566421509s, #samples=4010\n",
      "store to h5 file took 8.696984767913818s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4253549575805664s, #samples=4284\n",
      "100%|██████████| 4284/4284 [00:06<00:00, 653.78it/s]\n",
      "\n",
      "fold 95 took 10.075766563415527s, #samples=4284\n",
      "store to h5 file took 10.765888690948486s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3460817337036133s, #samples=4609\n",
      "100%|██████████| 4609/4609 [00:07<00:00, 656.99it/s]\n",
      "\n",
      "fold 96 took 10.776267290115356s, #samples=4609\n",
      "store to h5 file took 10.855015277862549s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.49094295501708984s, #samples=4346\n",
      "100%|██████████| 4346/4346 [00:07<00:00, 557.91it/s]\n",
      "\n",
      "fold 97 took 11.066846132278442s, #samples=4346\n",
      "store to h5 file took 10.35939335823059s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.31461381912231445s, #samples=5460\n",
      "100%|██████████| 5460/5460 [00:08<00:00, 625.27it/s]\n",
      "\n",
      "fold 98 took 12.28154993057251s, #samples=5460\n",
      "store to h5 file took 12.74111294746399s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.333956241607666s, #samples=3868\n",
      "100%|██████████| 3868/3868 [00:06<00:00, 633.21it/s]\n",
      "\n",
      "fold 99 took 9.405027627944946s, #samples=3868\n",
      "store to h5 file took 8.819826602935791s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.37470197677612305s, #samples=4495\n",
      "100%|██████████| 4495/4495 [00:06<00:00, 645.16it/s]\n",
      "\n",
      "fold 100 took 10.329179525375366s, #samples=4495\n",
      "store to h5 file took 10.337721824645996s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3571004867553711s, #samples=5385\n",
      "100%|██████████| 5385/5385 [00:09<00:00, 596.07it/s]\n",
      "\n",
      "fold 101 took 12.601232528686523s, #samples=5385\n",
      "store to h5 file took 12.30838918685913s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constrcuting file list took 0.32653069496154785s, #samples=4346\n",
      "100%|██████████| 4346/4346 [00:07<00:00, 566.23it/s]\n",
      "\n",
      "fold 102 took 11.299989223480225s, #samples=4346\n",
      "store to h5 file took 9.996408462524414s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.27845287322998047s, #samples=3926\n",
      "100%|██████████| 3926/3926 [00:05<00:00, 663.88it/s]\n",
      "\n",
      "fold 103 took 9.479287385940552s, #samples=3926\n",
      "store to h5 file took 9.005536794662476s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.35833287239074707s, #samples=4266\n",
      "100%|██████████| 4266/4266 [00:07<00:00, 639.19it/s]\n",
      "\n",
      "fold 104 took 10.781177759170532s, #samples=4266\n",
      "store to h5 file took 10.02457880973816s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.22187161445617676s, #samples=3735\n",
      "100%|██████████| 3735/3735 [00:06<00:00, 587.63it/s]\n",
      "\n",
      "fold 105 took 9.8218092918396s, #samples=3735\n",
      "store to h5 file took 8.233407497406006s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2812175750732422s, #samples=3924\n",
      "100%|██████████| 3924/3924 [00:06<00:00, 640.00it/s]\n",
      "\n",
      "fold 106 took 9.582082509994507s, #samples=3924\n",
      "store to h5 file took 8.808354616165161s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3480072021484375s, #samples=5787\n",
      "100%|██████████| 5787/5787 [00:09<00:00, 635.98it/s]\n",
      "\n",
      "fold 107 took 12.611169815063477s, #samples=5787\n",
      "store to h5 file took 14.006826400756836s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.25637364387512207s, #samples=6387\n",
      "100%|██████████| 6387/6387 [00:10<00:00, 598.32it/s]\n",
      "\n",
      "fold 108 took 13.981318473815918s, #samples=6387\n",
      "store to h5 file took 15.719211339950562s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3879585266113281s, #samples=4445\n",
      "100%|██████████| 4445/4445 [00:07<00:00, 611.93it/s]\n",
      "\n",
      "fold 109 took 11.012845993041992s, #samples=4445\n",
      "store to h5 file took 10.502723217010498s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.13892340660095215s, #samples=4761\n",
      "100%|██████████| 4761/4761 [00:07<00:00, 615.64it/s]\n",
      "\n",
      "fold 110 took 11.251612424850464s, #samples=4761\n",
      "store to h5 file took 11.420002222061157s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.39980173110961914s, #samples=3926\n",
      "100%|██████████| 3926/3926 [00:06<00:00, 612.69it/s]\n",
      "\n",
      "fold 111 took 9.942172527313232s, #samples=3926\n",
      "store to h5 file took 9.117390632629395s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.12306571006774902s, #samples=3776\n",
      "100%|██████████| 3776/3776 [00:06<00:00, 629.23it/s]\n",
      "\n",
      "fold 112 took 9.550456285476685s, #samples=3776\n",
      "store to h5 file took 9.042745351791382s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4389820098876953s, #samples=3400\n",
      "100%|██████████| 3400/3400 [00:05<00:00, 623.83it/s]\n",
      "\n",
      "fold 113 took 9.4791579246521s, #samples=3400\n",
      "store to h5 file took 7.3765037059783936s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2849583625793457s, #samples=3127\n",
      "100%|██████████| 3127/3127 [00:04<00:00, 642.83it/s]\n",
      "\n",
      "fold 114 took 8.282578706741333s, #samples=3127\n",
      "store to h5 file took 7.244905233383179s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5316565036773682s, #samples=4489\n",
      "100%|██████████| 4489/4489 [00:07<00:00, 630.06it/s]\n",
      "\n",
      "fold 115 took 10.446860313415527s, #samples=4489\n",
      "store to h5 file took 10.382608413696289s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.36635446548461914s, #samples=3528\n",
      "100%|██████████| 3528/3528 [00:05<00:00, 638.28it/s]\n",
      "\n",
      "fold 116 took 8.85416865348816s, #samples=3528\n",
      "store to h5 file took 8.724771976470947s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2974710464477539s, #samples=3469\n",
      "100%|██████████| 3469/3469 [00:05<00:00, 644.17it/s]\n",
      "\n",
      "fold 117 took 8.979777812957764s, #samples=3469\n",
      "store to h5 file took 7.873778343200684s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.22128582000732422s, #samples=3245\n",
      "100%|██████████| 3245/3245 [00:04<00:00, 654.06it/s]\n",
      "\n",
      "fold 118 took 8.501989364624023s, #samples=3245\n",
      "store to h5 file took 7.730318546295166s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.35146164894104004s, #samples=4500\n",
      "100%|██████████| 4500/4500 [00:06<00:00, 647.83it/s]\n",
      "\n",
      "fold 119 took 10.486987352371216s, #samples=4500\n",
      "store to h5 file took 10.855243682861328s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.15122175216674805s, #samples=3841\n",
      "100%|██████████| 3841/3841 [00:05<00:00, 652.19it/s]\n",
      "\n",
      "fold 120 took 9.180916547775269s, #samples=3841\n",
      "store to h5 file took 8.920757055282593s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.41869020462036133s, #samples=3734\n",
      "100%|██████████| 3734/3734 [00:06<00:00, 605.49it/s]\n",
      "\n",
      "fold 121 took 9.32747197151184s, #samples=3734\n",
      "store to h5 file took 8.792049169540405s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.34856510162353516s, #samples=4001\n",
      "100%|██████████| 4001/4001 [00:06<00:00, 641.92it/s]\n",
      "\n",
      "fold 122 took 9.67031979560852s, #samples=4001\n",
      "store to h5 file took 9.185024976730347s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3103508949279785s, #samples=3308\n",
      "100%|██████████| 3308/3308 [00:05<00:00, 626.80it/s]\n",
      "\n",
      "fold 123 took 8.660352230072021s, #samples=3308\n",
      "store to h5 file took 7.856794834136963s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3782346248626709s, #samples=4551\n",
      "100%|██████████| 4551/4551 [00:07<00:00, 638.12it/s]\n",
      "\n",
      "fold 124 took 10.876656770706177s, #samples=4551\n",
      "store to h5 file took 10.946670770645142s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5040547847747803s, #samples=3622\n",
      "100%|██████████| 3622/3622 [00:05<00:00, 658.64it/s]\n",
      "\n",
      "fold 125 took 9.024765729904175s, #samples=3622\n",
      "store to h5 file took 9.338428020477295s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4856131076812744s, #samples=4101\n",
      "100%|██████████| 4101/4101 [00:06<00:00, 637.60it/s]\n",
      "\n",
      "fold 126 took 9.729340314865112s, #samples=4101\n",
      "store to h5 file took 9.674277782440186s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3464503288269043s, #samples=4274\n",
      "100%|██████████| 4274/4274 [00:06<00:00, 632.05it/s]\n",
      "\n",
      "fold 127 took 10.175848484039307s, #samples=4274\n",
      "store to h5 file took 9.943598747253418s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.23629307746887207s, #samples=4508\n",
      "100%|██████████| 4508/4508 [00:06<00:00, 656.57it/s]\n",
      "\n",
      "fold 128 took 10.231574296951294s, #samples=4508\n",
      "store to h5 file took 10.994720220565796s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.1640622615814209s, #samples=3119\n",
      "100%|██████████| 3119/3119 [00:05<00:00, 543.48it/s]\n",
      "\n",
      "fold 129 took 9.16749906539917s, #samples=3119\n",
      "store to h5 file took 6.914926052093506s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.23764300346374512s, #samples=3634\n",
      "100%|██████████| 3634/3634 [00:05<00:00, 676.10it/s]\n",
      "\n",
      "fold 130 took 8.768096923828125s, #samples=3634\n",
      "store to h5 file took 8.467892169952393s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.061781883239746094s, #samples=3936\n",
      "100%|██████████| 3936/3936 [00:06<00:00, 641.39it/s]\n",
      "\n",
      "fold 131 took 9.609885215759277s, #samples=3936\n",
      "store to h5 file took 8.721705198287964s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6353142261505127s, #samples=4010\n",
      "100%|██████████| 4010/4010 [00:06<00:00, 614.58it/s]\n",
      "\n",
      "fold 132 took 9.973142385482788s, #samples=4010\n",
      "store to h5 file took 9.988059043884277s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5402328968048096s, #samples=4309\n",
      "100%|██████████| 4309/4309 [00:06<00:00, 629.03it/s]\n",
      "\n",
      "fold 133 took 10.519913911819458s, #samples=4309\n",
      "store to h5 file took 10.195116758346558s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4359307289123535s, #samples=2644\n",
      "100%|██████████| 2644/2644 [00:04<00:00, 647.21it/s]\n",
      "\n",
      "fold 134 took 7.923553705215454s, #samples=2644\n",
      "store to h5 file took 6.075292348861694s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5918302536010742s, #samples=3839\n",
      "100%|██████████| 3839/3839 [00:06<00:00, 625.03it/s]\n",
      "\n",
      "fold 135 took 9.715686559677124s, #samples=3839\n",
      "store to h5 file took 9.056344747543335s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constrcuting file list took 0.6704933643341064s, #samples=3686\n",
      "100%|██████████| 3686/3686 [00:05<00:00, 624.31it/s]\n",
      "\n",
      "fold 136 took 9.372170209884644s, #samples=3686\n",
      "store to h5 file took 8.689716815948486s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5570604801177979s, #samples=4164\n",
      "100%|██████████| 4164/4164 [00:06<00:00, 597.36it/s]\n",
      "\n",
      "fold 137 took 10.521311521530151s, #samples=4164\n",
      "store to h5 file took 10.588420152664185s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4930269718170166s, #samples=3708\n",
      "100%|██████████| 3708/3708 [00:05<00:00, 649.91it/s]\n",
      "\n",
      "fold 138 took 9.033170461654663s, #samples=3708\n",
      "store to h5 file took 8.574602603912354s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7490124702453613s, #samples=4335\n",
      "100%|██████████| 4335/4335 [00:06<00:00, 640.25it/s]\n",
      "\n",
      "fold 139 took 10.034619569778442s, #samples=4335\n",
      "store to h5 file took 10.30495834350586s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.37049245834350586s, #samples=4942\n",
      "100%|██████████| 4942/4942 [00:07<00:00, 620.30it/s]\n",
      "\n",
      "fold 140 took 11.57908582687378s, #samples=4942\n",
      "store to h5 file took 11.892720222473145s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.9448862075805664s, #samples=4072\n",
      "100%|██████████| 4072/4072 [00:06<00:00, 629.44it/s]\n",
      "\n",
      "fold 141 took 9.735809564590454s, #samples=4072\n",
      "store to h5 file took 9.18624496459961s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.23427891731262207s, #samples=3008\n",
      "100%|██████████| 3008/3008 [00:05<00:00, 598.23it/s]\n",
      "\n",
      "fold 142 took 8.788165092468262s, #samples=3008\n",
      "store to h5 file took 7.050762891769409s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.31605005264282227s, #samples=3998\n",
      "100%|██████████| 3998/3998 [00:06<00:00, 634.22it/s]\n",
      "\n",
      "fold 143 took 9.6010582447052s, #samples=3998\n",
      "store to h5 file took 9.281306505203247s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3287081718444824s, #samples=4310\n",
      "100%|██████████| 4310/4310 [00:06<00:00, 629.35it/s]\n",
      "\n",
      "fold 144 took 10.475176095962524s, #samples=4310\n",
      "store to h5 file took 10.418100595474243s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6330592632293701s, #samples=4229\n",
      "100%|██████████| 4229/4229 [00:07<00:00, 583.30it/s]\n",
      "\n",
      "fold 145 took 10.64004921913147s, #samples=4229\n",
      "store to h5 file took 9.99303388595581s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6152856349945068s, #samples=2970\n",
      "100%|██████████| 2970/2970 [00:04<00:00, 623.56it/s]\n",
      "\n",
      "fold 146 took 8.360682487487793s, #samples=2970\n",
      "store to h5 file took 7.484758377075195s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6215066909790039s, #samples=2750\n",
      "100%|██████████| 2750/2750 [00:04<00:00, 617.38it/s]\n",
      "\n",
      "fold 147 took 8.350231885910034s, #samples=2750\n",
      "store to h5 file took 6.3208091259002686s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.38862037658691406s, #samples=3842\n",
      "100%|██████████| 3842/3842 [00:05<00:00, 654.17it/s]\n",
      "\n",
      "fold 148 took 9.385406494140625s, #samples=3842\n",
      "store to h5 file took 8.906589984893799s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.47021985054016113s, #samples=4011\n",
      "100%|██████████| 4011/4011 [00:06<00:00, 587.84it/s]\n",
      "\n",
      "fold 149 took 10.533077239990234s, #samples=4011\n",
      "store to h5 file took 9.002056121826172s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3301239013671875s, #samples=3654\n",
      "100%|██████████| 3654/3654 [00:06<00:00, 592.98it/s]\n",
      "\n",
      "fold 150 took 9.689075946807861s, #samples=3654\n",
      "store to h5 file took 8.535757541656494s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.35294437408447266s, #samples=3165\n",
      "100%|██████████| 3165/3165 [00:04<00:00, 635.63it/s]\n",
      "\n",
      "fold 151 took 8.347718477249146s, #samples=3165\n",
      "store to h5 file took 7.266490936279297s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3204071521759033s, #samples=4818\n",
      "100%|██████████| 4818/4818 [00:07<00:00, 623.13it/s]\n",
      "\n",
      "fold 152 took 11.159351110458374s, #samples=4818\n",
      "store to h5 file took 11.215888500213623s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4265923500061035s, #samples=6593\n",
      "100%|██████████| 6593/6593 [00:10<00:00, 618.59it/s]\n",
      "\n",
      "fold 153 took 14.107156038284302s, #samples=6593\n",
      "store to h5 file took 15.698356628417969s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.40724945068359375s, #samples=4206\n",
      "100%|██████████| 4206/4206 [00:06<00:00, 647.88it/s]\n",
      "\n",
      "fold 154 took 9.866208791732788s, #samples=4206\n",
      "store to h5 file took 10.313598394393921s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6933038234710693s, #samples=3747\n",
      "100%|██████████| 3747/3747 [00:06<00:00, 597.45it/s]\n",
      "\n",
      "fold 155 took 9.928168058395386s, #samples=3747\n",
      "store to h5 file took 8.64033317565918s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6284170150756836s, #samples=3168\n",
      "100%|██████████| 3168/3168 [00:05<00:00, 629.84it/s]\n",
      "\n",
      "fold 156 took 8.543762922286987s, #samples=3168\n",
      "store to h5 file took 7.526997089385986s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5529463291168213s, #samples=3371\n",
      "100%|██████████| 3371/3371 [00:05<00:00, 590.46it/s]\n",
      "\n",
      "fold 157 took 9.219552516937256s, #samples=3371\n",
      "store to h5 file took 7.447737693786621s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6550583839416504s, #samples=3152\n",
      "100%|██████████| 3152/3152 [00:04<00:00, 655.15it/s]\n",
      "\n",
      "fold 158 took 8.674169301986694s, #samples=3152\n",
      "store to h5 file took 7.314630508422852s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.17273306846618652s, #samples=3890\n",
      "100%|██████████| 3890/3890 [00:06<00:00, 617.86it/s]\n",
      "\n",
      "fold 159 took 9.9295072555542s, #samples=3890\n",
      "store to h5 file took 9.460713624954224s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3858048915863037s, #samples=3244\n",
      "100%|██████████| 3244/3244 [00:04<00:00, 658.20it/s]\n",
      "\n",
      "fold 160 took 8.59105372428894s, #samples=3244\n",
      "store to h5 file took 7.396625995635986s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4139831066131592s, #samples=3277\n",
      "100%|██████████| 3277/3277 [00:05<00:00, 574.13it/s]\n",
      "\n",
      "fold 161 took 9.223435163497925s, #samples=3277\n",
      "store to h5 file took 7.318165063858032s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8242447376251221s, #samples=2772\n",
      "100%|██████████| 2772/2772 [00:04<00:00, 666.50it/s]\n",
      "\n",
      "fold 162 took 7.609380722045898s, #samples=2772\n",
      "store to h5 file took 6.455041408538818s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.792426586151123s, #samples=3194\n",
      "100%|██████████| 3194/3194 [00:05<00:00, 609.34it/s]\n",
      "\n",
      "fold 163 took 8.496675729751587s, #samples=3194\n",
      "store to h5 file took 7.620332479476929s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4452495574951172s, #samples=3327\n",
      "100%|██████████| 3327/3327 [00:05<00:00, 556.86it/s]\n",
      "\n",
      "fold 164 took 9.705588579177856s, #samples=3327\n",
      "store to h5 file took 7.427879095077515s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.47940921783447266s, #samples=3508\n",
      "100%|██████████| 3508/3508 [00:05<00:00, 623.60it/s]\n",
      "\n",
      "fold 165 took 9.188948392868042s, #samples=3508\n",
      "store to h5 file took 8.10971736907959s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.442288875579834s, #samples=2788\n",
      "100%|██████████| 2788/2788 [00:04<00:00, 627.57it/s]\n",
      "\n",
      "fold 166 took 7.792091369628906s, #samples=2788\n",
      "store to h5 file took 6.482861280441284s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.35060596466064453s, #samples=3396\n",
      "100%|██████████| 3396/3396 [00:05<00:00, 603.63it/s]\n",
      "\n",
      "fold 167 took 9.140305280685425s, #samples=3396\n",
      "store to h5 file took 8.342737913131714s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.32105398178100586s, #samples=3727\n",
      "100%|██████████| 3727/3727 [00:05<00:00, 622.96it/s]\n",
      "\n",
      "fold 168 took 9.396595001220703s, #samples=3727\n",
      "store to h5 file took 8.889590978622437s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2609059810638428s, #samples=3315\n",
      "100%|██████████| 3315/3315 [00:05<00:00, 621.14it/s]\n",
      "\n",
      "fold 169 took 9.054461240768433s, #samples=3315\n",
      "store to h5 file took 8.025165319442749s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constrcuting file list took 0.3281748294830322s, #samples=3215\n",
      "100%|██████████| 3215/3215 [00:05<00:00, 591.22it/s]\n",
      "\n",
      "fold 170 took 9.06594467163086s, #samples=3215\n",
      "store to h5 file took 7.3618597984313965s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.07894277572631836s, #samples=3680\n",
      "100%|██████████| 3680/3680 [00:05<00:00, 617.70it/s]\n",
      "\n",
      "fold 171 took 9.352170467376709s, #samples=3680\n",
      "store to h5 file took 8.542196989059448s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2126939296722412s, #samples=3467\n",
      "100%|██████████| 3467/3467 [00:05<00:00, 651.31it/s]\n",
      "\n",
      "fold 172 took 8.51024079322815s, #samples=3467\n",
      "store to h5 file took 8.242997407913208s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.9717135429382324s, #samples=3424\n",
      "100%|██████████| 3424/3424 [00:05<00:00, 646.07it/s]\n",
      "\n",
      "fold 173 took 8.918948411941528s, #samples=3424\n",
      "store to h5 file took 7.89178204536438s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.0053291320800781s, #samples=3013\n",
      "100%|██████████| 3013/3013 [00:04<00:00, 634.05it/s]\n",
      "\n",
      "fold 174 took 8.244498252868652s, #samples=3013\n",
      "store to h5 file took 6.769197940826416s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.41807985305786133s, #samples=3372\n",
      "100%|██████████| 3372/3372 [00:05<00:00, 643.19it/s]\n",
      "\n",
      "fold 175 took 8.532142162322998s, #samples=3372\n",
      "store to h5 file took 8.646015405654907s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.30988264083862305s, #samples=3371\n",
      "100%|██████████| 3371/3371 [00:05<00:00, 620.43it/s]\n",
      "\n",
      "fold 176 took 8.59462857246399s, #samples=3371\n",
      "store to h5 file took 7.845057725906372s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.49060583114624023s, #samples=3363\n",
      "100%|██████████| 3363/3363 [00:05<00:00, 636.99it/s]\n",
      "\n",
      "fold 177 took 8.844302892684937s, #samples=3363\n",
      "store to h5 file took 7.886443614959717s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6409261226654053s, #samples=3465\n",
      "100%|██████████| 3465/3465 [00:05<00:00, 671.65it/s]\n",
      "\n",
      "fold 178 took 8.934335231781006s, #samples=3465\n",
      "store to h5 file took 8.480666160583496s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.123321771621704s, #samples=3336\n",
      "100%|██████████| 3336/3336 [00:05<00:00, 613.59it/s]\n",
      "\n",
      "fold 179 took 9.235065698623657s, #samples=3336\n",
      "store to h5 file took 7.5620081424713135s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8583889007568359s, #samples=3077\n",
      "100%|██████████| 3077/3077 [00:05<00:00, 607.85it/s]\n",
      "\n",
      "fold 180 took 8.381516218185425s, #samples=3077\n",
      "store to h5 file took 7.035127878189087s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5988380908966064s, #samples=3109\n",
      "100%|██████████| 3109/3109 [00:04<00:00, 630.96it/s]\n",
      "\n",
      "fold 181 took 8.146122694015503s, #samples=3109\n",
      "store to h5 file took 7.869556903839111s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4739713668823242s, #samples=2291\n",
      "100%|██████████| 2291/2291 [00:03<00:00, 636.34it/s]\n",
      "\n",
      "fold 182 took 6.703500032424927s, #samples=2291\n",
      "store to h5 file took 5.033883810043335s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.26715993881225586s, #samples=2617\n",
      "100%|██████████| 2617/2617 [00:04<00:00, 618.94it/s]\n",
      "\n",
      "fold 183 took 7.492666721343994s, #samples=2617\n",
      "store to h5 file took 6.768329620361328s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.4712510108947754s, #samples=2886\n",
      "100%|██████████| 2886/2886 [00:04<00:00, 644.64it/s]\n",
      "\n",
      "fold 184 took 8.411675691604614s, #samples=2886\n",
      "store to h5 file took 6.340703248977661s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6110913753509521s, #samples=3382\n",
      "100%|██████████| 3382/3382 [00:05<00:00, 643.65it/s]\n",
      "\n",
      "fold 185 took 8.723336696624756s, #samples=3382\n",
      "store to h5 file took 7.525570631027222s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6327564716339111s, #samples=3337\n",
      "100%|██████████| 3337/3337 [00:05<00:00, 643.18it/s]\n",
      "\n",
      "fold 186 took 8.66482663154602s, #samples=3337\n",
      "store to h5 file took 7.826430559158325s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.0201687812805176s, #samples=3016\n",
      "100%|██████████| 3016/3016 [00:04<00:00, 628.92it/s]\n",
      "\n",
      "fold 187 took 8.098838806152344s, #samples=3016\n",
      "store to h5 file took 6.819763422012329s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.47948384284973145s, #samples=3716\n",
      "100%|██████████| 3716/3716 [00:05<00:00, 643.54it/s]\n",
      "\n",
      "fold 188 took 9.193926811218262s, #samples=3716\n",
      "store to h5 file took 9.200036764144897s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2457568645477295s, #samples=3904\n",
      "100%|██████████| 3904/3904 [00:06<00:00, 632.72it/s]\n",
      "\n",
      "fold 189 took 9.96666669845581s, #samples=3904\n",
      "store to h5 file took 8.894443035125732s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.41621947288513184s, #samples=2712\n",
      "100%|██████████| 2712/2712 [00:04<00:00, 614.91it/s]\n",
      "\n",
      "fold 190 took 7.98180365562439s, #samples=2712\n",
      "store to h5 file took 5.901149034500122s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.749901533126831s, #samples=3160\n",
      "100%|██████████| 3160/3160 [00:05<00:00, 630.93it/s]\n",
      "\n",
      "fold 191 took 8.66545033454895s, #samples=3160\n",
      "store to h5 file took 7.281369686126709s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4776735305786133s, #samples=3047\n",
      "100%|██████████| 3047/3047 [00:04<00:00, 639.46it/s]\n",
      "\n",
      "fold 192 took 8.33708643913269s, #samples=3047\n",
      "store to h5 file took 7.039302349090576s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3388967514038086s, #samples=3105\n",
      "100%|██████████| 3105/3105 [00:05<00:00, 617.28it/s]\n",
      "\n",
      "fold 193 took 8.962652206420898s, #samples=3105\n",
      "store to h5 file took 6.959952116012573s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3122546672821045s, #samples=3193\n",
      "100%|██████████| 3193/3193 [00:05<00:00, 633.33it/s]\n",
      "\n",
      "fold 194 took 8.397650480270386s, #samples=3193\n",
      "store to h5 file took 7.527774810791016s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.42560410499572754s, #samples=2708\n",
      "100%|██████████| 2708/2708 [00:04<00:00, 616.57it/s]\n",
      "\n",
      "fold 195 took 7.758177042007446s, #samples=2708\n",
      "store to h5 file took 6.2145044803619385s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6581315994262695s, #samples=3516\n",
      "100%|██████████| 3516/3516 [00:05<00:00, 612.07it/s]\n",
      "\n",
      "fold 196 took 9.823237657546997s, #samples=3516\n",
      "store to h5 file took 8.025573492050171s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.49660754203796387s, #samples=3636\n",
      "100%|██████████| 3636/3636 [00:05<00:00, 616.48it/s]\n",
      "\n",
      "fold 197 took 9.178937435150146s, #samples=3636\n",
      "store to h5 file took 8.722946643829346s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6619200706481934s, #samples=3515\n",
      "100%|██████████| 3515/3515 [00:05<00:00, 629.84it/s]\n",
      "\n",
      "fold 198 took 9.4688241481781s, #samples=3515\n",
      "store to h5 file took 8.034588813781738s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5464704036712646s, #samples=3153\n",
      "100%|██████████| 3153/3153 [00:04<00:00, 644.17it/s]\n",
      "\n",
      "fold 199 took 8.14940619468689s, #samples=3153\n",
      "store to h5 file took 7.980501174926758s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2066349983215332s, #samples=3220\n",
      "100%|██████████| 3220/3220 [00:05<00:00, 596.75it/s]\n",
      "\n",
      "fold 200 took 9.086997747421265s, #samples=3220\n",
      "store to h5 file took 7.3325324058532715s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4111006259918213s, #samples=3719\n",
      "100%|██████████| 3719/3719 [00:05<00:00, 643.32it/s]\n",
      "\n",
      "fold 201 took 9.348814964294434s, #samples=3719\n",
      "store to h5 file took 8.371013164520264s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4974956512451172s, #samples=3327\n",
      "100%|██████████| 3327/3327 [00:05<00:00, 623.25it/s]\n",
      "\n",
      "fold 202 took 8.683492422103882s, #samples=3327\n",
      "store to h5 file took 7.408703804016113s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.37297844886779785s, #samples=2990\n",
      "100%|██████████| 2990/2990 [00:04<00:00, 641.36it/s]\n",
      "\n",
      "fold 203 took 8.170013904571533s, #samples=2990\n",
      "store to h5 file took 7.046019554138184s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constrcuting file list took 0.44625329971313477s, #samples=3349\n",
      "100%|██████████| 3349/3349 [00:05<00:00, 619.30it/s]\n",
      "\n",
      "fold 204 took 8.891187191009521s, #samples=3349\n",
      "store to h5 file took 8.170573711395264s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3040275573730469s, #samples=3275\n",
      "100%|██████████| 3275/3275 [00:05<00:00, 626.52it/s]\n",
      "\n",
      "fold 205 took 8.47877311706543s, #samples=3275\n",
      "store to h5 file took 7.530213832855225s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.26827549934387207s, #samples=2786\n",
      "100%|██████████| 2786/2786 [00:04<00:00, 637.91it/s]\n",
      "\n",
      "fold 206 took 7.733427047729492s, #samples=2786\n",
      "store to h5 file took 6.718735933303833s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5054724216461182s, #samples=3436\n",
      "100%|██████████| 3436/3436 [00:05<00:00, 646.26it/s]\n",
      "\n",
      "fold 207 took 8.844034671783447s, #samples=3436\n",
      "store to h5 file took 8.131415843963623s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.24479985237121582s, #samples=3077\n",
      "100%|██████████| 3077/3077 [00:04<00:00, 671.46it/s]\n",
      "\n",
      "fold 208 took 8.181959390640259s, #samples=3077\n",
      "store to h5 file took 6.81588339805603s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.14454364776611328s, #samples=2735\n",
      "100%|██████████| 2735/2735 [00:04<00:00, 622.77it/s]\n",
      "\n",
      "fold 209 took 7.744106769561768s, #samples=2735\n",
      "store to h5 file took 6.3157243728637695s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7896857261657715s, #samples=3472\n",
      "100%|██████████| 3472/3472 [00:05<00:00, 631.37it/s]\n",
      "\n",
      "fold 210 took 8.84215259552002s, #samples=3472\n",
      "store to h5 file took 8.084768533706665s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5636765956878662s, #samples=3780\n",
      "100%|██████████| 3780/3780 [00:06<00:00, 604.63it/s]\n",
      "\n",
      "fold 211 took 9.940147638320923s, #samples=3780\n",
      "store to h5 file took 8.787086486816406s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.621283769607544s, #samples=3455\n",
      "100%|██████████| 3455/3455 [00:05<00:00, 620.34it/s]\n",
      "\n",
      "fold 212 took 9.097998857498169s, #samples=3455\n",
      "store to h5 file took 7.673592805862427s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.403409481048584s, #samples=3337\n",
      "100%|██████████| 3337/3337 [00:05<00:00, 620.00it/s]\n",
      "\n",
      "fold 213 took 8.85254168510437s, #samples=3337\n",
      "store to h5 file took 7.71838903427124s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7425057888031006s, #samples=3424\n",
      "100%|██████████| 3424/3424 [00:05<00:00, 647.57it/s]\n",
      "\n",
      "fold 214 took 8.937600374221802s, #samples=3424\n",
      "store to h5 file took 7.782247304916382s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.015298843383789s, #samples=2638\n",
      "100%|██████████| 2638/2638 [00:04<00:00, 624.18it/s]\n",
      "\n",
      "fold 215 took 7.698770523071289s, #samples=2638\n",
      "store to h5 file took 5.956446647644043s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2863469123840332s, #samples=3455\n",
      "100%|██████████| 3455/3455 [00:05<00:00, 590.86it/s]\n",
      "\n",
      "fold 216 took 9.284021139144897s, #samples=3455\n",
      "store to h5 file took 7.3380632400512695s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.9790637493133545s, #samples=3291\n",
      "100%|██████████| 3291/3291 [00:05<00:00, 624.13it/s]\n",
      "\n",
      "fold 217 took 8.908109426498413s, #samples=3291\n",
      "store to h5 file took 7.693107604980469s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5818915367126465s, #samples=2665\n",
      "100%|██████████| 2665/2665 [00:04<00:00, 628.97it/s]\n",
      "\n",
      "fold 218 took 8.047584533691406s, #samples=2665\n",
      "store to h5 file took 6.483065366744995s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.712043046951294s, #samples=2628\n",
      "100%|██████████| 2628/2628 [00:04<00:00, 597.29it/s]\n",
      "\n",
      "fold 219 took 8.330831289291382s, #samples=2628\n",
      "store to h5 file took 5.777981996536255s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5570354461669922s, #samples=2766\n",
      "100%|██████████| 2766/2766 [00:04<00:00, 557.57it/s]\n",
      "\n",
      "fold 220 took 8.708674907684326s, #samples=2766\n",
      "store to h5 file took 6.4774816036224365s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.629002571105957s, #samples=3313\n",
      "100%|██████████| 3313/3313 [00:05<00:00, 622.34it/s]\n",
      "\n",
      "fold 221 took 8.484276294708252s, #samples=3313\n",
      "store to h5 file took 8.29204535484314s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.43272900581359863s, #samples=3019\n",
      "100%|██████████| 3019/3019 [00:04<00:00, 669.68it/s]\n",
      "\n",
      "fold 222 took 7.714841842651367s, #samples=3019\n",
      "store to h5 file took 6.866745471954346s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3035557270050049s, #samples=3072\n",
      "100%|██████████| 3072/3072 [00:04<00:00, 615.05it/s]\n",
      "\n",
      "fold 223 took 8.269272327423096s, #samples=3072\n",
      "store to h5 file took 6.692778587341309s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8587841987609863s, #samples=2858\n",
      "100%|██████████| 2858/2858 [00:04<00:00, 646.50it/s]\n",
      "\n",
      "fold 224 took 7.632064342498779s, #samples=2858\n",
      "store to h5 file took 6.4378602504730225s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6998810768127441s, #samples=2749\n",
      "100%|██████████| 2749/2749 [00:04<00:00, 587.96it/s]\n",
      "\n",
      "fold 225 took 8.300763130187988s, #samples=2749\n",
      "store to h5 file took 5.959615468978882s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4906008243560791s, #samples=2971\n",
      "100%|██████████| 2971/2971 [00:05<00:00, 598.90it/s]\n",
      "\n",
      "fold 226 took 8.253519058227539s, #samples=2971\n",
      "store to h5 file took 7.0193212032318115s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5358912944793701s, #samples=3937\n",
      "100%|██████████| 3937/3937 [00:06<00:00, 619.20it/s]\n",
      "\n",
      "fold 227 took 9.81487512588501s, #samples=3937\n",
      "store to h5 file took 9.093154907226562s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.2307989597320557s, #samples=3925\n",
      "100%|██████████| 3925/3925 [00:06<00:00, 629.07it/s]\n",
      "\n",
      "fold 228 took 9.673154830932617s, #samples=3925\n",
      "store to h5 file took 9.054174423217773s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.821202278137207s, #samples=3338\n",
      "100%|██████████| 3338/3338 [00:05<00:00, 629.93it/s]\n",
      "\n",
      "fold 229 took 8.50502896308899s, #samples=3338\n",
      "store to h5 file took 7.650384187698364s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.9461104869842529s, #samples=3804\n",
      "100%|██████████| 3804/3804 [00:06<00:00, 553.67it/s]\n",
      "\n",
      "fold 230 took 9.766191482543945s, #samples=3804\n",
      "store to h5 file took 8.922734260559082s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8848254680633545s, #samples=3135\n",
      "100%|██████████| 3135/3135 [00:04<00:00, 639.38it/s]\n",
      "\n",
      "fold 231 took 7.984745025634766s, #samples=3135\n",
      "store to h5 file took 7.243214845657349s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7366340160369873s, #samples=3716\n",
      "100%|██████████| 3716/3716 [00:05<00:00, 651.38it/s]\n",
      "\n",
      "fold 232 took 9.17072319984436s, #samples=3716\n",
      "store to h5 file took 9.248275995254517s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.1031813621520996s, #samples=2841\n",
      "100%|██████████| 2841/2841 [00:04<00:00, 602.82it/s]\n",
      "\n",
      "fold 233 took 8.3330078125s, #samples=2841\n",
      "store to h5 file took 6.44330620765686s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7474114894866943s, #samples=3257\n",
      "100%|██████████| 3257/3257 [00:04<00:00, 651.43it/s]\n",
      "\n",
      "fold 234 took 8.265929698944092s, #samples=3257\n",
      "store to h5 file took 7.487389802932739s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8373539447784424s, #samples=4057\n",
      "100%|██████████| 4057/4057 [00:06<00:00, 639.12it/s]\n",
      "\n",
      "fold 235 took 9.678940057754517s, #samples=4057\n",
      "store to h5 file took 9.735414028167725s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6020662784576416s, #samples=3752\n",
      "100%|██████████| 3752/3752 [00:05<00:00, 631.96it/s]\n",
      "\n",
      "fold 236 took 9.217827081680298s, #samples=3752\n",
      "store to h5 file took 9.110103607177734s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8533942699432373s, #samples=2798\n",
      "100%|██████████| 2798/2798 [00:04<00:00, 662.47it/s]\n",
      "\n",
      "fold 237 took 7.617290258407593s, #samples=2798\n",
      "store to h5 file took 6.380370378494263s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6571791172027588s, #samples=3005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3005/3005 [00:04<00:00, 620.41it/s]\n",
      "\n",
      "fold 238 took 8.371150732040405s, #samples=3005\n",
      "store to h5 file took 7.039880752563477s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5016911029815674s, #samples=3254\n",
      "100%|██████████| 3254/3254 [00:05<00:00, 578.62it/s]\n",
      "\n",
      "fold 239 took 9.046669483184814s, #samples=3254\n",
      "store to h5 file took 8.005309343338013s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.30315518379211426s, #samples=3136\n",
      "100%|██████████| 3136/3136 [00:04<00:00, 673.03it/s]\n",
      "\n",
      "fold 240 took 7.907079696655273s, #samples=3136\n",
      "store to h5 file took 6.78567361831665s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4966249465942383s, #samples=2927\n",
      "100%|██████████| 2927/2927 [00:04<00:00, 626.75it/s]\n",
      "\n",
      "fold 241 took 8.06395435333252s, #samples=2927\n",
      "store to h5 file took 7.246431589126587s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.9280903339385986s, #samples=3709\n",
      "100%|██████████| 3709/3709 [00:05<00:00, 624.18it/s]\n",
      "\n",
      "fold 242 took 9.462614059448242s, #samples=3709\n",
      "store to h5 file took 8.00649380683899s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7366888523101807s, #samples=3408\n",
      "100%|██████████| 3408/3408 [00:05<00:00, 610.82it/s]\n",
      "\n",
      "fold 243 took 8.724841117858887s, #samples=3408\n",
      "store to h5 file took 8.051692962646484s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6253969669342041s, #samples=3651\n",
      "100%|██████████| 3651/3651 [00:05<00:00, 639.22it/s]\n",
      "\n",
      "fold 244 took 9.18273138999939s, #samples=3651\n",
      "store to h5 file took 8.463669300079346s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.544548749923706s, #samples=2803\n",
      "100%|██████████| 2803/2803 [00:04<00:00, 598.40it/s]\n",
      "\n",
      "fold 245 took 7.861412525177002s, #samples=2803\n",
      "store to h5 file took 7.03288722038269s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.11387014389038086s, #samples=2791\n",
      "100%|██████████| 2791/2791 [00:04<00:00, 574.06it/s]\n",
      "\n",
      "fold 246 took 8.511221647262573s, #samples=2791\n",
      "store to h5 file took 6.480396270751953s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5810608863830566s, #samples=3243\n",
      "100%|██████████| 3243/3243 [00:05<00:00, 609.89it/s]\n",
      "\n",
      "fold 247 took 8.630271434783936s, #samples=3243\n",
      "store to h5 file took 7.6516923904418945s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7381620407104492s, #samples=3258\n",
      "100%|██████████| 3258/3258 [00:05<00:00, 650.66it/s]\n",
      "\n",
      "fold 248 took 8.784570693969727s, #samples=3258\n",
      "store to h5 file took 7.303863763809204s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.15161728858947754s, #samples=2106\n",
      "100%|██████████| 2106/2106 [00:03<00:00, 637.95it/s]\n",
      "\n",
      "fold 249 took 6.932196378707886s, #samples=2106\n",
      "store to h5 file took 5.059063911437988s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2270975112915039s, #samples=2378\n",
      "100%|██████████| 2378/2378 [00:03<00:00, 652.50it/s]\n",
      "\n",
      "fold 250 took 6.885588645935059s, #samples=2378\n",
      "store to h5 file took 5.310964345932007s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2279367446899414s, #samples=2653\n",
      "100%|██████████| 2653/2653 [00:04<00:00, 582.10it/s]\n",
      "\n",
      "fold 251 took 7.884080648422241s, #samples=2653\n",
      "store to h5 file took 5.991811990737915s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.48975586891174316s, #samples=2744\n",
      "100%|██████████| 2744/2744 [00:04<00:00, 595.85it/s]\n",
      "\n",
      "fold 252 took 7.829482555389404s, #samples=2744\n",
      "store to h5 file took 5.778399705886841s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.255018949508667s, #samples=2691\n",
      "100%|██████████| 2691/2691 [00:04<00:00, 598.20it/s]\n",
      "\n",
      "fold 253 took 8.306252241134644s, #samples=2691\n",
      "store to h5 file took 6.4329514503479s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.9299962520599365s, #samples=3450\n",
      "100%|██████████| 3450/3450 [00:05<00:00, 637.54it/s]\n",
      "\n",
      "fold 254 took 8.6686270236969s, #samples=3450\n",
      "store to h5 file took 8.843438863754272s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6936724185943604s, #samples=2935\n",
      "100%|██████████| 2935/2935 [00:04<00:00, 656.52it/s]\n",
      "\n",
      "fold 255 took 8.013648748397827s, #samples=2935\n",
      "store to h5 file took 6.697495698928833s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3668630123138428s, #samples=3161\n",
      "100%|██████████| 3161/3161 [00:05<00:00, 609.54it/s]\n",
      "\n",
      "fold 256 took 8.778863906860352s, #samples=3161\n",
      "store to h5 file took 7.827451705932617s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8583574295043945s, #samples=2794\n",
      "100%|██████████| 2794/2794 [00:04<00:00, 668.85it/s]\n",
      "\n",
      "fold 257 took 7.815183162689209s, #samples=2794\n",
      "store to h5 file took 6.303173542022705s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7183055877685547s, #samples=2631\n",
      "100%|██████████| 2631/2631 [00:04<00:00, 615.93it/s]\n",
      "\n",
      "fold 258 took 7.647629261016846s, #samples=2631\n",
      "store to h5 file took 6.046899080276489s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.501960277557373s, #samples=3458\n",
      "100%|██████████| 3458/3458 [00:05<00:00, 596.44it/s]\n",
      "\n",
      "fold 259 took 9.514893054962158s, #samples=3458\n",
      "store to h5 file took 8.025691032409668s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.39122986793518066s, #samples=3122\n",
      "100%|██████████| 3122/3122 [00:04<00:00, 627.15it/s]\n",
      "\n",
      "fold 260 took 8.5463387966156s, #samples=3122\n",
      "store to h5 file took 8.04268217086792s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8031110763549805s, #samples=3201\n",
      "100%|██████████| 3201/3201 [00:05<00:00, 619.52it/s]\n",
      "\n",
      "fold 261 took 8.653793334960938s, #samples=3201\n",
      "store to h5 file took 7.403200149536133s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7000758647918701s, #samples=3366\n",
      "100%|██████████| 3366/3366 [00:05<00:00, 620.14it/s]\n",
      "\n",
      "fold 262 took 8.7434241771698s, #samples=3366\n",
      "store to h5 file took 7.96229100227356s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4028315544128418s, #samples=3515\n",
      "100%|██████████| 3515/3515 [00:05<00:00, 631.01it/s]\n",
      "\n",
      "fold 263 took 9.205700874328613s, #samples=3515\n",
      "store to h5 file took 8.48992395401001s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.1435723304748535s, #samples=2724\n",
      "100%|██████████| 2724/2724 [00:04<00:00, 641.46it/s]\n",
      "\n",
      "fold 264 took 7.780780792236328s, #samples=2724\n",
      "store to h5 file took 6.3682873249053955s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3910374641418457s, #samples=2788\n",
      "100%|██████████| 2788/2788 [00:04<00:00, 643.19it/s]\n",
      "\n",
      "fold 265 took 7.993722200393677s, #samples=2788\n",
      "store to h5 file took 6.392683744430542s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.0232534408569336s, #samples=3731\n",
      "100%|██████████| 3731/3731 [00:05<00:00, 628.62it/s]\n",
      "\n",
      "fold 266 took 9.400750160217285s, #samples=3731\n",
      "store to h5 file took 8.565322637557983s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8708243370056152s, #samples=3011\n",
      "100%|██████████| 3011/3011 [00:04<00:00, 602.91it/s]\n",
      "\n",
      "fold 267 took 8.559224843978882s, #samples=3011\n",
      "store to h5 file took 7.066305160522461s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4379079341888428s, #samples=3412\n",
      "100%|██████████| 3412/3412 [00:05<00:00, 604.50it/s]\n",
      "\n",
      "fold 268 took 8.903371572494507s, #samples=3412\n",
      "store to h5 file took 7.976104974746704s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5365297794342041s, #samples=3603\n",
      "100%|██████████| 3603/3603 [00:05<00:00, 624.26it/s]\n",
      "\n",
      "fold 269 took 9.268045902252197s, #samples=3603\n",
      "store to h5 file took 8.901078462600708s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.48021483421325684s, #samples=3046\n",
      "100%|██████████| 3046/3046 [00:04<00:00, 641.25it/s]\n",
      "\n",
      "fold 270 took 8.451837062835693s, #samples=3046\n",
      "store to h5 file took 7.376286268234253s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5533511638641357s, #samples=2371\n",
      "100%|██████████| 2371/2371 [00:03<00:00, 639.53it/s]\n",
      "\n",
      "fold 271 took 7.0431153774261475s, #samples=2371\n",
      "store to h5 file took 5.2061967849731445s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.0780038833618164s, #samples=2624\n",
      "100%|██████████| 2624/2624 [00:04<00:00, 614.59it/s]\n",
      "\n",
      "fold 272 took 7.991690635681152s, #samples=2624\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store to h5 file took 6.632167100906372s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.89103102684021s, #samples=3194\n",
      "100%|██████████| 3194/3194 [00:04<00:00, 643.77it/s]\n",
      "\n",
      "fold 273 took 8.498438596725464s, #samples=3194\n",
      "store to h5 file took 7.158761501312256s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6029317378997803s, #samples=2929\n",
      "100%|██████████| 2929/2929 [00:04<00:00, 637.36it/s]\n",
      "\n",
      "fold 274 took 7.960360288619995s, #samples=2929\n",
      "store to h5 file took 7.333982229232788s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5535218715667725s, #samples=2886\n",
      "100%|██████████| 2886/2886 [00:04<00:00, 638.96it/s]\n",
      "\n",
      "fold 275 took 7.9075236320495605s, #samples=2886\n",
      "store to h5 file took 6.641973495483398s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.9439654350280762s, #samples=2835\n",
      "100%|██████████| 2835/2835 [00:04<00:00, 651.20it/s]\n",
      "\n",
      "fold 276 took 7.7041051387786865s, #samples=2835\n",
      "store to h5 file took 6.36326265335083s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5046539306640625s, #samples=2655\n",
      "100%|██████████| 2655/2655 [00:04<00:00, 641.49it/s]\n",
      "\n",
      "fold 277 took 7.379967212677002s, #samples=2655\n",
      "store to h5 file took 6.181349277496338s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4083118438720703s, #samples=3318\n",
      "100%|██████████| 3318/3318 [00:05<00:00, 612.41it/s]\n",
      "\n",
      "fold 278 took 9.046028852462769s, #samples=3318\n",
      "store to h5 file took 7.452649831771851s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.967296838760376s, #samples=2590\n",
      "100%|██████████| 2590/2590 [00:04<00:00, 609.80it/s]\n",
      "\n",
      "fold 279 took 7.526956796646118s, #samples=2590\n",
      "store to h5 file took 6.46305775642395s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.21944212913513184s, #samples=2768\n",
      "100%|██████████| 2768/2768 [00:04<00:00, 639.63it/s]\n",
      "\n",
      "fold 280 took 7.731133222579956s, #samples=2768\n",
      "store to h5 file took 6.2898170948028564s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3313007354736328s, #samples=3274\n",
      "100%|██████████| 3274/3274 [00:05<00:00, 647.83it/s]\n",
      "\n",
      "fold 281 took 8.683685302734375s, #samples=3274\n",
      "store to h5 file took 8.27382230758667s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5609900951385498s, #samples=2850\n",
      "100%|██████████| 2850/2850 [00:04<00:00, 626.96it/s]\n",
      "\n",
      "fold 282 took 7.908937692642212s, #samples=2850\n",
      "store to h5 file took 6.528446197509766s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8915157318115234s, #samples=3289\n",
      "100%|██████████| 3289/3289 [00:05<00:00, 633.86it/s]\n",
      "\n",
      "fold 283 took 8.559540510177612s, #samples=3289\n",
      "store to h5 file took 7.52693510055542s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6942262649536133s, #samples=2929\n",
      "100%|██████████| 2929/2929 [00:04<00:00, 639.22it/s]\n",
      "\n",
      "fold 284 took 8.26751971244812s, #samples=2929\n",
      "store to h5 file took 7.191658020019531s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4608917236328125s, #samples=2676\n",
      "100%|██████████| 2676/2676 [00:04<00:00, 623.04it/s]\n",
      "\n",
      "fold 285 took 7.674574613571167s, #samples=2676\n",
      "store to h5 file took 5.868433237075806s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.42674779891967773s, #samples=3566\n",
      "100%|██████████| 3566/3566 [00:05<00:00, 618.80it/s]\n",
      "\n",
      "fold 286 took 9.431772232055664s, #samples=3566\n",
      "store to h5 file took 8.524597644805908s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.37894201278686523s, #samples=2652\n",
      "100%|██████████| 2652/2652 [00:04<00:00, 618.24it/s]\n",
      "\n",
      "fold 287 took 7.803597688674927s, #samples=2652\n",
      "store to h5 file took 6.064498662948608s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.37335729598999023s, #samples=3581\n",
      "100%|██████████| 3581/3581 [00:05<00:00, 624.31it/s]\n",
      "\n",
      "fold 288 took 9.589618444442749s, #samples=3581\n",
      "store to h5 file took 8.572788000106812s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4493868350982666s, #samples=3817\n",
      "100%|██████████| 3817/3817 [00:06<00:00, 625.64it/s]\n",
      "\n",
      "fold 289 took 9.682309627532959s, #samples=3817\n",
      "store to h5 file took 9.004107475280762s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.390592098236084s, #samples=3713\n",
      "100%|██████████| 3713/3713 [00:06<00:00, 598.22it/s]\n",
      "\n",
      "fold 290 took 9.59311318397522s, #samples=3713\n",
      "store to h5 file took 8.487014532089233s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3596940040588379s, #samples=3542\n",
      "100%|██████████| 3542/3542 [00:05<00:00, 639.34it/s]\n",
      "\n",
      "fold 291 took 9.117989540100098s, #samples=3542\n",
      "store to h5 file took 8.627303838729858s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7077338695526123s, #samples=3060\n",
      "100%|██████████| 3060/3060 [00:04<00:00, 645.47it/s]\n",
      "\n",
      "fold 292 took 8.487533807754517s, #samples=3060\n",
      "store to h5 file took 6.769854307174683s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.708254337310791s, #samples=3318\n",
      "100%|██████████| 3318/3318 [00:05<00:00, 607.70it/s]\n",
      "\n",
      "fold 293 took 9.072171926498413s, #samples=3318\n",
      "store to h5 file took 8.299673080444336s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7609965801239014s, #samples=3544\n",
      "100%|██████████| 3544/3544 [00:05<00:00, 626.49it/s]\n",
      "\n",
      "fold 294 took 9.103237628936768s, #samples=3544\n",
      "store to h5 file took 8.4535653591156s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5282301902770996s, #samples=2940\n",
      "100%|██████████| 2940/2940 [00:04<00:00, 667.56it/s]\n",
      "\n",
      "fold 295 took 8.244183540344238s, #samples=2940\n",
      "store to h5 file took 7.721033811569214s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5773599147796631s, #samples=4515\n",
      "100%|██████████| 4515/4515 [00:07<00:00, 614.55it/s]\n",
      "\n",
      "fold 296 took 10.929374933242798s, #samples=4515\n",
      "store to h5 file took 11.5077486038208s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.342353105545044s, #samples=3034\n",
      "100%|██████████| 3034/3034 [00:05<00:00, 588.73it/s]\n",
      "\n",
      "fold 297 took 8.64159345626831s, #samples=3034\n",
      "store to h5 file took 7.045726776123047s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6014680862426758s, #samples=3692\n",
      "100%|██████████| 3692/3692 [00:05<00:00, 630.49it/s]\n",
      "\n",
      "fold 298 took 9.356626033782959s, #samples=3692\n",
      "store to h5 file took 8.473519086837769s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.45656609535217285s, #samples=3417\n",
      "100%|██████████| 3417/3417 [00:05<00:00, 663.53it/s]\n",
      "\n",
      "fold 299 took 8.496792316436768s, #samples=3417\n",
      "store to h5 file took 7.922048091888428s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5361590385437012s, #samples=2923\n",
      "100%|██████████| 2923/2923 [00:04<00:00, 619.60it/s]\n",
      "\n",
      "fold 300 took 8.193075180053711s, #samples=2923\n",
      "store to h5 file took 6.732162952423096s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.0353667736053467s, #samples=3897\n",
      "100%|██████████| 3897/3897 [00:06<00:00, 606.40it/s]\n",
      "\n",
      "fold 301 took 10.130365133285522s, #samples=3897\n",
      "store to h5 file took 9.368009567260742s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8710360527038574s, #samples=3716\n",
      "100%|██████████| 3716/3716 [00:06<00:00, 588.07it/s]\n",
      "\n",
      "fold 302 took 9.619133234024048s, #samples=3716\n",
      "store to h5 file took 9.270668983459473s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7838864326477051s, #samples=3748\n",
      "100%|██████████| 3748/3748 [00:05<00:00, 652.19it/s]\n",
      "\n",
      "fold 303 took 9.570752620697021s, #samples=3748\n",
      "store to h5 file took 8.651484489440918s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.453260898590088s, #samples=3072\n",
      "100%|██████████| 3072/3072 [00:04<00:00, 624.35it/s]\n",
      "\n",
      "fold 304 took 8.176202774047852s, #samples=3072\n",
      "store to h5 file took 6.671477556228638s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3126382827758789s, #samples=2440\n",
      "100%|██████████| 2440/2440 [00:04<00:00, 541.71it/s]\n",
      "\n",
      "fold 305 took 8.10939884185791s, #samples=2440\n",
      "store to h5 file took 5.255326271057129s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3884732723236084s, #samples=2677\n",
      "100%|██████████| 2677/2677 [00:04<00:00, 612.08it/s]\n",
      "\n",
      "fold 306 took 8.034493684768677s, #samples=2677\n",
      "store to h5 file took 6.363059759140015s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constrcuting file list took 0.43418288230895996s, #samples=2762\n",
      "100%|██████████| 2762/2762 [00:04<00:00, 618.28it/s]\n",
      "\n",
      "fold 307 took 7.767933368682861s, #samples=2762\n",
      "store to h5 file took 6.301193475723267s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2832214832305908s, #samples=3015\n",
      "100%|██████████| 3015/3015 [00:05<00:00, 596.48it/s]\n",
      "\n",
      "fold 308 took 8.58064317703247s, #samples=3015\n",
      "store to h5 file took 7.033282995223999s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.44623732566833496s, #samples=3467\n",
      "100%|██████████| 3467/3467 [00:05<00:00, 592.42it/s]\n",
      "\n",
      "fold 309 took 9.317676544189453s, #samples=3467\n",
      "store to h5 file took 8.069223880767822s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.65679931640625s, #samples=2435\n",
      "100%|██████████| 2435/2435 [00:04<00:00, 566.00it/s]\n",
      "\n",
      "fold 310 took 7.42851185798645s, #samples=2435\n",
      "store to h5 file took 5.919002532958984s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.21936345100402832s, #samples=3286\n",
      "100%|██████████| 3286/3286 [00:05<00:00, 584.98it/s]\n",
      "\n",
      "fold 311 took 9.205417394638062s, #samples=3286\n",
      "store to h5 file took 8.255220174789429s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4498281478881836s, #samples=3368\n",
      "100%|██████████| 3368/3368 [00:05<00:00, 621.87it/s]\n",
      "\n",
      "fold 312 took 9.053472757339478s, #samples=3368\n",
      "store to h5 file took 7.7437334060668945s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6549727916717529s, #samples=4016\n",
      "100%|██████████| 4016/4016 [00:06<00:00, 617.93it/s]\n",
      "\n",
      "fold 313 took 10.245676517486572s, #samples=4016\n",
      "store to h5 file took 9.388066291809082s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3882429599761963s, #samples=3382\n",
      "100%|██████████| 3382/3382 [00:05<00:00, 599.32it/s]\n",
      "\n",
      "fold 314 took 8.949249982833862s, #samples=3382\n",
      "store to h5 file took 7.710333347320557s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.44971466064453125s, #samples=3128\n",
      "100%|██████████| 3128/3128 [00:04<00:00, 641.82it/s]\n",
      "\n",
      "fold 315 took 8.547249794006348s, #samples=3128\n",
      "store to h5 file took 7.536967515945435s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5857317447662354s, #samples=2770\n",
      "100%|██████████| 2770/2770 [00:04<00:00, 619.59it/s]\n",
      "\n",
      "fold 316 took 7.952731132507324s, #samples=2770\n",
      "store to h5 file took 5.750832796096802s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.14767003059387207s, #samples=2852\n",
      "100%|██████████| 2852/2852 [00:04<00:00, 655.07it/s]\n",
      "\n",
      "fold 317 took 7.947494983673096s, #samples=2852\n",
      "store to h5 file took 6.191462278366089s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5680999755859375s, #samples=2661\n",
      "100%|██████████| 2661/2661 [00:04<00:00, 620.14it/s]\n",
      "\n",
      "fold 318 took 7.853594541549683s, #samples=2661\n",
      "store to h5 file took 5.943794012069702s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.0482983589172363s, #samples=3239\n",
      "100%|██████████| 3239/3239 [00:05<00:00, 520.96it/s]\n",
      "\n",
      "fold 319 took 9.181118726730347s, #samples=3239\n",
      "store to h5 file took 7.749772310256958s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6774628162384033s, #samples=2725\n",
      "100%|██████████| 2725/2725 [00:04<00:00, 626.97it/s]\n",
      "\n",
      "fold 320 took 7.393001317977905s, #samples=2725\n",
      "store to h5 file took 6.5213611125946045s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6109521389007568s, #samples=2934\n",
      "100%|██████████| 2934/2934 [00:04<00:00, 647.99it/s]\n",
      "\n",
      "fold 321 took 8.172177791595459s, #samples=2934\n",
      "store to h5 file took 7.096685171127319s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3639512062072754s, #samples=2877\n",
      "100%|██████████| 2877/2877 [00:04<00:00, 596.10it/s]\n",
      "\n",
      "fold 322 took 8.153568983078003s, #samples=2877\n",
      "store to h5 file took 6.006331443786621s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.40410661697387695s, #samples=3440\n",
      "100%|██████████| 3440/3440 [00:05<00:00, 628.68it/s]\n",
      "\n",
      "fold 323 took 9.001991987228394s, #samples=3440\n",
      "store to h5 file took 7.858613014221191s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.9618151187896729s, #samples=3669\n",
      "100%|██████████| 3669/3669 [00:05<00:00, 631.67it/s]\n",
      "\n",
      "fold 324 took 9.432779788970947s, #samples=3669\n",
      "store to h5 file took 8.661668539047241s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5705833435058594s, #samples=3016\n",
      "100%|██████████| 3016/3016 [00:04<00:00, 608.16it/s]\n",
      "\n",
      "fold 325 took 8.417672157287598s, #samples=3016\n",
      "store to h5 file took 7.037777900695801s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8879728317260742s, #samples=3620\n",
      "100%|██████████| 3620/3620 [00:05<00:00, 635.02it/s]\n",
      "\n",
      "fold 326 took 9.199185132980347s, #samples=3620\n",
      "store to h5 file took 8.313852310180664s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.40774083137512207s, #samples=4327\n",
      "100%|██████████| 4327/4327 [00:06<00:00, 629.12it/s]\n",
      "\n",
      "fold 327 took 10.241333246231079s, #samples=4327\n",
      "store to h5 file took 10.235229730606079s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8753976821899414s, #samples=3383\n",
      "100%|██████████| 3383/3383 [00:05<00:00, 632.20it/s]\n",
      "\n",
      "fold 328 took 8.822031259536743s, #samples=3383\n",
      "store to h5 file took 7.410874843597412s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7164750099182129s, #samples=3196\n",
      "100%|██████████| 3196/3196 [00:04<00:00, 645.08it/s]\n",
      "\n",
      "fold 329 took 8.842041730880737s, #samples=3196\n",
      "store to h5 file took 7.68735671043396s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7578322887420654s, #samples=2365\n",
      "100%|██████████| 2365/2365 [00:04<00:00, 588.02it/s]\n",
      "\n",
      "fold 330 took 7.61594820022583s, #samples=2365\n",
      "store to h5 file took 5.601335525512695s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8207430839538574s, #samples=3154\n",
      "100%|██████████| 3154/3154 [00:05<00:00, 618.32it/s]\n",
      "\n",
      "fold 331 took 8.833113193511963s, #samples=3154\n",
      "store to h5 file took 7.498455762863159s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4206504821777344s, #samples=3125\n",
      "100%|██████████| 3125/3125 [00:04<00:00, 636.69it/s]\n",
      "\n",
      "fold 332 took 8.924522876739502s, #samples=3125\n",
      "store to h5 file took 6.995826244354248s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.364077091217041s, #samples=3688\n",
      "100%|██████████| 3688/3688 [00:05<00:00, 626.01it/s]\n",
      "\n",
      "fold 333 took 9.155672788619995s, #samples=3688\n",
      "store to h5 file took 8.712274312973022s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4185953140258789s, #samples=3178\n",
      "100%|██████████| 3178/3178 [00:05<00:00, 633.78it/s]\n",
      "\n",
      "fold 334 took 8.797961473464966s, #samples=3178\n",
      "store to h5 file took 7.80413556098938s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.40213584899902344s, #samples=3395\n",
      "100%|██████████| 3395/3395 [00:05<00:00, 663.12it/s]\n",
      "\n",
      "fold 335 took 8.77544116973877s, #samples=3395\n",
      "store to h5 file took 7.685081243515015s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.43246912956237793s, #samples=3118\n",
      "100%|██████████| 3118/3118 [00:05<00:00, 622.30it/s]\n",
      "\n",
      "fold 336 took 8.680719137191772s, #samples=3118\n",
      "store to h5 file took 7.081284523010254s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3771522045135498s, #samples=4296\n",
      "100%|██████████| 4296/4296 [00:06<00:00, 665.72it/s]\n",
      "\n",
      "fold 337 took 9.996451377868652s, #samples=4296\n",
      "store to h5 file took 10.46341347694397s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5290849208831787s, #samples=3653\n",
      "100%|██████████| 3653/3653 [00:06<00:00, 571.87it/s]\n",
      "\n",
      "fold 338 took 9.679396867752075s, #samples=3653\n",
      "store to h5 file took 8.19193410873413s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4468560218811035s, #samples=3260\n",
      "100%|██████████| 3260/3260 [00:05<00:00, 646.87it/s]\n",
      "\n",
      "fold 339 took 8.227129220962524s, #samples=3260\n",
      "store to h5 file took 8.295497179031372s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4647982120513916s, #samples=3310\n",
      "100%|██████████| 3310/3310 [00:05<00:00, 576.28it/s]\n",
      "\n",
      "fold 340 took 9.255355596542358s, #samples=3310\n",
      "store to h5 file took 7.5431435108184814s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "constrcuting file list took 0.2593531608581543s, #samples=3221\n",
      "100%|██████████| 3221/3221 [00:05<00:00, 636.84it/s]\n",
      "\n",
      "fold 341 took 8.162222146987915s, #samples=3221\n",
      "store to h5 file took 6.773712396621704s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4850955009460449s, #samples=3222\n",
      "100%|██████████| 3222/3222 [00:05<00:00, 600.96it/s]\n",
      "\n",
      "fold 342 took 9.002705335617065s, #samples=3222\n",
      "store to h5 file took 7.815322399139404s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4357719421386719s, #samples=3267\n",
      "100%|██████████| 3267/3267 [00:05<00:00, 639.57it/s]\n",
      "\n",
      "fold 343 took 8.632122039794922s, #samples=3267\n",
      "store to h5 file took 7.843161582946777s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.43337154388427734s, #samples=3016\n",
      "100%|██████████| 3016/3016 [00:04<00:00, 634.98it/s]\n",
      "\n",
      "fold 344 took 8.361802101135254s, #samples=3016\n",
      "store to h5 file took 6.979812383651733s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.9352829456329346s, #samples=2937\n",
      "100%|██████████| 2937/2937 [00:04<00:00, 613.95it/s]\n",
      "\n",
      "fold 345 took 7.970193147659302s, #samples=2937\n",
      "store to h5 file took 6.771195888519287s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7421669960021973s, #samples=2808\n",
      "100%|██████████| 2808/2808 [00:04<00:00, 652.38it/s]\n",
      "\n",
      "fold 346 took 7.793668746948242s, #samples=2808\n",
      "store to h5 file took 6.391190767288208s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.29770612716674805s, #samples=2599\n",
      "100%|██████████| 2599/2599 [00:04<00:00, 612.64it/s]\n",
      "\n",
      "fold 347 took 7.417028427124023s, #samples=2599\n",
      "store to h5 file took 5.776181221008301s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.9548194408416748s, #samples=2836\n",
      "100%|██████████| 2836/2836 [00:04<00:00, 616.87it/s]\n",
      "\n",
      "fold 348 took 8.28379225730896s, #samples=2836\n",
      "store to h5 file took 7.004284381866455s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8802590370178223s, #samples=3993\n",
      "100%|██████████| 3993/3993 [00:06<00:00, 623.57it/s]\n",
      "\n",
      "fold 349 took 9.92536211013794s, #samples=3993\n",
      "store to h5 file took 9.486523151397705s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.41637253761291504s, #samples=3272\n",
      "100%|██████████| 3272/3272 [00:05<00:00, 596.34it/s]\n",
      "\n",
      "fold 350 took 9.036797046661377s, #samples=3272\n",
      "store to h5 file took 7.480865240097046s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.48148298263549805s, #samples=3292\n",
      "100%|██████████| 3292/3292 [00:05<00:00, 612.15it/s]\n",
      "\n",
      "fold 351 took 9.082588195800781s, #samples=3292\n",
      "store to h5 file took 7.785479784011841s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5306620597839355s, #samples=3407\n",
      "100%|██████████| 3407/3407 [00:05<00:00, 619.48it/s]\n",
      "\n",
      "fold 352 took 9.108853578567505s, #samples=3407\n",
      "store to h5 file took 8.058814764022827s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5641891956329346s, #samples=3436\n",
      "100%|██████████| 3436/3436 [00:05<00:00, 592.47it/s]\n",
      "\n",
      "fold 353 took 9.353192567825317s, #samples=3436\n",
      "store to h5 file took 8.884387731552124s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.38606762886047363s, #samples=3289\n",
      "100%|██████████| 3289/3289 [00:05<00:00, 632.28it/s]\n",
      "\n",
      "fold 354 took 8.885046005249023s, #samples=3289\n",
      "store to h5 file took 7.585401296615601s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.1936178207397461s, #samples=3502\n",
      "100%|██████████| 3502/3502 [00:05<00:00, 657.37it/s]\n",
      "\n",
      "fold 355 took 8.763685703277588s, #samples=3502\n",
      "store to h5 file took 8.134777545928955s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5006668567657471s, #samples=3853\n",
      "100%|██████████| 3853/3853 [00:06<00:00, 617.75it/s]\n",
      "\n",
      "fold 356 took 9.565353155136108s, #samples=3853\n",
      "store to h5 file took 8.824119806289673s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.46190762519836426s, #samples=3519\n",
      "100%|██████████| 3519/3519 [00:05<00:00, 688.25it/s]\n",
      "\n",
      "fold 357 took 8.636706352233887s, #samples=3519\n",
      "store to h5 file took 8.625632047653198s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4136512279510498s, #samples=3654\n",
      "100%|██████████| 3654/3654 [00:05<00:00, 609.76it/s]\n",
      "\n",
      "fold 358 took 9.624301671981812s, #samples=3654\n",
      "store to h5 file took 8.508208990097046s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.0309538841247559s, #samples=3276\n",
      "100%|██████████| 3276/3276 [00:04<00:00, 657.41it/s]\n",
      "\n",
      "fold 359 took 8.49087929725647s, #samples=3276\n",
      "store to h5 file took 7.202956199645996s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7618205547332764s, #samples=3029\n",
      "100%|██████████| 3029/3029 [00:04<00:00, 636.99it/s]\n",
      "\n",
      "fold 360 took 8.128126382827759s, #samples=3029\n",
      "store to h5 file took 7.039799690246582s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.089045524597168s, #samples=2817\n",
      "100%|██████████| 2817/2817 [00:04<00:00, 629.44it/s]\n",
      "\n",
      "fold 361 took 8.08360743522644s, #samples=2817\n",
      "store to h5 file took 6.187617063522339s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.8081870079040527s, #samples=3076\n",
      "100%|██████████| 3076/3076 [00:04<00:00, 626.10it/s]\n",
      "\n",
      "fold 362 took 8.367786884307861s, #samples=3076\n",
      "store to h5 file took 7.398534297943115s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.202542781829834s, #samples=3087\n",
      "100%|██████████| 3087/3087 [00:05<00:00, 616.74it/s]\n",
      "\n",
      "fold 363 took 8.449916124343872s, #samples=3087\n",
      "store to h5 file took 7.150402069091797s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7779388427734375s, #samples=2988\n",
      "100%|██████████| 2988/2988 [00:04<00:00, 646.73it/s]\n",
      "\n",
      "fold 364 took 8.242639064788818s, #samples=2988\n",
      "store to h5 file took 7.617762327194214s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5744330883026123s, #samples=3525\n",
      "100%|██████████| 3525/3525 [00:05<00:00, 651.38it/s]\n",
      "\n",
      "fold 365 took 8.896385431289673s, #samples=3525\n",
      "store to h5 file took 7.96894645690918s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.9322066307067871s, #samples=3717\n",
      "100%|██████████| 3717/3717 [00:05<00:00, 678.30it/s]\n",
      "\n",
      "fold 366 took 8.823745250701904s, #samples=3717\n",
      "store to h5 file took 8.79778790473938s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6534261703491211s, #samples=3253\n",
      "100%|██████████| 3253/3253 [00:05<00:00, 581.74it/s]\n",
      "\n",
      "fold 367 took 9.102640867233276s, #samples=3253\n",
      "store to h5 file took 7.813427448272705s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.44166135787963867s, #samples=3228\n",
      "100%|██████████| 3228/3228 [00:05<00:00, 624.12it/s]\n",
      "\n",
      "fold 368 took 8.603918552398682s, #samples=3228\n",
      "store to h5 file took 7.473206996917725s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4681980609893799s, #samples=3309\n",
      "100%|██████████| 3309/3309 [00:05<00:00, 616.83it/s]\n",
      "\n",
      "fold 369 took 8.644111633300781s, #samples=3309\n",
      "store to h5 file took 7.921477317810059s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.40483689308166504s, #samples=3272\n",
      "100%|██████████| 3272/3272 [00:05<00:00, 649.90it/s]\n",
      "\n",
      "fold 370 took 8.763154983520508s, #samples=3272\n",
      "store to h5 file took 7.405132055282593s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.39856648445129395s, #samples=3430\n",
      "100%|██████████| 3430/3430 [00:05<00:00, 624.00it/s]\n",
      "\n",
      "fold 371 took 8.9045569896698s, #samples=3430\n",
      "store to h5 file took 8.388002395629883s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3713352680206299s, #samples=3087\n",
      "100%|██████████| 3087/3087 [00:05<00:00, 615.10it/s]\n",
      "\n",
      "fold 372 took 8.441421031951904s, #samples=3087\n",
      "store to h5 file took 7.266700267791748s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.41609859466552734s, #samples=2803\n",
      "100%|██████████| 2803/2803 [00:04<00:00, 604.30it/s]\n",
      "\n",
      "fold 373 took 8.07815146446228s, #samples=2803\n",
      "store to h5 file took 6.380353689193726s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.30112504959106445s, #samples=3355\n",
      "100%|██████████| 3355/3355 [00:05<00:00, 596.76it/s]\n",
      "\n",
      "fold 374 took 9.19197702407837s, #samples=3355\n",
      "store to h5 file took 8.53849720954895s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.834911584854126s, #samples=3325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3325/3325 [00:05<00:00, 664.52it/s]\n",
      "\n",
      "fold 375 took 8.718817949295044s, #samples=3325\n",
      "store to h5 file took 7.90099024772644s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.1463253498077393s, #samples=2821\n",
      "100%|██████████| 2821/2821 [00:04<00:00, 613.79it/s]\n",
      "\n",
      "fold 376 took 8.283306360244751s, #samples=2821\n",
      "store to h5 file took 6.3786468505859375s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5194580554962158s, #samples=3536\n",
      "100%|██████████| 3536/3536 [00:05<00:00, 645.96it/s]\n",
      "\n",
      "fold 377 took 8.99210500717163s, #samples=3536\n",
      "store to h5 file took 8.382829904556274s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.2963576316833496s, #samples=2908\n",
      "100%|██████████| 2908/2908 [00:04<00:00, 651.47it/s]\n",
      "\n",
      "fold 378 took 8.01525592803955s, #samples=2908\n",
      "store to h5 file took 7.0658159255981445s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3031473159790039s, #samples=3103\n",
      "100%|██████████| 3103/3103 [00:05<00:00, 620.41it/s]\n",
      "\n",
      "fold 379 took 8.320807695388794s, #samples=3103\n",
      "store to h5 file took 6.971530437469482s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3662130832672119s, #samples=2599\n",
      "100%|██████████| 2599/2599 [00:04<00:00, 636.35it/s]\n",
      "\n",
      "fold 380 took 7.419769048690796s, #samples=2599\n",
      "store to h5 file took 6.194820880889893s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3342914581298828s, #samples=3115\n",
      "100%|██████████| 3115/3115 [00:05<00:00, 602.13it/s]\n",
      "\n",
      "fold 381 took 8.388828992843628s, #samples=3115\n",
      "store to h5 file took 7.252782821655273s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3565940856933594s, #samples=3190\n",
      "100%|██████████| 3190/3190 [00:04<00:00, 664.76it/s]\n",
      "\n",
      "fold 382 took 8.577081680297852s, #samples=3190\n",
      "store to h5 file took 7.539694786071777s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5107359886169434s, #samples=3884\n",
      "100%|██████████| 3884/3884 [00:06<00:00, 598.91it/s]\n",
      "\n",
      "fold 383 took 10.26248574256897s, #samples=3884\n",
      "store to h5 file took 9.493505001068115s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7934105396270752s, #samples=2921\n",
      "100%|██████████| 2921/2921 [00:04<00:00, 660.63it/s]\n",
      "\n",
      "fold 384 took 8.10843276977539s, #samples=2921\n",
      "store to h5 file took 6.529819488525391s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6625134944915771s, #samples=2953\n",
      "100%|██████████| 2953/2953 [00:04<00:00, 600.22it/s]\n",
      "\n",
      "fold 385 took 8.3472580909729s, #samples=2953\n",
      "store to h5 file took 7.028250694274902s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4910926818847656s, #samples=3644\n",
      "100%|██████████| 3644/3644 [00:05<00:00, 636.48it/s]\n",
      "\n",
      "fold 386 took 9.200662612915039s, #samples=3644\n",
      "store to h5 file took 7.937472581863403s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4976041316986084s, #samples=3226\n",
      "100%|██████████| 3226/3226 [00:04<00:00, 659.02it/s]\n",
      "\n",
      "fold 387 took 8.634731769561768s, #samples=3226\n",
      "store to h5 file took 7.5742950439453125s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.925609827041626s, #samples=3700\n",
      "100%|██████████| 3700/3700 [00:05<00:00, 620.80it/s]\n",
      "\n",
      "fold 388 took 9.37927532196045s, #samples=3700\n",
      "store to h5 file took 8.269754886627197s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7758288383483887s, #samples=3212\n",
      "100%|██████████| 3212/3212 [00:05<00:00, 620.01it/s]\n",
      "\n",
      "fold 389 took 8.620607614517212s, #samples=3212\n",
      "store to h5 file took 7.493045806884766s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.42900705337524414s, #samples=3067\n",
      "100%|██████████| 3067/3067 [00:04<00:00, 622.89it/s]\n",
      "\n",
      "fold 390 took 8.381624698638916s, #samples=3067\n",
      "store to h5 file took 7.1300787925720215s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4104599952697754s, #samples=3150\n",
      "100%|██████████| 3150/3150 [00:05<00:00, 587.31it/s]\n",
      "\n",
      "fold 391 took 8.581774473190308s, #samples=3150\n",
      "store to h5 file took 7.816507339477539s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.0762500762939453s, #samples=3191\n",
      "100%|██████████| 3191/3191 [00:05<00:00, 615.55it/s]\n",
      "\n",
      "fold 392 took 8.596115350723267s, #samples=3191\n",
      "store to h5 file took 7.459721326828003s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7749848365783691s, #samples=3227\n",
      "100%|██████████| 3227/3227 [00:05<00:00, 595.45it/s]\n",
      "\n",
      "fold 393 took 8.795835256576538s, #samples=3227\n",
      "store to h5 file took 7.318239450454712s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4468710422515869s, #samples=3073\n",
      "100%|██████████| 3073/3073 [00:05<00:00, 607.33it/s]\n",
      "\n",
      "fold 394 took 8.702813625335693s, #samples=3073\n",
      "store to h5 file took 7.1010308265686035s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.4368405342102051s, #samples=2741\n",
      "100%|██████████| 2741/2741 [00:04<00:00, 590.14it/s]\n",
      "\n",
      "fold 395 took 8.24040961265564s, #samples=2741\n",
      "store to h5 file took 6.377635478973389s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.3023948669433594s, #samples=3044\n",
      "100%|██████████| 3044/3044 [00:04<00:00, 629.76it/s]\n",
      "\n",
      "fold 396 took 8.252480506896973s, #samples=3044\n",
      "store to h5 file took 7.218716382980347s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.45462799072265625s, #samples=3133\n",
      "100%|██████████| 3133/3133 [00:04<00:00, 634.37it/s]\n",
      "\n",
      "fold 397 took 8.239396810531616s, #samples=3133\n",
      "store to h5 file took 7.335567235946655s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.0573768615722656s, #samples=4024\n",
      "100%|██████████| 4024/4024 [00:06<00:00, 628.20it/s]\n",
      "\n",
      "fold 398 took 9.86428165435791s, #samples=4024\n",
      "store to h5 file took 9.27396559715271s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.7992241382598877s, #samples=3700\n",
      "100%|██████████| 3700/3700 [00:05<00:00, 629.96it/s]\n",
      "\n",
      "fold 399 took 9.667372226715088s, #samples=3700\n",
      "store to h5 file took 8.031460285186768s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.741419792175293s, #samples=4257\n",
      "100%|██████████| 4257/4257 [00:07<00:00, 571.69it/s]\n",
      "\n",
      "fold 400 took 11.117856740951538s, #samples=4257\n",
      "store to h5 file took 10.250692129135132s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5509226322174072s, #samples=3276\n",
      "100%|██████████| 3276/3276 [00:05<00:00, 645.27it/s]\n",
      "\n",
      "fold 401 took 8.81619143486023s, #samples=3276\n",
      "store to h5 file took 7.941090106964111s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.5931012630462646s, #samples=3277\n",
      "100%|██████████| 3277/3277 [00:05<00:00, 575.96it/s]\n",
      "\n",
      "fold 402 took 9.39953351020813s, #samples=3277\n",
      "store to h5 file took 7.910680532455444s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 1.5289552211761475s, #samples=3064\n",
      "100%|██████████| 3064/3064 [00:04<00:00, 646.93it/s]\n",
      "\n",
      "fold 403 took 8.009268522262573s, #samples=3064\n",
      "store to h5 file took 6.782719850540161s\n",
      "clean tmp\n",
      "max len: 20 , #files: 74\n",
      "constrcuting file list took 0.6516835689544678s, #samples=3859\n",
      "100%|██████████| 3859/3859 [00:06<00:00, 598.97it/s]\n",
      "\n",
      "fold 404 took 9.877627611160278s, #samples=3859\n",
      "store to h5 file took 9.203708171844482s\n",
      "clean tmp\n",
      "max len: 20 , #files: 71\n",
      "constrcuting file list took 0.34104347229003906s, #samples=3462\n",
      "100%|██████████| 3462/3462 [00:05<00:00, 637.23it/s]\n",
      "\n",
      "fold 405 took 8.97915005683899s, #samples=3462\n",
      "store to h5 file took 7.795031785964966s\n",
      "clean tmp\n",
      "max len: 20 , #files: 69\n",
      "constrcuting file list took 0.47916364669799805s, #samples=2860\n",
      "100%|██████████| 2860/2860 [00:04<00:00, 626.10it/s]\n",
      "\n",
      "fold 406 took 7.896669149398804s, #samples=2860\n",
      "store to h5 file took 6.934028387069702s\n",
      "clean tmp\n",
      "max len: 20 , #files: 69\n",
      "constrcuting file list took 0.2952399253845215s, #samples=3610\n",
      "100%|██████████| 3610/3610 [00:05<00:00, 618.60it/s]\n",
      "\n",
      "fold 407 took 9.552458763122559s, #samples=3610\n",
      "store to h5 file took 8.128109216690063s\n",
      "clean tmp\n",
      "max len: 20 , #files: 69\n",
      "constrcuting file list took 0.4482920169830322s, #samples=3542\n",
      "100%|██████████| 3542/3542 [00:05<00:00, 642.67it/s]\n",
      "\n",
      "fold 408 took 8.946202039718628s, #samples=3542\n",
      "store to h5 file took 7.941234588623047s\n",
      "clean tmp\n",
      "max len: 20 , #files: 69\n",
      "constrcuting file list took 0.36338329315185547s, #samples=3222\n",
      "100%|██████████| 3222/3222 [00:05<00:00, 618.20it/s]\n",
      "\n",
      "fold 409 took 9.235989809036255s, #samples=3222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store to h5 file took 7.4814159870147705s\n",
      "clean tmp\n",
      "max len: 20 , #files: 69\n",
      "constrcuting file list took 0.4624485969543457s, #samples=3176\n",
      "100%|██████████| 3176/3176 [00:05<00:00, 571.12it/s]\n",
      "\n",
      "fold 410 took 8.941154479980469s, #samples=3176\n",
      "store to h5 file took 7.0432891845703125s\n",
      "clean tmp\n",
      "max len: 20 , #files: 69\n",
      "constrcuting file list took 0.13893604278564453s, #samples=4205\n",
      "100%|██████████| 4205/4205 [00:06<00:00, 615.77it/s]\n",
      "\n",
      "fold 411 took 10.069733142852783s, #samples=4205\n",
      "store to h5 file took 9.435304641723633s\n",
      "clean tmp\n",
      "max len: 20 , #files: 69\n",
      "constrcuting file list took 0.3472776412963867s, #samples=3407\n",
      "100%|██████████| 3407/3407 [00:05<00:00, 632.56it/s]\n",
      "\n",
      "fold 412 took 8.674015283584595s, #samples=3407\n",
      "store to h5 file took 8.019977331161499s\n",
      "clean tmp\n",
      "max len: 20 , #files: 65\n",
      "constrcuting file list took 1.0295624732971191s, #samples=2908\n",
      "100%|██████████| 2908/2908 [00:04<00:00, 628.64it/s]\n",
      "\n",
      "fold 413 took 8.3165283203125s, #samples=2908\n",
      "store to h5 file took 6.86670708656311s\n",
      "clean tmp\n",
      "max len: 20 , #files: 65\n",
      "constrcuting file list took 0.5889861583709717s, #samples=3194\n",
      "100%|██████████| 3194/3194 [00:04<00:00, 656.43it/s]\n",
      "\n",
      "fold 414 took 8.315444231033325s, #samples=3194\n",
      "store to h5 file took 7.604418516159058s\n",
      "clean tmp\n",
      "max len: 20 , #files: 65\n",
      "constrcuting file list took 0.4474668502807617s, #samples=2512\n",
      "100%|██████████| 2512/2512 [00:04<00:00, 603.46it/s]\n",
      "\n",
      "fold 415 took 7.917471170425415s, #samples=2512\n",
      "store to h5 file took 6.047412872314453s\n",
      "clean tmp\n",
      "max len: 20 , #files: 65\n",
      "constrcuting file list took 0.616002082824707s, #samples=3204\n",
      "100%|██████████| 3204/3204 [00:05<00:00, 599.60it/s]\n",
      "\n",
      "fold 416 took 8.712520360946655s, #samples=3204\n",
      "store to h5 file took 7.2302892208099365s\n",
      "clean tmp\n",
      "max len: 20 , #files: 65\n",
      "constrcuting file list took 0.26151394844055176s, #samples=2757\n",
      "100%|██████████| 2757/2757 [00:04<00:00, 644.14it/s]\n",
      "\n",
      "fold 417 took 7.582097053527832s, #samples=2757\n",
      "store to h5 file took 6.766394376754761s\n",
      "clean tmp\n",
      "max len: 20 , #files: 65\n",
      "constrcuting file list took 0.8036737442016602s, #samples=2356\n",
      "100%|██████████| 2356/2356 [00:03<00:00, 623.70it/s]\n",
      "\n",
      "fold 418 took 7.454291105270386s, #samples=2356\n",
      "store to h5 file took 5.684201717376709s\n",
      "clean tmp\n",
      "max len: 20 , #files: 65\n",
      "constrcuting file list took 1.3981406688690186s, #samples=2904\n",
      "100%|██████████| 2904/2904 [00:04<00:00, 662.71it/s]\n",
      "\n",
      "fold 419 took 7.832254409790039s, #samples=2904\n",
      "store to h5 file took 6.585214614868164s\n",
      "clean tmp\n",
      "max len: 20 , #files: 65\n",
      "constrcuting file list took 0.286022424697876s, #samples=3256\n",
      "100%|██████████| 3256/3256 [00:05<00:00, 635.99it/s]\n",
      "\n",
      "fold 420 took 8.723767280578613s, #samples=3256\n",
      "store to h5 file took 7.4203245639801025s\n",
      "clean tmp\n",
      "max len: 20 , #files: 65\n",
      "constrcuting file list took 0.7945201396942139s, #samples=3978\n",
      "100%|██████████| 3978/3978 [00:06<00:00, 641.44it/s]\n",
      "\n",
      "fold 421 took 9.92906665802002s, #samples=3978\n",
      "store to h5 file took 9.579402446746826s\n",
      "clean tmp\n",
      "max len: 20 , #files: 65\n",
      "constrcuting file list took 0.7100205421447754s, #samples=2490\n",
      "100%|██████████| 2490/2490 [00:04<00:00, 578.21it/s]\n",
      "\n",
      "fold 422 took 7.677310466766357s, #samples=2490\n",
      "store to h5 file took 5.653845548629761s\n",
      "clean tmp\n",
      "max len: 20 , #files: 65\n",
      "constrcuting file list took 0.5667376518249512s, #samples=2329\n",
      "100%|██████████| 2329/2329 [00:03<00:00, 592.72it/s]\n",
      "\n",
      "fold 423 took 7.225488901138306s, #samples=2329\n",
      "store to h5 file took 4.950406074523926s\n",
      "clean tmp\n",
      "max len: 20 , #files: 59\n",
      "constrcuting file list took 0.4254922866821289s, #samples=2317\n",
      "100%|██████████| 2317/2317 [00:03<00:00, 642.79it/s]\n",
      "\n",
      "fold 424 took 7.226290702819824s, #samples=2317\n",
      "store to h5 file took 5.450232267379761s\n",
      "clean tmp\n",
      "max len: 20 , #files: 59\n",
      "constrcuting file list took 0.42969751358032227s, #samples=2669\n",
      "100%|██████████| 2669/2669 [00:04<00:00, 643.89it/s]\n",
      "\n",
      "fold 425 took 7.604792833328247s, #samples=2669\n",
      "store to h5 file took 5.920315980911255s\n",
      "clean tmp\n",
      "max len: 20 , #files: 59\n",
      "constrcuting file list took 0.826948881149292s, #samples=2358\n",
      "100%|██████████| 2358/2358 [00:03<00:00, 612.17it/s]\n",
      "\n",
      "fold 426 took 7.363135576248169s, #samples=2358\n",
      "store to h5 file took 5.701717138290405s\n",
      "clean tmp\n",
      "max len: 20 , #files: 56\n",
      "constrcuting file list took 0.5733921527862549s, #samples=2437\n",
      "100%|██████████| 2437/2437 [00:03<00:00, 630.78it/s]\n",
      "\n",
      "fold 427 took 6.959763765335083s, #samples=2437\n",
      "store to h5 file took 5.552750825881958s\n",
      "clean tmp\n",
      "max len: 20 , #files: 54\n",
      "constrcuting file list took 0.27764201164245605s, #samples=2215\n",
      "100%|██████████| 2215/2215 [00:03<00:00, 623.71it/s]\n",
      "\n",
      "fold 428 took 7.0867063999176025s, #samples=2215\n",
      "store to h5 file took 5.225845813751221s\n",
      "clean tmp\n",
      "max len: 20 , #files: 51\n",
      "constrcuting file list took 0.2022993564605713s, #samples=2486\n",
      "100%|██████████| 2486/2486 [00:04<00:00, 614.29it/s]\n",
      "\n",
      "fold 429 took 7.457269191741943s, #samples=2486\n",
      "store to h5 file took 5.118508815765381s\n",
      "clean tmp\n",
      "max len: 20 , #files: 48\n",
      "constrcuting file list took 0.11426424980163574s, #samples=2241\n",
      "100%|██████████| 2241/2241 [00:03<00:00, 647.03it/s]\n",
      "\n",
      "fold 430 took 6.707115888595581s, #samples=2241\n",
      "store to h5 file took 5.452733516693115s\n",
      "clean tmp\n",
      "max len: 20 , #files: 48\n",
      "constrcuting file list took 0.28453660011291504s, #samples=1693\n",
      "100%|██████████| 1693/1693 [00:02<00:00, 592.01it/s]\n",
      "\n",
      "fold 431 took 6.498100280761719s, #samples=1693\n",
      "store to h5 file took 3.7440097332000732s\n",
      "clean tmp\n",
      "max len: 20 , #files: 48\n",
      "constrcuting file list took 0.5606045722961426s, #samples=1595\n",
      "100%|██████████| 1595/1595 [00:02<00:00, 579.84it/s]\n",
      "\n",
      "fold 432 took 6.022989749908447s, #samples=1595\n",
      "store to h5 file took 3.742009162902832s\n",
      "clean tmp\n",
      "max len: 20 , #files: 48\n",
      "constrcuting file list took 0.506458044052124s, #samples=2027\n",
      "100%|██████████| 2027/2027 [00:03<00:00, 644.54it/s]\n",
      "\n",
      "fold 433 took 6.763725757598877s, #samples=2027\n",
      "store to h5 file took 4.414978265762329s\n",
      "clean tmp\n",
      "max len: 20 , #files: 48\n",
      "constrcuting file list took 0.27660274505615234s, #samples=1826\n",
      "100%|██████████| 1826/1826 [00:03<00:00, 601.38it/s]\n",
      "\n",
      "fold 434 took 6.830398797988892s, #samples=1826\n",
      "store to h5 file took 4.733792066574097s\n",
      "clean tmp\n",
      "max len: 20 , #files: 48\n",
      "constrcuting file list took 0.8514106273651123s, #samples=2007\n",
      "100%|██████████| 2007/2007 [00:03<00:00, 552.20it/s]\n",
      "\n",
      "fold 435 took 6.856881856918335s, #samples=2007\n",
      "store to h5 file took 4.239915370941162s\n",
      "clean tmp\n",
      "max len: 20 , #files: 48\n",
      "constrcuting file list took 0.6022815704345703s, #samples=1719\n",
      "100%|██████████| 1719/1719 [00:02<00:00, 653.44it/s]\n",
      "\n",
      "fold 436 took 6.318849325180054s, #samples=1719\n",
      "store to h5 file took 3.762680768966675s\n",
      "clean tmp\n",
      "max len: 20 , #files: 48\n",
      "constrcuting file list took 0.2469191551208496s, #samples=2064\n",
      "100%|██████████| 2064/2064 [00:03<00:00, 613.96it/s]\n",
      "\n",
      "fold 437 took 6.922911643981934s, #samples=2064\n",
      "store to h5 file took 4.710706472396851s\n",
      "clean tmp\n",
      "max len: 20 , #files: 47\n",
      "constrcuting file list took 0.2154242992401123s, #samples=2072\n",
      "100%|██████████| 2072/2072 [00:03<00:00, 621.91it/s]\n",
      "\n",
      "fold 438 took 6.594833135604858s, #samples=2072\n",
      "store to h5 file took 4.9157116413116455s\n",
      "clean tmp\n",
      "max len: 20 , #files: 41\n",
      "constrcuting file list took 0.23939180374145508s, #samples=2504\n",
      "100%|██████████| 2504/2504 [00:04<00:00, 618.84it/s]\n",
      "\n",
      "fold 439 took 7.1988136768341064s, #samples=2504\n",
      "store to h5 file took 5.351956367492676s\n",
      "clean tmp\n",
      "max len: 20 , #files: 41\n",
      "constrcuting file list took 0.2877192497253418s, #samples=1918\n",
      "100%|██████████| 1918/1918 [00:03<00:00, 618.79it/s]\n",
      "\n",
      "fold 440 took 6.749822616577148s, #samples=1918\n",
      "store to h5 file took 4.176311016082764s\n",
      "clean tmp\n",
      "max len: 20 , #files: 41\n",
      "constrcuting file list took 0.2551145553588867s, #samples=2092\n",
      "100%|██████████| 2092/2092 [00:03<00:00, 569.96it/s]\n",
      "\n",
      "fold 441 took 7.408209562301636s, #samples=2092\n",
      "store to h5 file took 5.110525369644165s\n",
      "clean tmp\n",
      "max len: 20 , #files: 41\n",
      "constrcuting file list took 0.7670843601226807s, #samples=1784\n",
      "100%|██████████| 1784/1784 [00:02<00:00, 609.76it/s]\n",
      "\n",
      "fold 442 took 6.387873888015747s, #samples=1784\n",
      "store to h5 file took 4.016944646835327s\n",
      "clean tmp\n",
      "max len: 20 , #files: 41\n",
      "constrcuting file list took 1.1219115257263184s, #samples=1835\n",
      "100%|██████████| 1835/1835 [00:02<00:00, 659.53it/s]\n",
      "\n",
      "fold 443 took 6.437166690826416s, #samples=1835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store to h5 file took 4.369058132171631s\n",
      "clean tmp\n",
      "max len: 20 , #files: 40\n",
      "constrcuting file list took 0.1027839183807373s, #samples=1702\n",
      "100%|██████████| 1702/1702 [00:02<00:00, 604.97it/s]\n",
      "\n",
      "fold 444 took 6.427604675292969s, #samples=1702\n",
      "store to h5 file took 4.197311639785767s\n",
      "clean tmp\n",
      "max len: 20 , #files: 36\n",
      "constrcuting file list took 0.04167509078979492s, #samples=1286\n",
      "100%|██████████| 1286/1286 [00:02<00:00, 623.97it/s]\n",
      "\n",
      "fold 445 took 5.42546820640564s, #samples=1286\n",
      "store to h5 file took 2.494291305541992s\n",
      "clean tmp\n",
      "max len: 20 , #files: 36\n",
      "constrcuting file list took 0.06525254249572754s, #samples=1296\n",
      "100%|██████████| 1296/1296 [00:02<00:00, 631.11it/s]\n",
      "\n",
      "fold 446 took 5.693660259246826s, #samples=1296\n",
      "store to h5 file took 2.834798812866211s\n",
      "clean tmp\n",
      "max len: 20 , #files: 36\n",
      "constrcuting file list took 0.09298086166381836s, #samples=1784\n",
      "100%|██████████| 1784/1784 [00:02<00:00, 648.79it/s]\n",
      "\n",
      "fold 447 took 6.205524682998657s, #samples=1784\n",
      "store to h5 file took 4.065388202667236s\n",
      "clean tmp\n",
      "max len: 20 , #files: 32\n",
      "constrcuting file list took 0.03585982322692871s, #samples=1552\n",
      "100%|██████████| 1552/1552 [00:02<00:00, 571.32it/s]\n",
      "\n",
      "fold 448 took 6.111872434616089s, #samples=1552\n",
      "store to h5 file took 3.5628316402435303s\n",
      "clean tmp\n",
      "max len: 20 , #files: 31\n",
      "constrcuting file list took 0.4176003932952881s, #samples=1237\n",
      "100%|██████████| 1237/1237 [00:02<00:00, 601.30it/s]\n",
      "\n",
      "fold 449 took 5.713648080825806s, #samples=1237\n",
      "store to h5 file took 3.006060838699341s\n",
      "clean tmp\n",
      "max len: 20 , #files: 31\n",
      "constrcuting file list took 0.17485857009887695s, #samples=1572\n",
      "100%|██████████| 1572/1572 [00:02<00:00, 584.84it/s]\n",
      "\n",
      "fold 450 took 6.474004507064819s, #samples=1572\n",
      "store to h5 file took 3.506279230117798s\n",
      "clean tmp\n",
      "max len: 20 , #files: 31\n",
      "constrcuting file list took 0.17217564582824707s, #samples=1339\n",
      "100%|██████████| 1339/1339 [00:02<00:00, 552.86it/s]\n",
      "\n",
      "fold 451 took 5.719273328781128s, #samples=1339\n",
      "store to h5 file took 3.0604898929595947s\n",
      "clean tmp\n",
      "max len: 20 , #files: 31\n",
      "constrcuting file list took 0.062323570251464844s, #samples=1490\n",
      "100%|██████████| 1490/1490 [00:02<00:00, 565.51it/s]\n",
      "\n",
      "fold 452 took 6.1999993324279785s, #samples=1490\n",
      "store to h5 file took 3.8049991130828857s\n",
      "clean tmp\n",
      "max len: 20 , #files: 31\n",
      "constrcuting file list took 0.10453152656555176s, #samples=1211\n",
      "100%|██████████| 1211/1211 [00:02<00:00, 501.81it/s]\n",
      "\n",
      "fold 453 took 5.972215890884399s, #samples=1211\n",
      "store to h5 file took 2.7357122898101807s\n",
      "clean tmp\n",
      "max len: 20 , #files: 31\n",
      "constrcuting file list took 0.06272411346435547s, #samples=1282\n",
      "100%|██████████| 1282/1282 [00:02<00:00, 586.03it/s]\n",
      "\n",
      "fold 454 took 5.834275722503662s, #samples=1282\n",
      "store to h5 file took 2.8232829570770264s\n",
      "clean tmp\n",
      "max len: 20 , #files: 31\n",
      "constrcuting file list took 0.16956472396850586s, #samples=1103\n",
      "100%|██████████| 1103/1103 [00:01<00:00, 635.03it/s]\n",
      "\n",
      "fold 455 took 5.437877178192139s, #samples=1103\n",
      "store to h5 file took 2.7419703006744385s\n",
      "clean tmp\n",
      "max len: 20 , #files: 31\n",
      "constrcuting file list took 0.048308610916137695s, #samples=1083\n",
      "100%|██████████| 1083/1083 [00:01<00:00, 590.69it/s]\n",
      "\n",
      "fold 456 took 5.333494663238525s, #samples=1083\n",
      "store to h5 file took 2.669833183288574s\n",
      "clean tmp\n",
      "max len: 20 , #files: 30\n",
      "constrcuting file list took 0.06171083450317383s, #samples=1945\n",
      "100%|██████████| 1945/1945 [00:03<00:00, 615.36it/s]\n",
      "\n",
      "fold 457 took 6.8742406368255615s, #samples=1945\n",
      "store to h5 file took 4.648805379867554s\n",
      "clean tmp\n",
      "max len: 20 , #files: 27\n",
      "constrcuting file list took 0.37857794761657715s, #samples=1098\n",
      "100%|██████████| 1098/1098 [00:01<00:00, 597.01it/s]\n",
      "\n",
      "fold 458 took 5.779815912246704s, #samples=1098\n",
      "store to h5 file took 2.8867669105529785s\n",
      "clean tmp\n",
      "max len: 20 , #files: 27\n",
      "constrcuting file list took 0.11973834037780762s, #samples=1142\n",
      "100%|██████████| 1142/1142 [00:01<00:00, 666.92it/s]\n",
      "\n",
      "fold 459 took 5.083438158035278s, #samples=1142\n",
      "store to h5 file took 2.7710561752319336s\n",
      "clean tmp\n",
      "max len: 20 , #files: 23\n",
      "constrcuting file list took 0.025901079177856445s, #samples=1478\n",
      "100%|██████████| 1478/1478 [00:02<00:00, 618.02it/s]\n",
      "\n",
      "fold 460 took 5.864819288253784s, #samples=1478\n",
      "store to h5 file took 3.6423027515411377s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.053619384765625s, #samples=1302\n",
      "100%|██████████| 1302/1302 [00:02<00:00, 612.48it/s]\n",
      "\n",
      "fold 461 took 5.449491024017334s, #samples=1302\n",
      "store to h5 file took 3.212224245071411s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.09785604476928711s, #samples=1437\n",
      "100%|██████████| 1437/1437 [00:02<00:00, 586.67it/s]\n",
      "\n",
      "fold 462 took 5.686229944229126s, #samples=1437\n",
      "store to h5 file took 3.2742011547088623s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.04965829849243164s, #samples=1023\n",
      "100%|██████████| 1023/1023 [00:01<00:00, 598.75it/s]\n",
      "\n",
      "fold 463 took 5.156755208969116s, #samples=1023\n",
      "store to h5 file took 2.3060810565948486s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.16368508338928223s, #samples=1756\n",
      "100%|██████████| 1756/1756 [00:02<00:00, 641.22it/s]\n",
      "\n",
      "fold 464 took 6.222687482833862s, #samples=1756\n",
      "store to h5 file took 4.252426385879517s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.08191204071044922s, #samples=1521\n",
      "100%|██████████| 1521/1521 [00:02<00:00, 610.65it/s]\n",
      "\n",
      "fold 465 took 6.293149709701538s, #samples=1521\n",
      "store to h5 file took 3.5428473949432373s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.06522607803344727s, #samples=1574\n",
      "100%|██████████| 1574/1574 [00:02<00:00, 620.53it/s]\n",
      "\n",
      "fold 466 took 6.268728971481323s, #samples=1574\n",
      "store to h5 file took 3.4231016635894775s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.03391718864440918s, #samples=1337\n",
      "100%|██████████| 1337/1337 [00:02<00:00, 624.75it/s]\n",
      "\n",
      "fold 467 took 5.870929002761841s, #samples=1337\n",
      "store to h5 file took 3.234494686126709s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.02187967300415039s, #samples=1024\n",
      "100%|██████████| 1024/1024 [00:01<00:00, 603.26it/s]\n",
      "\n",
      "fold 468 took 4.956491947174072s, #samples=1024\n",
      "store to h5 file took 2.2843406200408936s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.14075994491577148s, #samples=1995\n",
      "100%|██████████| 1995/1995 [00:03<00:00, 616.43it/s]\n",
      "\n",
      "fold 469 took 6.540545701980591s, #samples=1995\n",
      "store to h5 file took 4.847132682800293s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.09327316284179688s, #samples=886\n",
      "100%|██████████| 886/886 [00:01<00:00, 670.08it/s]\n",
      "\n",
      "fold 470 took 4.71555495262146s, #samples=886\n",
      "store to h5 file took 2.3327276706695557s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.04578423500061035s, #samples=1156\n",
      "100%|██████████| 1156/1156 [00:02<00:00, 538.24it/s]\n",
      "\n",
      "fold 471 took 5.874387979507446s, #samples=1156\n",
      "store to h5 file took 2.7664380073547363s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.04236030578613281s, #samples=1034\n",
      "100%|██████████| 1034/1034 [00:01<00:00, 583.17it/s]\n",
      "\n",
      "fold 472 took 5.1883580684661865s, #samples=1034\n",
      "store to h5 file took 2.3010876178741455s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.06566905975341797s, #samples=1362\n",
      "100%|██████████| 1362/1362 [00:02<00:00, 673.82it/s]\n",
      "\n",
      "fold 473 took 5.713422775268555s, #samples=1362\n",
      "store to h5 file took 3.3025667667388916s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.10632085800170898s, #samples=1094\n",
      "100%|██████████| 1094/1094 [00:01<00:00, 646.03it/s]\n",
      "\n",
      "fold 474 took 5.061241388320923s, #samples=1094\n",
      "store to h5 file took 2.6646480560302734s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.03751182556152344s, #samples=1198\n",
      "100%|██████████| 1198/1198 [00:02<00:00, 532.24it/s]\n",
      "\n",
      "fold 475 took 5.642458438873291s, #samples=1198\n",
      "store to h5 file took 3.353553533554077s\n",
      "clean tmp\n",
      "max len: 20 , #files: 20\n",
      "constrcuting file list took 0.02368021011352539s, #samples=1246\n",
      "100%|██████████| 1246/1246 [00:02<00:00, 621.14it/s]\n",
      "\n",
      "fold 476 took 5.160707950592041s, #samples=1246\n",
      "store to h5 file took 2.810007333755493s\n",
      "clean tmp\n",
      "max len: 20 , #files: 18\n",
      "constrcuting file list took 0.04278063774108887s, #samples=1479\n",
      "100%|██████████| 1479/1479 [00:02<00:00, 560.38it/s]\n",
      "\n",
      "fold 477 took 6.160268545150757s, #samples=1479\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store to h5 file took 3.45965838432312s\n",
      "clean tmp\n",
      "max len: 20 , #files: 16\n",
      "constrcuting file list took 0.05682373046875s, #samples=1248\n",
      "100%|██████████| 1248/1248 [00:02<00:00, 570.62it/s]\n",
      "\n",
      "fold 478 took 5.444918155670166s, #samples=1248\n",
      "store to h5 file took 2.953315019607544s\n",
      "clean tmp\n",
      "max len: 20 , #files: 16\n",
      "constrcuting file list took 0.0593569278717041s, #samples=1340\n",
      "100%|██████████| 1340/1340 [00:02<00:00, 655.73it/s]\n",
      "\n",
      "fold 479 took 5.998789072036743s, #samples=1340\n",
      "store to h5 file took 2.9744389057159424s\n",
      "clean tmp\n",
      "max len: 20 , #files: 16\n",
      "constrcuting file list took 0.012856006622314453s, #samples=636\n",
      "100%|██████████| 636/636 [00:01<00:00, 600.38it/s]\n",
      "\n",
      "fold 480 took 4.278408765792847s, #samples=636\n",
      "store to h5 file took 1.512770175933838s\n",
      "clean tmp\n",
      "max len: 20 , #files: 16\n",
      "constrcuting file list took 0.039916038513183594s, #samples=935\n",
      "100%|██████████| 935/935 [00:01<00:00, 683.70it/s]\n",
      "\n",
      "fold 481 took 4.894913196563721s, #samples=935\n",
      "store to h5 file took 2.3180313110351562s\n",
      "clean tmp\n",
      "max len: 20 , #files: 16\n",
      "constrcuting file list took 0.12560129165649414s, #samples=927\n",
      "100%|██████████| 927/927 [00:01<00:00, 581.39it/s]\n",
      "\n",
      "fold 482 took 5.074282646179199s, #samples=927\n",
      "store to h5 file took 1.9950449466705322s\n",
      "clean tmp\n",
      "max len: 20 , #files: 16\n",
      "constrcuting file list took 0.03626251220703125s, #samples=1147\n",
      "100%|██████████| 1147/1147 [00:01<00:00, 618.18it/s]\n",
      "\n",
      "fold 483 took 5.270049810409546s, #samples=1147\n",
      "store to h5 file took 2.389554262161255s\n",
      "clean tmp\n",
      "max len: 20 , #files: 16\n",
      "constrcuting file list took 0.03224921226501465s, #samples=1046\n",
      "100%|██████████| 1046/1046 [00:01<00:00, 618.33it/s]\n",
      "\n",
      "fold 484 took 5.164508104324341s, #samples=1046\n",
      "store to h5 file took 2.5011112689971924s\n",
      "clean tmp\n",
      "max len: 20 , #files: 13\n",
      "constrcuting file list took 0.06350255012512207s, #samples=737\n",
      "100%|██████████| 737/737 [00:01<00:00, 567.95it/s]\n",
      "\n",
      "fold 485 took 4.818758487701416s, #samples=737\n",
      "store to h5 file took 1.7961175441741943s\n",
      "clean tmp\n",
      "max len: 20 , #files: 11\n",
      "constrcuting file list took 0.00869297981262207s, #samples=431\n",
      "100%|██████████| 431/431 [00:00<00:00, 563.90it/s]\n",
      "\n",
      "fold 486 took 4.0463385581970215s, #samples=431\n",
      "store to h5 file took 0.8787186145782471s\n",
      "clean tmp\n",
      "max len: 20 , #files: 11\n",
      "constrcuting file list took 0.01472616195678711s, #samples=695\n",
      "100%|██████████| 695/695 [00:01<00:00, 660.83it/s]\n",
      "\n",
      "fold 487 took 4.715057611465454s, #samples=695\n",
      "store to h5 file took 1.6527893543243408s\n",
      "clean tmp\n",
      "max len: 20 , #files: 11\n",
      "constrcuting file list took 0.032822608947753906s, #samples=474\n",
      "100%|██████████| 474/474 [00:00<00:00, 532.41it/s]\n",
      "\n",
      "fold 488 took 4.4776411056518555s, #samples=474\n",
      "store to h5 file took 1.175532579421997s\n",
      "clean tmp\n",
      "max len: 20 , #files: 11\n",
      "constrcuting file list took 0.021984100341796875s, #samples=913\n",
      "100%|██████████| 913/913 [00:01<00:00, 577.43it/s]\n",
      "\n",
      "fold 489 took 5.285320043563843s, #samples=913\n",
      "store to h5 file took 2.1333985328674316s\n",
      "clean tmp\n",
      "max len: 20 , #files: 11\n",
      "constrcuting file list took 0.13489961624145508s, #samples=716\n",
      "100%|██████████| 716/716 [00:01<00:00, 581.10it/s]\n",
      "\n",
      "fold 490 took 5.141896963119507s, #samples=716\n",
      "store to h5 file took 1.555999517440796s\n",
      "clean tmp\n",
      "max len: 20 , #files: 11\n",
      "constrcuting file list took 0.06995177268981934s, #samples=602\n",
      "100%|██████████| 602/602 [00:01<00:00, 565.15it/s]\n",
      "\n",
      "fold 491 took 4.462295770645142s, #samples=602\n",
      "store to h5 file took 1.2498235702514648s\n",
      "clean tmp\n",
      "max len: 20 , #files: 11\n",
      "constrcuting file list took 0.027524948120117188s, #samples=1101\n",
      "100%|██████████| 1101/1101 [00:01<00:00, 597.87it/s]\n",
      "\n",
      "fold 492 took 5.131321907043457s, #samples=1101\n",
      "store to h5 file took 2.259042501449585s\n",
      "clean tmp\n",
      "max len: 20 , #files: 11\n",
      "constrcuting file list took 0.029542922973632812s, #samples=1233\n",
      "100%|██████████| 1233/1233 [00:02<00:00, 606.51it/s]\n",
      "\n",
      "fold 493 took 5.2170281410217285s, #samples=1233\n",
      "store to h5 file took 3.0411412715911865s\n",
      "clean tmp\n",
      "max len: 20 , #files: 11\n",
      "constrcuting file list took 0.041298627853393555s, #samples=1135\n",
      "100%|██████████| 1135/1135 [00:01<00:00, 575.30it/s]\n",
      "\n",
      "fold 494 took 5.410428762435913s, #samples=1135\n",
      "store to h5 file took 2.5546481609344482s\n",
      "clean tmp\n",
      "max len: 20 , #files: 11\n",
      "constrcuting file list took 0.09680628776550293s, #samples=900\n",
      "100%|██████████| 900/900 [00:01<00:00, 618.02it/s]\n",
      "\n",
      "fold 495 took 5.145940065383911s, #samples=900\n",
      "store to h5 file took 2.046893835067749s\n",
      "clean tmp\n",
      "max len: 20 , #files: 10\n",
      "constrcuting file list took 0.03209114074707031s, #samples=788\n",
      "100%|██████████| 788/788 [00:01<00:00, 556.21it/s]\n",
      "\n",
      "fold 496 took 4.652138948440552s, #samples=788\n",
      "store to h5 file took 1.891782283782959s\n",
      "clean tmp\n",
      "max len: 20 , #files: 5\n",
      "constrcuting file list took 0.017862558364868164s, #samples=350\n",
      "100%|██████████| 350/350 [00:00<00:00, 611.33it/s]\n",
      "\n",
      "fold 497 took 4.296206474304199s, #samples=350\n",
      "store to h5 file took 0.7070863246917725s\n",
      "clean tmp\n",
      "max len: 20 , #files: 0\n",
      "constrcuting file list took 0.0003800392150878906s, #samples=0\n",
      "Traceback (most recent call last):\n",
      "\n",
      "  File \"/data/xiaowentao/.local/bin/trdg\", line 8, in <module>\n",
      "\n",
      "    sys.exit(main())\n",
      "\n",
      "  File \"/data/xiaowentao/.local/lib/python3.7/site-packages/trdg/run.py\", line 353, in main\n",
      "\n",
      "    strings = create_strings_from_file(args.input_file, args.count, args.length)\n",
      "\n",
      "  File \"/data/xiaowentao/.local/lib/python3.7/site-packages/trdg/string_generator.py\", line 32, in create_strings_from_file\n",
      "\n",
      "    raise Exception(\"No lines could be read in file\")\n",
      "\n",
      "Exception: No lines could be read in file\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_generated/tmp/labels.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-218a2c1c72b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'labels.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_generated/tmp/labels.txt'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from time import time \n",
    "import os\n",
    "from math import ceil\n",
    "from random import shuffle\n",
    "from textwrap3 import wrap\n",
    "import h5py\n",
    "from PIL import Image, ImageOps\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "\n",
    "def split_list(l, n):\n",
    "    len_ = ceil(len(l) / n)\n",
    "    return [l[i*len_:(i+1)*len_] for i in range(n)]\n",
    "\n",
    "command_temp = '''\n",
    "trdg --output_dir {} --count {} \\\n",
    "    --language cn --length {} \\\n",
    "    --format 40 --thread_count 20  \\\n",
    "    --background 0 --distorsion 3 --space_width 0 \\\n",
    "    --text_color '#000000,#888888' --name_format 2 --character_spacing 6 \\\n",
    "    -fd ./data_generated/font_files \\\n",
    "    -ft `ls ./data_generated/font_files | sort -R | head -n 1` \\\n",
    "    --input_file {} --fit'''\n",
    "base = 'THUCNews'\n",
    "target = 'data_generated'\n",
    "fold = 500\n",
    "all_files = {\n",
    "    c: split_list(\n",
    "        [(base + '/' + c + '/' + name) for name in os.listdir(\n",
    "            base + '/' + c\n",
    "        )[:file_need[c]]], fold\n",
    "    ) for c in os.listdir(base)\n",
    "}\n",
    "assert all([len(v) == fold for v in all_files.values()])\n",
    "total_samples = 0\n",
    "dict_dataset = ''.join([i.strip() for i in open(\n",
    "    'data_generated/label_cn.txt').readlines()[:-1]] + [' ',])\n",
    "dict_dataset_fonts = {}\n",
    "target_w = 560\n",
    "target_h = 32\n",
    "# DCMMC: 固定长度应该更加适合训练一点\n",
    "line_length = 20\n",
    "# !rm -v data_generated/dataset.h5\n",
    "\n",
    "def split_long_line(line, line_length):\n",
    "    # deprecated\n",
    "#     return [(''.join(l.strip().split()) + '\\n') for l in filter_short(wrap(\n",
    "#                     line.strip(), width=max_len,\n",
    "#                     replace_whitespace=False, break_long_words=True)),]\n",
    "    end_tokens = ',，。！？?…-'\n",
    "    # 去掉所有空白字符，降低难度并消除非打印字符\n",
    "    line = ''.join(line.strip().split())\n",
    "    if len(line) < line_length:\n",
    "        return []\n",
    "    lines = []\n",
    "    offset = 0\n",
    "    while (offset + line_length) < len(line):\n",
    "        tmp = line[offset:(offset+line_length)].strip()\n",
    "        unk = False\n",
    "        cnt_unk = 0\n",
    "        for c in tmp:\n",
    "            if c not in dict_dataset:\n",
    "                unk = True\n",
    "            if '?' == c:\n",
    "                cnt_unk += 1\n",
    "        if not unk and cnt_unk < 3:\n",
    "            lines.append(tmp)\n",
    "        for i,c in enumerate(line[offset+1:]):\n",
    "            if c in end_tokens:\n",
    "                offset += (i + 2)\n",
    "                break\n",
    "            if offset + i + line_length >= len(line):\n",
    "                offset = len(line)\n",
    "    assert all([(line_length - 2) <= len(l) <= line_length for l in lines])\n",
    "    for c in ''.join(lines):\n",
    "        if c not in dict_dataset_fonts:\n",
    "            dict_dataset_fonts[c] = 1\n",
    "        else:\n",
    "            dict_dataset_fonts[c] += 1\n",
    "    lines = [l + '\\n' for l in lines]\n",
    "    return lines\n",
    "        \n",
    "for idx in range(fold):\n",
    "    print('clean tmp')\n",
    "#     fp = 'data_generated/fold_{}/'.format(idx)\n",
    "    fp = 'data_generated/tmp/'\n",
    "    try:\n",
    "        os.makedirs(fp)\n",
    "    except FileExistsError:\n",
    "        # directory already exists\n",
    "        pass\n",
    "    for f in os.listdir(fp):\n",
    "        os.remove(fp + f)\n",
    "    files = []\n",
    "    for v in all_files.values():\n",
    "        files += v[idx]\n",
    "    shuffle(files)\n",
    "    print('max len:', line_length, ', #files:', len(files))\n",
    "    strs = []\n",
    "    s_t = time()\n",
    "    filter_short = lambda l: [i for i in l if len(i) >= 4]\n",
    "    for f in files:\n",
    "        with open(f) as txt:\n",
    "            for line in txt.readlines():\n",
    "                strs += split_long_line(line, line_length)\n",
    "    shuffle(strs)\n",
    "    output_file = 'data_generated/fold_{}.txt'.format(idx)\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.writelines(strs)\n",
    "    print('constrcuting file list took {}s, #samples={}'.format(\n",
    "        time() - s_t, len(strs)))\n",
    "    s_t = time()\n",
    "    command = command_temp.format(fp, len(strs), 20, output_file)\n",
    "    p = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE,\n",
    "                         stderr=subprocess.STDOUT)\n",
    "    for line in p.stdout.readlines():\n",
    "        print(str(line, encoding='utf-8'))\n",
    "    retval = p.wait()\n",
    "    with open(fp + 'labels.txt') as f:\n",
    "        labels = f.readlines()\n",
    "        shuffle(labels)\n",
    "    print('fold {} took {}s, #samples={}'.format(idx, time() - s_t, len(labels)))\n",
    "    s_t = time()\n",
    "    with h5py.File('data_generated/dataset_fonts.h5', 'a') as dataset:\n",
    "        for i, sample in enumerate(labels):\n",
    "            name = sample.split(' ')[0]\n",
    "            label = sample[len(name) + 1:].strip()\n",
    "            # 灰度图\n",
    "            img = Image.open(\n",
    "                'data_generated/tmp/' + name).convert('L')\n",
    "            # resize and pad\n",
    "            ratio = target_h / img.size[1]\n",
    "            new_sz = [int(s * ratio) for s in img.size]\n",
    "            img = img.resize(new_sz, Image.ANTIALIAS)\n",
    "            delta_w = target_w - img.size[0]\n",
    "            img = ImageOps.expand(img, (delta_w // 2, 0, delta_w - delta_w // 2, 0),\n",
    "                                  fill=255)\n",
    "            img = np.array(img)\n",
    "            dataset[str(i + total_samples) + '/img'] = img\n",
    "            dataset[str(i + total_samples) + '/y'] = label\n",
    "    total_samples += len(labels)\n",
    "    print('store to h5 file took {}s'.format(time() - s_t))\n",
    "print('#dict_dataset_RGB:', len(dict_dataset_RGB))\n",
    "with open('data_generated/dict_dataset_fonts.json', 'w') as f:\n",
    "    json.dump(list(dict_dataset_RGB), f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n",
      "['新浪娱乐讯9月26日，张杰(微博)在自己', '张杰(微博)在自己与谢娜(微博)的婚礼上', '首度演唱了新专辑第二主打歌《第一夫人》。', '《第一夫人》由王力宏(微博)作曲、姚若龙', '据悉，这首歌正是王力宏获悉张杰即将迈入人', '这首歌正是王力宏获悉张杰即将迈入人生全新']\n",
      "[20, 20, 20, 20, 20, 20]\n"
     ]
    }
   ],
   "source": [
    "dict_dataset = ''.join([i.strip() for i in open(\n",
    "    'data_generated/label_cn.txt').readlines()[:-1]] + [' ',])\n",
    "\n",
    "def foo(line, line_length=20):\n",
    "    end_tokens = ',，。！？?…-'\n",
    "    # 去掉所有空白字符，降低难度并消除非打印字符\n",
    "    line = ''.join(line.strip().split())\n",
    "    if len(line) < line_length:\n",
    "        return []\n",
    "    lines = []\n",
    "    offset = 0\n",
    "    while (offset + line_length) < len(line):\n",
    "        tmp = line[offset:(offset+line_length)].strip()\n",
    "        unk = False\n",
    "        for c in tmp:\n",
    "            if c not in dict_dataset:\n",
    "                unk = True\n",
    "        if not unk:\n",
    "            lines.append(tmp)\n",
    "        for i,c in enumerate(line[offset+1:]):\n",
    "            if c in end_tokens:\n",
    "                offset += (i + 2)\n",
    "                break\n",
    "            if offset + i + line_length >= len(line):\n",
    "                offset = len(line)\n",
    "    assert all([(line_length - 2) <= len(l) <= line_length  for l in lines])\n",
    "    return lines\n",
    "\n",
    "s = '　　新浪娱乐讯 9月26日，张杰(微博)在自己  与谢娜(微博)的婚礼上，首度演唱了新专辑第二主打歌《第一夫人》。《第一夫人》由王力宏(微博)作曲、姚若龙作词、陈子鸿制作。据悉，这首歌正是王力宏获悉张杰即将迈入人生全新阶段后灵感涌现创作而成的。'\n",
    "print(len(s))\n",
    "res = foo(s)\n",
    "print(res)\n",
    "print([len(r) for r in res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 1637012, #dict: 5430\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import json\n",
    "\n",
    "dict_dataset = {}\n",
    "cnt = 0\n",
    "with h5py.File('data_generated/dataset_fonts.h5', 'r') as d:\n",
    "    for idx in d:\n",
    "        cnt += 1\n",
    "        for c in str(d[idx]['y'][...]):\n",
    "            if c not in dict_dataset:\n",
    "                dict_dataset[c] = 1\n",
    "            else:\n",
    "                dict_dataset[c] += 1\n",
    "print('#samples: {}, #dict: {}'.format(cnt, len(dict_dataset)))\n",
    "with open('data_generated/dict_dataset_fonts.json', 'w') as f:\n",
    "    json.dump(dict_dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#label_cn:6425, #dict_dataset:6425\n",
      "label_cn - dict_dataset: 0\n",
      "dict_dataset - label_cn: 0\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import json\n",
    "\n",
    "with open('data_generated/label_cn.txt') as f:\n",
    "    dict_dataset = [c.strip() for c in f.readlines()[:-1]] + [' ']\n",
    "with open('data_generated/dict_dataset.json') as f:\n",
    "    dict_dataset_generated = json.load(f)\n",
    "print(f'#label_cn:{len(dict_dataset)}, #dict_dataset:{len(dict_dataset_generated)}')\n",
    "print(f'label_cn - dict_dataset: {len(set(dict_dataset) - set(dict_dataset_generated))}')\n",
    "print(f'dict_dataset - label_cn: {len(set(dict_dataset_generated) - set(dict_dataset))}')\n",
    "cnt_p = 0\n",
    "cnt_eng = 0\n",
    "cnt_ch_p = 0\n",
    "chinese_p = '，。“？《》{}【】、（）(),.!*&^%#@￥！ '\n",
    "chinese_p_t = set()\n",
    "for c in dict_dataset:\n",
    "    if c in string.punctuation:\n",
    "        cnt_p += 1\n",
    "    if c in string.ascii_letters:\n",
    "        cnt_eng += 1\n",
    "    if c in chinese_p:\n",
    "        cnt_ch_p += 1\n",
    "        chinese_p_t.add(c)\n",
    "assert cnt_p == len(string.punctuation), cnt_p\n",
    "assert cnt_eng == len(string.ascii_letters), cnt_eng\n",
    "assert cnt_ch_p == len(chinese_p), set(chinese_p) - chinese_p_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/cnocr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2020-05-06 18:54:15,583 _get_module:182] loading model parameters from dir /data/xiaowentao/.cnocr/1.1.0/conv-lite-lstm \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 560)\n",
      "专案组已依法对谢亚龙、李冬生、蔚少辉立案\n",
      "20\n",
      "(560, 32)\n",
      "(32, 560)\n",
      "招商局地产北京管理总部营销总监李杰先生对\n",
      "20\n",
      "['专案粗已依法对谢亚龙·李冬生·蔚少耀立案', '招商局地产北京管理总部营销总监李杰先生对']\n",
      "/amax/data/xiaowentao/chineseocr/cnocr\n",
      "(32, 560)\n",
      "是这样一部电影，所以我们的官网大家可以看\n",
      "20\n",
      "(560, 32)\n",
      "(32, 560)\n",
      "“哎呀！小曹，你怎么把我的票给打错了呢？\n",
      "20\n",
      "['是这样一部电影，所以我们的官网大家可以看', '”哎呀！小曹，你怎么把我的票给打错了呢？']\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=560x32 at 0x7F1891538DD0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%cd /data/xiaowentao/chineseocr/cnocr\n",
    "import h5py\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "from cnocr import CnOcr\n",
    "from cnocr.line_split import line_split\n",
    "import numpy as np\n",
    "\n",
    "idx = '0'\n",
    "with h5py.File('../data_generated/dataset_fonts.h5', 'r') as d:\n",
    "    print(d[idx]['img'][...].shape)\n",
    "    img = Image.fromarray(d[idx]['img'][...])\n",
    "    label = str(d[idx]['y'][...])\n",
    "    print(label)\n",
    "    print(len(label))\n",
    "\n",
    "print(img.size)\n",
    "# ratio = 32 / img.size[1]\n",
    "# new_sz = [int(s * ratio) for s in img.size]\n",
    "# print('new_sz:', new_sz)\n",
    "# # img.thumbnail(new_sz, Image.ANTIALIAS)\n",
    "# img = img.resize(new_sz, Image.ANTIALIAS)\n",
    "# print(img.size)\n",
    "# target_w = 1000\n",
    "# delta_w = target_w - img.size[0]\n",
    "# img = ImageOps.expand(img, (delta_w // 2, 0, delta_w - delta_w // 2, 0), fill=255)\n",
    "# print(img.size)\n",
    "# for i in line_split(img):\n",
    "#     print('img: ', i[0].shape)\n",
    "\n",
    "idx2 = '1'\n",
    "with h5py.File('../data_generated/dataset_fonts.h5', 'r') as d:\n",
    "    print(d[idx2]['img'][...].shape)\n",
    "    img2 = Image.fromarray(d[idx2]['img'][...])\n",
    "    label = str(d[idx2]['y'][...])\n",
    "    print(label)\n",
    "    print(len(label))\n",
    "    \n",
    "crnn = CnOcr(\n",
    "    model_name='conv-lite-lstm', model_epoch=47)\n",
    "print([''.join(r) for r in crnn.ocr_for_single_lines(\n",
    "    [np.array(img), np.array(img2)])])\n",
    "# img\n",
    "\n",
    "\n",
    "%cd /data/xiaowentao/chineseocr/cnocr\n",
    "import h5py\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "from cnocr import CnOcr\n",
    "from cnocr.line_split import line_split\n",
    "import numpy as np\n",
    "\n",
    "idx = '100086'\n",
    "with h5py.File('../data_generated/dataset_fonts.h5', 'r') as d:\n",
    "    print(d[idx]['img'][...].shape)\n",
    "    img = Image.fromarray(d[idx]['img'][...])\n",
    "    label = str(d[idx]['y'][...])\n",
    "    print(label)\n",
    "    print(len(label))\n",
    "\n",
    "print(img.size)\n",
    "# ratio = 32 / img.size[1]\n",
    "# new_sz = [int(s * ratio) for s in img.size]\n",
    "# print('new_sz:', new_sz)\n",
    "# # img.thumbnail(new_sz, Image.ANTIALIAS)\n",
    "# img = img.resize(new_sz, Image.ANTIALIAS)\n",
    "# print(img.size)\n",
    "# target_w = 1000\n",
    "# delta_w = target_w - img.size[0]\n",
    "# img = ImageOps.expand(img, (delta_w // 2, 0, delta_w - delta_w // 2, 0), fill=255)\n",
    "# print(img.size)\n",
    "# for i in line_split(img):\n",
    "#     print('img: ', i[0].shape)\n",
    "\n",
    "idx2 = '11'\n",
    "with h5py.File('../data_generated/dataset_fonts.h5', 'r') as d:\n",
    "    print(d[idx2]['img'][...].shape)\n",
    "    img2 = Image.fromarray(d[idx2]['img'][...])\n",
    "    label = str(d[idx2]['y'][...])\n",
    "    print(label)\n",
    "    print(len(label))\n",
    "    \n",
    "# crnn = CnOcr(\n",
    "#     model_name='conv-lite-lstm', model_epoch=47)\n",
    "print([''.join(r) for r in crnn.ocr_for_single_lines(\n",
    "    [np.array(img), np.array(img2)])])\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_len: 745 idx: 1046079\n",
      "cnt w/ width <= 700: 1374731, total: 1374903\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "\n",
    "\n",
    "max_len = 0\n",
    "max_id = None\n",
    "threshold = int(560 / 32 * 40)\n",
    "# 至少五种不同字符\n",
    "thre_cnt_char = 6\n",
    "cnt = 0\n",
    "cnt_total = 0\n",
    "with h5py.File('data_generated/dataset_new2.h5', 'r') as d:\n",
    "    cnt_total = len(d)\n",
    "    for idx in d:\n",
    "        len_ = d[idx]['img'][...].shape[1]\n",
    "        label = str(d[idx]['y'][...])\n",
    "        if max_len < len_:\n",
    "            max_len = len_\n",
    "            max_id = idx\n",
    "        if len_ <= threshold and len(set(label)) >= thre_cnt_char:\n",
    "            cnt += 1\n",
    "print('max_len:', max_len, 'idx:', max_id)\n",
    "print(f'cnt w/ width <= {threshold}: {cnt}, total: {cnt_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target height: 32, width: 560\n",
      "max_len: 633 idx: 1004925\n",
      "max_len: 284 idx: 195574\n",
      "final samples: 1374731, previous total: 1374903\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from PIL import Image, ImageOps\n",
    "import torch\n",
    "\n",
    "\n",
    "max_len = 0\n",
    "max_id = None\n",
    "min_len = 100000\n",
    "min_id = None\n",
    "# suitable for CnOcr\n",
    "thre_w = int(280 * 2 / 32 * 40)\n",
    "target_w = int(280 * 2)\n",
    "target_h = 32\n",
    "min_cnt_char = 6\n",
    "cnt = 0\n",
    "cnt_total = 0\n",
    "print(f'target height: {target_h}, width: {target_w}')\n",
    "\n",
    "with h5py.File('data_generated/dataset_new.h5', 'r') as d_in, h5py.File(\n",
    "        'data_generated/dataset_new2.h5', 'a') as d_out:\n",
    "    cnt_total = len(d_in)\n",
    "    for idx in d_in:\n",
    "        img = d_in[idx]['img'][...]\n",
    "        lbl = str(d_in[idx]['y'][...])\n",
    "        len_ = img.shape[1]\n",
    "        if len_ <= thre_w and len(set(lbl)) >= min_cnt_char:\n",
    "            img = Image.fromarray(img)\n",
    "            ratio = target_h / img.size[1]\n",
    "            new_sz = [int(s * ratio) for s in img.size]\n",
    "            img = img.resize(new_sz, Image.ANTIALIAS)\n",
    "            delta_w = target_w - img.size[0]\n",
    "            img = ImageOps.expand(img, (delta_w // 2, 0, delta_w - delta_w // 2, 0),\n",
    "                                  fill=255)\n",
    "            img = np.array(img)\n",
    "            assert list(img.shape) == [target_h, target_w]\n",
    "            d_out[str(cnt) + '/img'] = img\n",
    "            d_out[str(cnt) + '/y'] = lbl\n",
    "            cnt += 1\n",
    "            if max_len < len_:\n",
    "                max_len = len_\n",
    "                max_id = idx\n",
    "            if min_len > len_:\n",
    "                min_len = len_\n",
    "                min_id = idx\n",
    "print('max_len:', max_len, 'idx:', max_id)\n",
    "print('max_len:', min_len, 'idx:', min_id)\n",
    "print(f'final samples: {cnt}, previous total: {cnt_total}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1374731\n",
      "#charset:5373, #label_cn:6425\n",
      "label_cn - charset: 1051\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "with open('data_generated/label_cn.txt') as f:\n",
    "    dict_dataset = [c.strip() for c in f.readlines()[:-1]] + [' ']\n",
    "charset = {}\n",
    "cnt = 0\n",
    "h, w = (32, 560)\n",
    "label_len = {}\n",
    "with h5py.File('data_generated/dataset.h5', 'r') as d:\n",
    "    print(len(d))\n",
    "    for idx in d:\n",
    "        label = str(d[idx]['y'][...])\n",
    "        for c in label:\n",
    "            if c not in charset:\n",
    "                charset[c] = 1\n",
    "            else:\n",
    "                charset[c] += 1\n",
    "        shape = d[idx]['img'][...].shape\n",
    "        assert [h, w] == list(shape), shape\n",
    "print(f'#charset:{len(charset)}, #label_cn:{len(dict_dataset)}')\n",
    "print(f'label_cn - charset: {len(set(dict_dataset) - set(charset))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'charset - label_cn: {len(set(charset) - set(dict_dataset))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['王力宏表示，“这个世界上每一位为生活和事业打拼，',\n",
       " '为名利和地位奔波，！，为理想和未来忙碌的男人，',\n",
       " '只要他找到了那个无论贫富贵贱都愿意和他牵手陪伴他一生的女人',\n",
       " '，他就是这个世界上比总统还要成功的男人。”']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textwrap3 import wrap\n",
    "# 修改版的 wrap，偏向于按照中文标点符号断句\n",
    "\n",
    "wrap('王力宏表示，“这个世界上每一位为生活和事业打拼，为名利和地位奔波，！，为理想和未来忙碌的男人，只要他找到了那个无论贫富贵贱都愿意和他牵手陪伴他一生的女人，他就是这个世界上比总统还要成功的男人。”',\n",
    "     width=24, replace_whitespace=False, break_long_words=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 19 00:39:28 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 430.14       Driver Version: 430.14       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:18:00.0  On |                  N/A |\n",
      "| 22%   26C    P8    15W / 250W |   2686MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:3B:00.0  On |                  N/A |\n",
      "| 22%   35C    P2    59W / 250W |   1076MiB / 11019MiB |      6%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:86:00.0  On |                  N/A |\n",
      "| 26%   49C    P2   219W / 250W |   5900MiB / 11019MiB |     33%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:AF:00.0  On |                  N/A |\n",
      "| 22%   26C    P8    17W / 250W |     27MiB / 11019MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     21519      C   python                                      2635MiB |\n",
      "|    0    243184      G   /usr/lib/xorg/Xorg                            39MiB |\n",
      "|    1     49741      C   /data/xiaowentao/.anaconda3/bin/python      1049MiB |\n",
      "|    1    243184      G   /usr/lib/xorg/Xorg                            15MiB |\n",
      "|    2    203071      C   python                                      5873MiB |\n",
      "|    2    243184      G   /usr/lib/xorg/Xorg                            15MiB |\n",
      "|    3    243184      G   /usr/lib/xorg/Xorg                            15MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:           125G         35G         42G        1.4G         46G         87G\n",
      "Swap:            0B          0B          0B\n",
      " 00:39:28 up 176 days,  7:31,  5 users,  load average: 6.06, 13.53, 14.15\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi && free -h && uptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['薄利汽配', '薄利海配'], ['露得清', '野得清'], ['NFC', 'NFC'], ['抢购', '抢购'], ['色', '色'], ['H3', 'F0Sn'], ['3C', '30'], ['ABTH', 'A9870483'], ['?我?来自原厂', '来官原局'], ['优品', '优品']]\n",
      "[['薄利汽配', '薄利海配'], ['露得清', '野得清'], ['NFC', 'NFC'], ['抢购', '抢购'], ['色', '色'], ['H3', 'FS'], ['3C', '30'], ['ABTH', 'A974'], ['?我?来自原厂', '来官原局'], ['优品', '优品']]\n",
      "dataset: 136368\n",
      "train: 109094, val: 27274\n",
      "write done.\n",
      "广丽\n",
      "扬爱义\n",
      "2 可是 沙冰\n",
      "oaa\n",
      "--------------------------------------------------\n",
      "广丽\n",
      "扬爱义\n",
      "2 可是 沙冰\n",
      "oaa\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from random import shuffle\n",
    "import jieba\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "with open('res_aux_ctc_beam.json') as f1, open('res_aux_greedy.json') as f2:\n",
    "    a1 = json.load(f1)\n",
    "    a2 = json.load(f2)\n",
    "print(a1[:10])\n",
    "print(a2[:10])\n",
    "dataset = a1 + a1\n",
    "dataset = [d for d in dataset if d[0] != d[1]]\n",
    "shuffle(dataset)\n",
    "print('dataset:', len(dataset))\n",
    "train_cnt = int(len(dataset) * train_ratio)\n",
    "train = dataset[:train_cnt]\n",
    "val = dataset[train_cnt:]\n",
    "print('train: {}, val: {}'.format(len(train), len(val)))\n",
    "train_tgt = [(' '.join(jieba.cut(s[0])) + '\\n') for s in train]\n",
    "train_src = [(' '.join(jieba.cut(s[1])) + '\\n') for s in train]\n",
    "val_tgt = [(' '.join(jieba.cut(s[0])) + '\\n') for s in val]\n",
    "val_src = [(' '.join(jieba.cut(s[1])) + '\\n') for s in val]\n",
    "with open('./NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train.tok.src', 'w') as f:\n",
    "    f.writelines(train_src)\n",
    "with open('./NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train.tok.trg', 'w') as f:\n",
    "    f.writelines(train_src)\n",
    "with open('./NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev.tok.src', 'w') as f:\n",
    "    f.writelines(val_src)\n",
    "with open('./NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev.tok.trg', 'w') as f:\n",
    "    f.writelines(val_tgt)\n",
    "print('write done.')\n",
    "!head -n 4 NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train.tok.src\n",
    "print('-'*50)\n",
    "!head -n 4 NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train.tok.trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training\n",
      "+ source ../paths.sh\n",
      "++++ dirname ../paths.sh\n",
      "+++ cd ..\n",
      "+++ pwd\n",
      "++ BASE_DIR=/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb\n",
      "++ DATA_DIR=/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data\n",
      "++ MODEL_DIR=/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/models\n",
      "++ SCRIPTS_DIR=/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts\n",
      "++ SOFTWARE_DIR=/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software\n",
      "++ TRAINING_DIR=/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training\n",
      "+ src_ext=src\n",
      "+ trg_ext=trg\n",
      "+ train_data_prefix=/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train.tok\n",
      "+ dev_data_prefix=/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev.tok\n",
      "+ SUBWORD_NMT=/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/subword-nmt\n",
      "+ FAIRSEQPY=/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py\n",
      "+ mkdir -p models/bpe_model\n",
      "+ bpe_operations=30000\n",
      "+ cat /amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train.tok.trg\n",
      "+ python /amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/subword-nmt/learn_bpe.py -s 30000\n",
      "+ mkdir -p processed/\n",
      "+ python /amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts/apply_bpe.py -c models/bpe_model/train.bpe.model\n",
      "+ python /amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts/apply_bpe.py -c models/bpe_model/train.bpe.model\n",
      "+ python /amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts/apply_bpe.py -c models/bpe_model/train.bpe.model\n",
      "+ python /amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts/apply_bpe.py -c models/bpe_model/train.bpe.model\n",
      "+ cp /amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev.tok.src processed/dev.input.txt\n",
      "+ less processed/train.all.src\n",
      "+ less processed/train.all.trg\n",
      "+ python /amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py/preprocess.py --source-lang src --target-lang trg --trainpref processed/train --validpref processed/dev --testpref processed/dev --nwordssrc 37000 --nwordstgt 37000 --destdir processed/bin\n",
      "Namespace(alignfile=None, destdir='processed/bin', joined_dictionary=False, nwordssrc=37000, nwordstgt=37000, only_source=False, output_format='binary', source_lang='src', srcdict=None, target_lang='trg', testpref='processed/dev', tgtdict=None, thresholdsrc=0, thresholdtgt=0, trainpref='processed/train', validpref='processed/dev')\n",
      "| [src] Dictionary: 31944 types\n",
      "| [src] processed/train.src: 109094 sents, 436853 tokens, 0.0% replaced by <unk>\n",
      "| [src] Dictionary: 31944 types\n",
      "| [src] processed/dev.src: 27274 sents, 116963 tokens, 0.121% replaced by <unk>\n",
      "| [src] Dictionary: 31944 types\n",
      "| [src] processed/dev.src: 27274 sents, 116963 tokens, 0.121% replaced by <unk>\n",
      "| [trg] Dictionary: 31944 types\n",
      "| [trg] processed/train.trg: 109094 sents, 436853 tokens, 0.0% replaced by <unk>\n",
      "| [trg] Dictionary: 31944 types\n",
      "| [trg] processed/dev.trg: 27274 sents, 116948 tokens, 4.93% replaced by <unk>\n",
      "| [trg] Dictionary: 31944 types\n",
      "| [trg] processed/dev.trg: 27274 sents, 116948 tokens, 4.93% replaced by <unk>\n",
      "| Wrote preprocessed data to processed/bin\n"
     ]
    }
   ],
   "source": [
    "!cd training && pwd && chmod +x preprocess.sh && ./preprocess.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ set -e\n",
      "+ source ../paths.sh\n",
      "++++ dirname ../paths.sh\n",
      "+++ cd ..\n",
      "+++ pwd\n",
      "++ BASE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb\n",
      "++ DATA_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data\n",
      "++ MODEL_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/models\n",
      "++ SCRIPTS_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts\n",
      "++ SOFTWARE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software\n",
      "++ TRAINING_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training\n",
      "+ FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py\n",
      "+ SEED=1\n",
      "+ DATA_BIN_DIR=processed/bin\n",
      "+ OUT_DIR=models/mlconv/model1/\n",
      "+ mkdir -p models/mlconv/model1/\n",
      "+ PYTHONPATH=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py:\n",
      "+ CUDA_VISIBLE_DEVICES=1,2\n",
      "+ python /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py/train.py --arch fconv --save-dir models/mlconv/model1/ --encoder-embed-dim 500 --decoder-embed-dim 500 --decoder-out-embed-dim 500 --dropout 0.2 --clip-norm 0.1 --lr 0.25 --min-lr 1e-4 --encoder-layers '[(1024,3)] * 7' --decoder-layers '[(1024,3)] * 7' --momentum 0.99 --max-epoch 100 --batch-size 32 --no-progress-bar --seed 1 processed/bin\n",
      "| distributed init (rank 0): tcp://localhost:18626\n",
      "| distributed init (rank 1): tcp://localhost:18626\n",
      "Namespace(arch='fconv', clip_norm=0.1, criterion='cross_entropy', curriculum=0, data='processed/bin', decoder_attention='True', decoder_embed_dim=500, decoder_embed_path=None, decoder_layers='[(1024,3)] * 7', decoder_out_embed_dim=500, device_id=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:18626', distributed_port=-1, distributed_rank=0, distributed_world_size=2, dropout=0.2, encoder_embed_dim=500, encoder_embed_path=None, encoder_layers='[(1024,3)] * 7', log_format=None, log_interval=1000, lr=[0.25], lr_scheduler='reduce_lr_on_plateau', lr_shrink=0.1, max_epoch=100, max_sentences=32, max_sentences_valid=32, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_update=0, min_lr=0.0001, momentum=0.99, no_epoch_checkpoints=False, no_progress_bar=True, no_save=False, optimizer='nag', restore_file='checkpoint_last.pt', sample_without_replacement=0, save_dir='models/mlconv/model1/', save_interval=-1, seed=1, sentence_avg=False, share_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, train_subset='train', valid_subset='valid', validate_interval=1, weight_decay=0.0)\n",
      "| [src] dictionary: 31945 types\n",
      "| [trg] dictionary: 31945 types\n",
      "| processed/bin train 109094 examples\n",
      "| processed/bin valid 27274 examples\n",
      "| model fconv, criterion CrossEntropyCriterion\n",
      "| num. model params: 146386550\n",
      "| training on 2 GPUs\n",
      "| max tokens per GPU = 6000 and max sentences per GPU = 32\n",
      "| loaded checkpoint models/mlconv/model1/checkpoint_last.pt (epoch 3)\n",
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/autograd/function.py:41: UserWarning: mark_shared_storage is deprecated. Tensors with shared storages are automatically tracked. Note that calls to `set_()` are not tracked\n",
      "  'mark_shared_storage is deprecated. '\n",
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/autograd/function.py:41: UserWarning: mark_shared_storage is deprecated. Tensors with shared storages are automatically tracked. Note that calls to `set_()` are not tracked\n",
      "  'mark_shared_storage is deprecated. '\n",
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py/fairseq/trainer.py:193: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  grad_norm = utils.item(torch.nn.utils.clip_grad_norm(self.model.parameters(), self.args.clip_norm))\n",
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py/fairseq/trainer.py:193: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  grad_norm = utils.item(torch.nn.utils.clip_grad_norm(self.model.parameters(), self.args.clip_norm))\n",
      "| epoch 004:   1000 / 1705 loss=0.119, ppl=1.09, wps=1221, ups=4.7, wpb=256, bsz=64, num_updates=6116, lr=0.25, gnorm=0.690, clip=96%, oom=0, sample_size=256.099\n",
      "| epoch 004 | loss 0.147 | ppl 1.11 | wps 1220 | ups 4.7 | wpb 256 | bsz 64 | num_updates 6820 | lr 0.25 | gnorm 0.714 | clip 97% | oom 0 | sample_size 256.219\n",
      "| epoch 004 | valid on 'valid' subset | valid_loss 35.0784 | valid_ppl 36278179441.45 | sample_size 266.396\n",
      "| epoch 005:   1000 / 1705 loss=0.066, ppl=1.05, wps=1243, ups=4.8, wpb=260, bsz=64, num_updates=7821, lr=0.025, gnorm=0.548, clip=68%, oom=0, sample_size=259.829\n",
      "| epoch 005 | loss 0.048 | ppl 1.03 | wps 1231 | ups 4.8 | wpb 256 | bsz 64 | num_updates 8525 | lr 0.025 | gnorm 0.463 | clip 56% | oom 0 | sample_size 256.219\n",
      "| epoch 005 | valid on 'valid' subset | valid_loss 40.1803 | valid_ppl 1245919501777.60 | sample_size 266.396\n",
      "| epoch 006:   1000 / 1705 loss=0.036, ppl=1.03, wps=1236, ups=4.8, wpb=257, bsz=64, num_updates=9526, lr=0.0025, gnorm=0.378, clip=22%, oom=0, sample_size=257.376\n",
      "| epoch 006 | loss 0.030 | ppl 1.02 | wps 1224 | ups 4.8 | wpb 256 | bsz 64 | num_updates 10230 | lr 0.0025 | gnorm 0.335 | clip 20% | oom 0 | sample_size 256.219\n",
      "| epoch 006 | valid on 'valid' subset | valid_loss 40.8954 | valid_ppl 2045269827324.55 | sample_size 266.396\n",
      "| epoch 007:   1000 / 1705 loss=0.045, ppl=1.03, wps=1263, ups=4.8, wpb=261, bsz=64, num_updates=11231, lr=0.00025, gnorm=0.292, clip=16%, oom=0, sample_size=261.303\n",
      "| epoch 007 | loss 0.028 | ppl 1.02 | wps 1227 | ups 4.8 | wpb 256 | bsz 64 | num_updates 11935 | lr 0.00025 | gnorm 0.267 | clip 15% | oom 0 | sample_size 256.219\n",
      "| epoch 007 | valid on 'valid' subset | valid_loss 40.9472 | valid_ppl 2119976331146.30 | sample_size 266.396\n",
      "| done training in 1485.3 seconds\n"
     ]
    }
   ],
   "source": [
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training && \\\n",
    "    chmod +x train.sh && ./train.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr\n",
      "[['薄利汽配', '薄利海配'], ['露得清', '野得清'], ['NFC', 'NFC'], ['抢购', '抢购'], ['色', '色'], ['H3', 'F0Sn'], ['3C', '30'], ['ABTH', 'A9870483'], ['?我?来自原厂', '来官原局'], ['优品', '优品']]\n",
      "[['薄利汽配', '薄利海配'], ['露得清', '野得清'], ['NFC', 'NFC'], ['抢购', '抢购'], ['色', '色'], ['H3', 'FS'], ['3C', '30'], ['ABTH', 'A974'], ['?我?来自原厂', '来官原局'], ['优品', '优品']]\n",
      "dataset: 136368\n",
      "train: 109094, val: 27274\n",
      "write done.\n",
      "广丽\n",
      "扬爱义\n",
      "2 可是 沙冰\n",
      "oaa\n",
      "--------------------------------------------------\n",
      "广丽\n",
      "扬爱义\n",
      "2 可是 沙冰\n",
      "oaa\n",
      "+ source ../paths.sh\n",
      "++++ dirname ../paths.sh\n",
      "+++ cd ..\n",
      "+++ pwd\n",
      "++ BASE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb\n",
      "++ DATA_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data\n",
      "++ MODEL_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/models\n",
      "++ SCRIPTS_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts\n",
      "++ SOFTWARE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software\n",
      "++ TRAINING_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training\n",
      "+ src_ext=src\n",
      "+ trg_ext=trg\n",
      "+ train_data_prefix=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train_no_bpe.tok\n",
      "+ dev_data_prefix=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe.tok\n",
      "+ SUBWORD_NMT=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/subword-nmt\n",
      "+ FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py\n",
      "+ mkdir -p processed/\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train_no_bpe.tok.src processed/train.all.src\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train_no_bpe.tok.trg processed/train.all.trg\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe.tok.src processed/dev.src\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe.tok.trg processed/dev.trg\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe.tok.src processed/dev.input.txt\n",
      "+ less processed/train.all.src\n",
      "+ less processed/train.all.trg\n",
      "+ python /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py/preprocess.py --source-lang src --target-lang trg --trainpref processed/train --validpref processed/dev --testpref processed/dev --destdir processed/bin\n",
      "Namespace(alignfile=None, destdir='processed/bin', joined_dictionary=False, nwordssrc=-1, nwordstgt=-1, only_source=False, output_format='binary', source_lang='src', srcdict=None, target_lang='trg', testpref='processed/dev', tgtdict=None, thresholdsrc=0, thresholdtgt=0, trainpref='processed/train', validpref='processed/dev')\n",
      "| [src] Dictionary: 3339 types\n",
      "| [src] processed/train.src: 109094 sents, 677224 tokens, 0.0% replaced by <unk>\n",
      "| [src] Dictionary: 3339 types\n",
      "| [src] processed/dev.src: 27274 sents, 170380 tokens, 0.0211% replaced by <unk>\n",
      "| [src] Dictionary: 3339 types\n",
      "| [src] processed/dev.src: 27274 sents, 170380 tokens, 0.0211% replaced by <unk>\n",
      "| [trg] Dictionary: 3339 types\n",
      "| [trg] processed/train.trg: 109094 sents, 677224 tokens, 0.0% replaced by <unk>\n",
      "| [trg] Dictionary: 3339 types\n",
      "| [trg] processed/dev.trg: 27274 sents, 180106 tokens, 2.79% replaced by <unk>\n",
      "| [trg] Dictionary: 3339 types\n",
      "| [trg] processed/dev.trg: 27274 sents, 180106 tokens, 2.79% replaced by <unk>\n",
      "| Wrote preprocessed data to processed/bin\n",
      "+ set -e\n",
      "+ source ../paths.sh\n",
      "++++ dirname ../paths.sh\n",
      "+++ cd ..\n",
      "+++ pwd\n",
      "++ BASE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb\n",
      "++ DATA_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data\n",
      "++ MODEL_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/models\n",
      "++ SCRIPTS_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts\n",
      "++ SOFTWARE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software\n",
      "++ TRAINING_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training\n",
      "+ FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py\n",
      "+ SEED=1\n",
      "+ DATA_BIN_DIR=processed/bin\n",
      "+ OUT_DIR=models/mlconv_embed/model1/\n",
      "+ mkdir -p models/mlconv_embed/model1/\n",
      "+ rm -rvf models/mlconv_embed/model1//checkpoint1.pt models/mlconv_embed/model1//checkpoint2.pt models/mlconv_embed/model1//checkpoint3.pt models/mlconv_embed/model1//checkpoint4.pt models/mlconv_embed/model1//checkpoint5.pt models/mlconv_embed/model1//checkpoint_best.pt models/mlconv_embed/model1//checkpoint_last.pt\n",
      "removed 'models/mlconv_embed/model1//checkpoint1.pt'\n",
      "removed 'models/mlconv_embed/model1//checkpoint2.pt'\n",
      "removed 'models/mlconv_embed/model1//checkpoint3.pt'\n",
      "removed 'models/mlconv_embed/model1//checkpoint4.pt'\n",
      "removed 'models/mlconv_embed/model1//checkpoint5.pt'\n",
      "removed 'models/mlconv_embed/model1//checkpoint_best.pt'\n",
      "removed 'models/mlconv_embed/model1//checkpoint_last.pt'\n",
      "+ env PYTHONPATH=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py: CUDA_VISIBLE_DEVICES=1,2 python /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py/train.py --save-dir models/mlconv_embed/model1/ --arch fconv --encoder-embed-dim 500 --decoder-embed-dim 500 --decoder-out-embed-dim 500 --dropout 0.2 --clip-norm 0.1 --lr 0.25 --min-lr 1e-4 --encoder-layers '[(1024,3)] * 7' --decoder-layers '[(1024,3)] * 7' --momentum 0.99 --max-epoch 100 --batch-size 32 --no-progress-bar --seed 1 processed/bin\n",
      "| distributed init (rank 0): tcp://localhost:18616\n",
      "| distributed init (rank 1): tcp://localhost:18616\n",
      "Namespace(arch='fconv', clip_norm=0.1, criterion='cross_entropy', curriculum=0, data='processed/bin', decoder_attention='True', decoder_embed_dim=500, decoder_embed_path=None, decoder_layers='[(1024,3)] * 7', decoder_out_embed_dim=500, device_id=0, distributed_backend='nccl', distributed_init_method='tcp://localhost:18616', distributed_port=-1, distributed_rank=0, distributed_world_size=2, dropout=0.2, encoder_embed_dim=500, encoder_embed_path=None, encoder_layers='[(1024,3)] * 7', log_format=None, log_interval=1000, lr=[0.25], lr_scheduler='reduce_lr_on_plateau', lr_shrink=0.1, max_epoch=100, max_sentences=32, max_sentences_valid=32, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_update=0, min_lr=0.0001, momentum=0.99, no_epoch_checkpoints=False, no_progress_bar=True, no_save=False, optimizer='nag', restore_file='checkpoint_last.pt', sample_without_replacement=0, save_dir='models/mlconv_embed/model1/', save_interval=-1, seed=1, sentence_avg=False, share_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, train_subset='train', valid_subset='valid', validate_interval=1, weight_decay=0.0)\n",
      "| [src] dictionary: 3340 types\n",
      "| [trg] dictionary: 3340 types\n",
      "| processed/bin train 109094 examples\n",
      "| processed/bin valid 27274 examples\n",
      "| model fconv, criterion CrossEntropyCriterion\n",
      "| num. model params: 103421840\n",
      "| training on 2 GPUs\n",
      "| max tokens per GPU = 6000 and max sentences per GPU = 32\n",
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/autograd/function.py:41: UserWarning: mark_shared_storage is deprecated. Tensors with shared storages are automatically tracked. Note that calls to `set_()` are not tracked\n",
      "  'mark_shared_storage is deprecated. '\n",
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/autograd/function.py:41: UserWarning: mark_shared_storage is deprecated. Tensors with shared storages are automatically tracked. Note that calls to `set_()` are not tracked\n",
      "  'mark_shared_storage is deprecated. '\n",
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py/fairseq/trainer.py:193: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  grad_norm = utils.item(torch.nn.utils.clip_grad_norm(self.model.parameters(), self.args.clip_norm))\n",
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py/fairseq/trainer.py:193: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  grad_norm = utils.item(torch.nn.utils.clip_grad_norm(self.model.parameters(), self.args.clip_norm))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 001:   1000 / 1705 loss=2.589, ppl=6.02, wps=2315, ups=5.9, wpb=391, bsz=64, num_updates=1001, lr=0.25, gnorm=1.680, clip=99%, oom=0, sample_size=390.952\n",
      "| epoch 001 | loss 1.628 | ppl 3.09 | wps 2384 | ups 6.0 | wpb 397 | bsz 64 | num_updates 1705 | lr 0.25 | gnorm 1.337 | clip 98% | oom 0 | sample_size 397.199\n",
      "| epoch 001 | valid on 'valid' subset | valid_loss 21.2719 | valid_ppl 2532162.29 | sample_size 401.127\n",
      "| epoch 002:   1000 / 1705 loss=0.231, ppl=1.17, wps=2392, ups=6.0, wpb=400, bsz=64, num_updates=2706, lr=0.25, gnorm=0.984, clip=88%, oom=0, sample_size=400.344\n",
      "| epoch 002 | loss 0.182 | ppl 1.13 | wps 2365 | ups 6.0 | wpb 397 | bsz 64 | num_updates 3410 | lr 0.25 | gnorm 0.852 | clip 86% | oom 0 | sample_size 397.199\n",
      "| epoch 002 | valid on 'valid' subset | valid_loss 27.263 | valid_ppl 161059454.41 | sample_size 401.127\n",
      "| epoch 003:   1000 / 1705 loss=0.053, ppl=1.04, wps=2450, ups=6.2, wpb=392, bsz=64, num_updates=4411, lr=0.025, gnorm=0.683, clip=28%, oom=0, sample_size=392.143\n",
      "| epoch 003 | loss 0.083 | ppl 1.06 | wps 2408 | ups 6.1 | wpb 397 | bsz 64 | num_updates 5115 | lr 0.025 | gnorm 0.597 | clip 21% | oom 0 | sample_size 397.199\n",
      "| epoch 003 | valid on 'valid' subset | valid_loss 27.7438 | valid_ppl 224755739.55 | sample_size 401.127\n",
      "| epoch 004:   1000 / 1705 loss=0.031, ppl=1.02, wps=2406, ups=6.1, wpb=397, bsz=64, num_updates=6116, lr=0.0025, gnorm=0.504, clip=7%, oom=0, sample_size=397.139\n",
      "| epoch 004 | loss 0.060 | ppl 1.04 | wps 2435 | ups 6.1 | wpb 397 | bsz 64 | num_updates 6820 | lr 0.0025 | gnorm 0.458 | clip 7% | oom 0 | sample_size 397.199\n",
      "| epoch 004 | valid on 'valid' subset | valid_loss 27.9061 | valid_ppl 251513380.21 | sample_size 401.127\n",
      "| epoch 005:   1000 / 1705 loss=0.070, ppl=1.05, wps=2370, ups=5.9, wpb=404, bsz=64, num_updates=7821, lr=0.00025, gnorm=0.403, clip=6%, oom=0, sample_size=404.136\n",
      "| epoch 005 | loss 0.057 | ppl 1.04 | wps 2379 | ups 6.0 | wpb 397 | bsz 64 | num_updates 8525 | lr 0.00025 | gnorm 0.373 | clip 6% | oom 0 | sample_size 397.199\n",
      "| epoch 005 | valid on 'valid' subset | valid_loss 27.9048 | valid_ppl 251299549.53 | sample_size 401.127\n",
      "| done training in 1479.9 seconds\n"
     ]
    }
   ],
   "source": [
    "%cd /data/xiaowentao/chineseocr/\n",
    "import json\n",
    "from random import shuffle\n",
    "# import jieba\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "with open('res_aux_ctc_beam.json') as f1, open('res_aux_greedy.json') as f2:\n",
    "    a1 = json.load(f1)\n",
    "    a2 = json.load(f2)\n",
    "print(a1[:10])\n",
    "print(a2[:10])\n",
    "dataset = a1 + a1\n",
    "dataset = [d for d in dataset if d[0] != d[1]]\n",
    "shuffle(dataset)\n",
    "print('dataset:', len(dataset))\n",
    "train_cnt = int(len(dataset) * train_ratio)\n",
    "train = dataset[:train_cnt]\n",
    "val = dataset[train_cnt:]\n",
    "print('train: {}, val: {}'.format(len(train), len(val)))\n",
    "train_tgt = [(' '.join(s[0]) + '\\n') for s in train]\n",
    "train_src = [(' '.join(s[1]) + '\\n') for s in train]\n",
    "val_tgt = [(' '.join(s[0]) + '\\n') for s in val]\n",
    "val_src = [(' '.join(s[1]) + '\\n') for s in val]\n",
    "with open('./NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train_no_bpe.tok.src', 'w') as f:\n",
    "    f.writelines(train_src)\n",
    "with open('./NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train_no_bpe.tok.trg', 'w') as f:\n",
    "    f.writelines(train_src)\n",
    "with open('./NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe.tok.src', 'w') as f:\n",
    "    f.writelines(val_src)\n",
    "with open('./NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe.tok.trg', 'w') as f:\n",
    "    f.writelines(val_tgt)\n",
    "print('write done.')\n",
    "!head -n 4 NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train.tok.src\n",
    "print('-'*50)\n",
    "!head -n 4 NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train.tok.trg\n",
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training && \\\n",
    "    chmod +x preprocess_no_bpe.sh && ./preprocess_no_bpe.sh && \\\n",
    "    chmod +x train_no_bpe.sh && ./train_no_bpe.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train=1309609, #dev=327403\n",
      "EM_train:843522, EM_dev:210426\n",
      "after downsampling, #train=887581, #dev=221931\n",
      "after downsampling, EM_train:421494, EM_dev:104954\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "prefix = '/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data'\n",
    "# !cp -v {prefix}/train_no_bpe.tok.src {prefix}/train_no_bpe.tok.src.old\n",
    "# !cp -v {prefix}/train_no_bpe.tok.trg {prefix}/train_no_bpe.tok.trg.old\n",
    "# !cp -v {prefix}/dev_no_bpe.tok.src {prefix}/dev_no_bpe.tok.src.old\n",
    "# !cp -v {prefix}/dev_no_bpe.tok.trg {prefix}/dev_no_bpe.tok.trg.old\n",
    "with open(prefix + '/train_no_bpe.tok.src.old', 'r') as f:\n",
    "    train_src = [' '.join(l.strip()) for l in f.readlines()]\n",
    "with open(prefix + '/train_no_bpe.tok.trg.old', 'r') as f:\n",
    "    train_trg = [' '.join(l.strip()) for l in f.readlines()]\n",
    "with open(prefix + '/dev_no_bpe.tok.src.old', 'r') as f:\n",
    "    val_src = [' '.join(l.strip()) for l in f.readlines()]\n",
    "with open(prefix + '/dev_no_bpe.tok.trg.old', 'r') as f:\n",
    "    val_trg = [' '.join(l.strip()) for l in f.readlines()]\n",
    "print(f'#train={len(train_src)}, #dev={len(val_src)}')\n",
    "assert len(train_src) == len(train_trg)\n",
    "assert len(val_src) == len(val_trg)\n",
    "cnt_em_train = sum([s == t for s, t in zip(train_src, train_trg)])\n",
    "cnt_em_val = sum([s == t for s, t in zip(val_src, val_trg)])\n",
    "print(f'EM_train:{cnt_em_train}, EM_dev:{cnt_em_val}')\n",
    "\n",
    "# downsample to 0.5 fot exact match samples\n",
    "train_src_ds, train_trg_ds = list(zip(*[(s, t) for s, t in zip(\n",
    "    train_src, train_trg) if ((s != t) or (random() < 0.5))]))\n",
    "val_src_ds, val_trg_ds = list(zip(*[(s, t) for s, t in zip(\n",
    "    val_src, val_trg) if ((s != t) or (random() < 0.5))]))\n",
    "cnt_em_train = sum([s == t for s, t in zip(train_src_ds, train_trg_ds)])\n",
    "cnt_em_val = sum([s == t for s, t in zip(val_src_ds, val_trg_ds)])\n",
    "assert len(train_src_ds) == len(train_trg_ds)\n",
    "assert len(val_src_ds) == len(val_trg_ds)\n",
    "print(f'after downsampling, #train={len(train_src_ds)}, #dev={len(val_src_ds)}')\n",
    "print(f'after downsampling, EM_train:{cnt_em_train}, EM_dev:{cnt_em_val}')\n",
    "\n",
    "with open(prefix + '/train_no_bpe.tok.src', 'w') as f:\n",
    "    f.writelines([l + '\\n' for l in train_src_ds])\n",
    "with open(prefix + '/train_no_bpe.tok.trg', 'w') as f:\n",
    "    f.writelines([l + '\\n' for l in train_trg_ds])\n",
    "with open(prefix + '/dev_no_bpe.tok.src', 'w') as f:\n",
    "    f.writelines([l + '\\n' for l in val_src_ds])\n",
    "with open(prefix + '/dev_no_bpe.tok.trg', 'w') as f:\n",
    "    f.writelines([l + '\\n' for l in val_trg_ds])\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "专 案 粗 已 依 法 对 谢 亚 龙 · 李 冬 生 · 蔚 少 耀 立 案\n",
      "招 商 局 地 产 北 京 管 理 总 部 营 销 总 监 李 杰 先 生 对\n",
      "美 国 军 方 显 然 已 经 走 在 了 理 论 研 究 的 前 面 。 1\n",
      "推 进 光 线 到 户 的 多 种 技 术 中 · E P O N 一 直 是\n",
      "--------------------------------------------------\n",
      "专 案 组 已 依 法 对 谢 亚 龙 、 李 冬 生 、 蔚 少 辉 立 案\n",
      "招 商 局 地 产 北 京 管 理 总 部 营 销 总 监 李 杰 先 生 对\n",
      "美 国 军 方 显 然 已 经 走 在 了 理 论 研 究 的 前 面 。 1\n",
      "推 进 光 线 到 户 的 多 种 技 术 中 ， E P O N 一 直 是\n",
      "+ source ../paths.sh\n",
      "++++ dirname ../paths.sh\n",
      "+++ cd ..\n",
      "+++ pwd\n",
      "++ BASE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb\n",
      "++ DATA_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data\n",
      "++ MODEL_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/models\n",
      "++ SCRIPTS_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts\n",
      "++ SOFTWARE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software\n",
      "++ TRAINING_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training\n",
      "+ src_ext=src\n",
      "+ trg_ext=trg\n",
      "+ train_data_prefix=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train_no_bpe.tok\n",
      "+ dev_data_prefix=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe.tok\n",
      "+ SUBWORD_NMT=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/subword-nmt\n",
      "+ FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py\n",
      "+ mkdir -p processed/\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train_no_bpe.tok.src processed/train.all.src\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train_no_bpe.tok.trg processed/train.all.trg\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe.tok.src processed/dev.src\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe.tok.trg processed/dev.trg\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe.tok.src processed/dev.input.txt\n",
      "+ less processed/train.all.src\n",
      "+ less processed/train.all.trg\n",
      "+ python /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py/preprocess.py --source-lang src --target-lang trg --trainpref processed/train --validpref processed/dev --testpref processed/dev --destdir processed/bin\n",
      "Namespace(alignfile=None, destdir='processed/bin', joined_dictionary=False, nwordssrc=-1, nwordstgt=-1, only_source=False, output_format='binary', source_lang='src', srcdict=None, target_lang='trg', testpref='processed/dev', tgtdict=None, thresholdsrc=0, thresholdtgt=0, trainpref='processed/train', validpref='processed/dev')\n",
      "| [src] Dictionary: 5417 types\n",
      "| [src] processed/train.src: 887581 sents, 18404200 tokens, 0.0% replaced by <unk>\n",
      "| [src] Dictionary: 5417 types\n",
      "| [src] processed/dev.src: 221931 sents, 4601600 tokens, 0.00178% replaced by <unk>\n",
      "| [src] Dictionary: 5417 types\n",
      "| [src] processed/dev.src: 221931 sents, 4601600 tokens, 0.00178% replaced by <unk>\n",
      "| [trg] Dictionary: 5324 types\n",
      "| [trg] processed/train.trg: 887581 sents, 18639201 tokens, 0.0% replaced by <unk>\n",
      "| [trg] Dictionary: 5324 types\n",
      "| [trg] processed/dev.trg: 221931 sents, 4660551 tokens, 0.00824% replaced by <unk>\n",
      "| [trg] Dictionary: 5324 types\n",
      "| [trg] processed/dev.trg: 221931 sents, 4660551 tokens, 0.00824% replaced by <unk>\n",
      "| Wrote preprocessed data to processed/bin\n",
      "+ set -e\n",
      "+ source ../paths.sh\n",
      "++++ dirname ../paths.sh\n",
      "+++ cd ..\n",
      "+++ pwd\n",
      "++ BASE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb\n",
      "++ DATA_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data\n",
      "++ MODEL_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/models\n",
      "++ SCRIPTS_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts\n",
      "++ SOFTWARE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software\n",
      "++ TRAINING_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training\n",
      "+ FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py\n",
      "+ SEED=1\n",
      "+ DATA_BIN_DIR=processed/bin\n",
      "+ OUT_DIR=models/mlconv_embed/model1/\n",
      "+ mkdir -p models/mlconv_embed/model1/\n",
      "+ rm -rvf models/mlconv_embed/model1//checkpoint1.pt models/mlconv_embed/model1//checkpoint2.pt models/mlconv_embed/model1//checkpoint3.pt models/mlconv_embed/model1//checkpoint4.pt models/mlconv_embed/model1//checkpoint5.pt models/mlconv_embed/model1//checkpoint_best.pt models/mlconv_embed/model1//checkpoint_last.pt\n",
      "removed 'models/mlconv_embed/model1//checkpoint1.pt'\n",
      "removed 'models/mlconv_embed/model1//checkpoint2.pt'\n",
      "removed 'models/mlconv_embed/model1//checkpoint3.pt'\n",
      "removed 'models/mlconv_embed/model1//checkpoint4.pt'\n",
      "removed 'models/mlconv_embed/model1//checkpoint5.pt'\n",
      "removed 'models/mlconv_embed/model1//checkpoint_best.pt'\n",
      "removed 'models/mlconv_embed/model1//checkpoint_last.pt'\n",
      "+ env PYTHONPATH=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py: CUDA_VISIBLE_DEVICES=3 python /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py/train.py --save-dir models/mlconv_embed/model1/ --arch fconv --encoder-embed-dim 500 --decoder-embed-dim 500 --decoder-out-embed-dim 500 --dropout 0.2 --clip-norm 0.1 --lr 0.25 --min-lr 1e-4 --encoder-layers '[(1024,3)] * 7' --decoder-layers '[(1024,3)] * 7' --momentum 0.99 --max-epoch 100 --batch-size 32 --no-progress-bar --seed 1 processed/bin\n",
      "Namespace(arch='fconv', clip_norm=0.1, criterion='cross_entropy', curriculum=0, data='processed/bin', decoder_attention='True', decoder_embed_dim=500, decoder_embed_path=None, decoder_layers='[(1024,3)] * 7', decoder_out_embed_dim=500, device_id=0, distributed_backend='nccl', distributed_init_method=None, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, encoder_embed_dim=500, encoder_embed_path=None, encoder_layers='[(1024,3)] * 7', log_format=None, log_interval=1000, lr=[0.25], lr_scheduler='reduce_lr_on_plateau', lr_shrink=0.1, max_epoch=100, max_sentences=32, max_sentences_valid=32, max_source_positions=1024, max_target_positions=1024, max_tokens=6000, max_update=0, min_lr=0.0001, momentum=0.99, no_epoch_checkpoints=False, no_progress_bar=True, no_save=False, optimizer='nag', restore_file='checkpoint_last.pt', sample_without_replacement=0, save_dir='models/mlconv_embed/model1/', save_interval=-1, seed=1, sentence_avg=False, share_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang=None, target_lang=None, train_subset='train', valid_subset='valid', validate_interval=1, weight_decay=0.0)\n",
      "| [src] dictionary: 5418 types\n",
      "| [trg] dictionary: 5325 types\n",
      "| processed/bin train 887581 examples\n",
      "| processed/bin valid 221931 examples\n",
      "| model fconv, criterion CrossEntropyCriterion\n",
      "| num. model params: 106449810\n",
      "| training on 1 GPUs\n",
      "| max tokens per GPU = 6000 and max sentences per GPU = 32\n",
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/autograd/function.py:41: UserWarning: mark_shared_storage is deprecated. Tensors with shared storages are automatically tracked. Note that calls to `set_()` are not tracked\n",
      "  'mark_shared_storage is deprecated. '\n",
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py/fairseq/trainer.py:193: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  grad_norm = utils.item(torch.nn.utils.clip_grad_norm(self.model.parameters(), self.args.clip_norm))\n",
      "| epoch 001:   1000 / 27737 loss=2.796, ppl=6.94, wps=8875, ups=13.0, wpb=672, bsz=32, num_updates=1001, lr=0.25, gnorm=0.840, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:   2000 / 27737 loss=1.873, ppl=3.66, wps=8913, ups=13.2, wpb=672, bsz=32, num_updates=2001, lr=0.25, gnorm=0.645, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:   3000 / 27737 loss=1.511, ppl=2.85, wps=8911, ups=13.2, wpb=672, bsz=32, num_updates=3001, lr=0.25, gnorm=0.553, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:   4000 / 27737 loss=1.296, ppl=2.45, wps=8881, ups=13.2, wpb=672, bsz=32, num_updates=4001, lr=0.25, gnorm=0.495, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:   5000 / 27737 loss=1.183, ppl=2.27, wps=8877, ups=13.2, wpb=672, bsz=32, num_updates=5001, lr=0.25, gnorm=0.459, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:   6000 / 27737 loss=1.083, ppl=2.12, wps=8864, ups=13.2, wpb=672, bsz=32, num_updates=6001, lr=0.25, gnorm=0.428, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:   7000 / 27737 loss=1.023, ppl=2.03, wps=8853, ups=13.1, wpb=672, bsz=32, num_updates=7001, lr=0.25, gnorm=0.406, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:   8000 / 27737 loss=0.973, ppl=1.96, wps=8826, ups=13.1, wpb=672, bsz=32, num_updates=8001, lr=0.25, gnorm=0.387, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:   9000 / 27737 loss=0.935, ppl=1.91, wps=8810, ups=13.1, wpb=672, bsz=32, num_updates=9001, lr=0.25, gnorm=0.373, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:  10000 / 27737 loss=0.899, ppl=1.86, wps=8812, ups=13.1, wpb=672, bsz=32, num_updates=10001, lr=0.25, gnorm=0.359, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:  11000 / 27737 loss=0.873, ppl=1.83, wps=8833, ups=13.1, wpb=672, bsz=32, num_updates=11001, lr=0.25, gnorm=0.348, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:  12000 / 27737 loss=0.842, ppl=1.79, wps=8866, ups=13.2, wpb=672, bsz=32, num_updates=12001, lr=0.25, gnorm=0.337, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:  13000 / 27737 loss=0.825, ppl=1.77, wps=8863, ups=13.2, wpb=672, bsz=32, num_updates=13001, lr=0.25, gnorm=0.329, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:  14000 / 27737 loss=0.813, ppl=1.76, wps=8857, ups=13.2, wpb=672, bsz=32, num_updates=14001, lr=0.25, gnorm=0.321, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:  15000 / 27737 loss=0.802, ppl=1.74, wps=8866, ups=13.2, wpb=672, bsz=32, num_updates=15001, lr=0.25, gnorm=0.315, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:  16000 / 27737 loss=0.787, ppl=1.73, wps=8864, ups=13.2, wpb=672, bsz=32, num_updates=16001, lr=0.25, gnorm=0.309, clip=100%, oom=0, sample_size=672\n",
      "| epoch 001:  17000 / 27737 loss=0.773, ppl=1.71, wps=8859, ups=13.2, wpb=672, bsz=32, num_updates=17001, lr=0.25, gnorm=0.302, clip=100%, oom=0, sample_size=671.996\n",
      "| epoch 001:  18000 / 27737 loss=0.763, ppl=1.70, wps=8863, ups=13.2, wpb=672, bsz=32, num_updates=18001, lr=0.25, gnorm=0.297, clip=100%, oom=0, sample_size=671.997\n",
      "| epoch 001:  19000 / 27737 loss=0.748, ppl=1.68, wps=8869, ups=13.2, wpb=672, bsz=32, num_updates=19001, lr=0.25, gnorm=0.292, clip=100%, oom=0, sample_size=671.997\n",
      "| epoch 001:  20000 / 27737 loss=0.736, ppl=1.67, wps=8866, ups=13.2, wpb=672, bsz=32, num_updates=20001, lr=0.25, gnorm=0.287, clip=100%, oom=0, sample_size=671.997\n",
      "| epoch 001:  21000 / 27737 loss=0.729, ppl=1.66, wps=8870, ups=13.2, wpb=672, bsz=32, num_updates=21001, lr=0.25, gnorm=0.282, clip=100%, oom=0, sample_size=671.997\n",
      "| epoch 001:  22000 / 27737 loss=0.723, ppl=1.65, wps=8866, ups=13.2, wpb=672, bsz=32, num_updates=22001, lr=0.25, gnorm=0.278, clip=99%, oom=0, sample_size=671.997\n",
      "| epoch 001:  23000 / 27737 loss=0.711, ppl=1.64, wps=8860, ups=13.2, wpb=672, bsz=32, num_updates=23001, lr=0.25, gnorm=0.273, clip=99%, oom=0, sample_size=671.997\n",
      "| epoch 001:  24000 / 27737 loss=0.705, ppl=1.63, wps=8859, ups=13.2, wpb=672, bsz=32, num_updates=24001, lr=0.25, gnorm=0.270, clip=99%, oom=0, sample_size=671.997\n",
      "| epoch 001:  25000 / 27737 loss=0.697, ppl=1.62, wps=8854, ups=13.2, wpb=672, bsz=32, num_updates=25001, lr=0.25, gnorm=0.266, clip=99%, oom=0, sample_size=671.997\n",
      "| epoch 001:  26000 / 27737 loss=0.689, ppl=1.61, wps=8850, ups=13.2, wpb=672, bsz=32, num_updates=26001, lr=0.25, gnorm=0.262, clip=99%, oom=0, sample_size=671.998\n",
      "| epoch 001:  27000 / 27737 loss=0.681, ppl=1.60, wps=8850, ups=13.2, wpb=672, bsz=32, num_updates=27001, lr=0.25, gnorm=0.259, clip=99%, oom=0, sample_size=671.998\n",
      "| epoch 001 | loss 0.676 | ppl 1.60 | wps 8850 | ups 13.2 | wpb 672 | bsz 32 | num_updates 27737 | lr 0.25 | gnorm 0.256 | clip 99% | oom 0 | sample_size 671.998\n",
      "| epoch 001 | valid on 'valid' subset:   1000 / 6947 valid_loss=0.228256, valid_ppl=1.17, sample_size=669.273\n",
      "| epoch 001 | valid on 'valid' subset:   2000 / 6947 valid_loss=0.222869, valid_ppl=1.17, sample_size=670.636\n",
      "| epoch 001 | valid on 'valid' subset:   3000 / 6947 valid_loss=0.216008, valid_ppl=1.16, sample_size=671.09\n",
      "| epoch 001 | valid on 'valid' subset:   4000 / 6947 valid_loss=0.212895, valid_ppl=1.16, sample_size=671.318\n",
      "| epoch 001 | valid on 'valid' subset:   5000 / 6947 valid_loss=0.211471, valid_ppl=1.16, sample_size=671.454\n",
      "| epoch 001 | valid on 'valid' subset:   6000 / 6947 valid_loss=0.212894, valid_ppl=1.16, sample_size=671.545\n",
      "| epoch 001 | valid on 'valid' subset | valid_loss 0.493169 | valid_ppl 1.41 | sample_size 670.872\n",
      "| epoch 002:   1000 / 27737 loss=0.512, ppl=1.43, wps=8803, ups=13.1, wpb=672, bsz=32, num_updates=28738, lr=0.25, gnorm=0.253, clip=93%, oom=0, sample_size=672\n",
      "| epoch 002:   2000 / 27737 loss=0.495, ppl=1.41, wps=8815, ups=13.1, wpb=672, bsz=32, num_updates=29738, lr=0.25, gnorm=0.250, clip=92%, oom=0, sample_size=672\n",
      "| epoch 002:   3000 / 27737 loss=0.487, ppl=1.40, wps=8804, ups=13.1, wpb=672, bsz=32, num_updates=30738, lr=0.25, gnorm=0.247, clip=92%, oom=0, sample_size=672\n",
      "| epoch 002:   4000 / 27737 loss=0.492, ppl=1.41, wps=8786, ups=13.1, wpb=672, bsz=32, num_updates=31738, lr=0.25, gnorm=0.244, clip=91%, oom=0, sample_size=672\n",
      "| epoch 002:   5000 / 27737 loss=0.480, ppl=1.40, wps=8787, ups=13.1, wpb=672, bsz=32, num_updates=32738, lr=0.25, gnorm=0.241, clip=91%, oom=0, sample_size=672\n",
      "| epoch 002:   6000 / 27737 loss=0.476, ppl=1.39, wps=8806, ups=13.1, wpb=672, bsz=32, num_updates=33738, lr=0.25, gnorm=0.239, clip=90%, oom=0, sample_size=672\n",
      "| epoch 002:   7000 / 27737 loss=0.478, ppl=1.39, wps=8801, ups=13.1, wpb=672, bsz=32, num_updates=34738, lr=0.25, gnorm=0.236, clip=90%, oom=0, sample_size=672\n",
      "| epoch 002:   8000 / 27737 loss=0.480, ppl=1.40, wps=8803, ups=13.1, wpb=672, bsz=32, num_updates=35738, lr=0.25, gnorm=0.234, clip=89%, oom=0, sample_size=672\n",
      "| epoch 002:   9000 / 27737 loss=0.476, ppl=1.39, wps=8797, ups=13.1, wpb=672, bsz=32, num_updates=36738, lr=0.25, gnorm=0.232, clip=89%, oom=0, sample_size=672\n",
      "| epoch 002:  10000 / 27737 loss=0.479, ppl=1.39, wps=8797, ups=13.1, wpb=672, bsz=32, num_updates=37738, lr=0.25, gnorm=0.229, clip=88%, oom=0, sample_size=672\n",
      "| epoch 002:  11000 / 27737 loss=0.475, ppl=1.39, wps=8795, ups=13.1, wpb=672, bsz=32, num_updates=38738, lr=0.25, gnorm=0.227, clip=88%, oom=0, sample_size=672\n",
      "| epoch 002:  12000 / 27737 loss=0.473, ppl=1.39, wps=8790, ups=13.1, wpb=672, bsz=32, num_updates=39738, lr=0.25, gnorm=0.225, clip=87%, oom=0, sample_size=672\n",
      "| epoch 002:  13000 / 27737 loss=0.472, ppl=1.39, wps=8791, ups=13.1, wpb=672, bsz=32, num_updates=40738, lr=0.25, gnorm=0.223, clip=86%, oom=0, sample_size=672\n",
      "| epoch 002:  14000 / 27737 loss=0.474, ppl=1.39, wps=8790, ups=13.1, wpb=672, bsz=32, num_updates=41738, lr=0.25, gnorm=0.221, clip=86%, oom=0, sample_size=672\n",
      "| epoch 002:  15000 / 27737 loss=0.469, ppl=1.38, wps=8789, ups=13.1, wpb=672, bsz=32, num_updates=42738, lr=0.25, gnorm=0.219, clip=85%, oom=0, sample_size=672\n",
      "| epoch 002:  16000 / 27737 loss=0.470, ppl=1.39, wps=8790, ups=13.1, wpb=672, bsz=32, num_updates=43738, lr=0.25, gnorm=0.217, clip=84%, oom=0, sample_size=672\n",
      "| epoch 002:  17000 / 27737 loss=0.470, ppl=1.39, wps=8797, ups=13.1, wpb=672, bsz=32, num_updates=44738, lr=0.25, gnorm=0.215, clip=84%, oom=0, sample_size=672\n",
      "| epoch 002:  18000 / 27737 loss=0.470, ppl=1.39, wps=8795, ups=13.1, wpb=672, bsz=32, num_updates=45738, lr=0.25, gnorm=0.213, clip=83%, oom=0, sample_size=672\n",
      "| epoch 002:  19000 / 27737 loss=0.468, ppl=1.38, wps=8796, ups=13.1, wpb=672, bsz=32, num_updates=46738, lr=0.25, gnorm=0.212, clip=82%, oom=0, sample_size=672\n",
      "| epoch 002:  20000 / 27737 loss=0.466, ppl=1.38, wps=8798, ups=13.1, wpb=672, bsz=32, num_updates=47738, lr=0.25, gnorm=0.210, clip=82%, oom=0, sample_size=671.997\n",
      "| epoch 002:  21000 / 27737 loss=0.465, ppl=1.38, wps=8796, ups=13.1, wpb=672, bsz=32, num_updates=48738, lr=0.25, gnorm=0.208, clip=81%, oom=0, sample_size=671.997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 002:  22000 / 27737 loss=0.465, ppl=1.38, wps=8795, ups=13.1, wpb=672, bsz=32, num_updates=49738, lr=0.25, gnorm=0.207, clip=80%, oom=0, sample_size=671.997\n",
      "| epoch 002:  23000 / 27737 loss=0.462, ppl=1.38, wps=8793, ups=13.1, wpb=672, bsz=32, num_updates=50738, lr=0.25, gnorm=0.205, clip=79%, oom=0, sample_size=671.997\n",
      "| epoch 002:  24000 / 27737 loss=0.460, ppl=1.38, wps=8792, ups=13.1, wpb=672, bsz=32, num_updates=51738, lr=0.25, gnorm=0.204, clip=79%, oom=0, sample_size=671.997\n",
      "| epoch 002:  25000 / 27737 loss=0.458, ppl=1.37, wps=8800, ups=13.1, wpb=672, bsz=32, num_updates=52738, lr=0.25, gnorm=0.202, clip=78%, oom=0, sample_size=671.997\n",
      "| epoch 002:  26000 / 27737 loss=0.457, ppl=1.37, wps=8801, ups=13.1, wpb=672, bsz=32, num_updates=53738, lr=0.25, gnorm=0.201, clip=77%, oom=0, sample_size=671.998\n",
      "| epoch 002:  27000 / 27737 loss=0.455, ppl=1.37, wps=8803, ups=13.1, wpb=672, bsz=32, num_updates=54738, lr=0.25, gnorm=0.199, clip=77%, oom=0, sample_size=671.998\n",
      "| epoch 002 | loss 0.454 | ppl 1.37 | wps 8806 | ups 13.1 | wpb 672 | bsz 32 | num_updates 55474 | lr 0.25 | gnorm 0.198 | clip 76% | oom 0 | sample_size 671.998\n",
      "| epoch 002 | valid on 'valid' subset:   1000 / 6947 valid_loss=0.179248, valid_ppl=1.13, sample_size=669.273\n",
      "| epoch 002 | valid on 'valid' subset:   2000 / 6947 valid_loss=0.178787, valid_ppl=1.13, sample_size=670.636\n",
      "| epoch 002 | valid on 'valid' subset:   3000 / 6947 valid_loss=0.173198, valid_ppl=1.13, sample_size=671.09\n",
      "| epoch 002 | valid on 'valid' subset:   4000 / 6947 valid_loss=0.171099, valid_ppl=1.13, sample_size=671.318\n",
      "| epoch 002 | valid on 'valid' subset:   5000 / 6947 valid_loss=0.169727, valid_ppl=1.12, sample_size=671.454\n",
      "| epoch 002 | valid on 'valid' subset:   6000 / 6947 valid_loss=0.171058, valid_ppl=1.13, sample_size=671.545\n",
      "| epoch 002 | valid on 'valid' subset | valid_loss 0.428993 | valid_ppl 1.35 | sample_size 670.872\n",
      "| epoch 003:   1000 / 27737 loss=0.444, ppl=1.36, wps=8923, ups=13.3, wpb=672, bsz=32, num_updates=56475, lr=0.25, gnorm=0.197, clip=59%, oom=0, sample_size=671.937\n",
      "| epoch 003:   2000 / 27737 loss=0.435, ppl=1.35, wps=8934, ups=13.3, wpb=672, bsz=32, num_updates=57475, lr=0.25, gnorm=0.196, clip=57%, oom=0, sample_size=671.969\n",
      "| epoch 003:   3000 / 27737 loss=0.443, ppl=1.36, wps=8922, ups=13.3, wpb=672, bsz=32, num_updates=58475, lr=0.25, gnorm=0.195, clip=57%, oom=0, sample_size=671.979\n",
      "| epoch 003:   4000 / 27737 loss=0.428, ppl=1.35, wps=8920, ups=13.3, wpb=672, bsz=32, num_updates=59475, lr=0.25, gnorm=0.193, clip=57%, oom=0, sample_size=671.984\n",
      "| epoch 003:   5000 / 27737 loss=0.413, ppl=1.33, wps=8901, ups=13.2, wpb=672, bsz=32, num_updates=60475, lr=0.25, gnorm=0.192, clip=58%, oom=0, sample_size=671.987\n",
      "| epoch 003:   6000 / 27737 loss=0.412, ppl=1.33, wps=8882, ups=13.2, wpb=672, bsz=32, num_updates=61475, lr=0.25, gnorm=0.191, clip=57%, oom=0, sample_size=671.99\n",
      "| epoch 003:   7000 / 27737 loss=0.404, ppl=1.32, wps=8892, ups=13.2, wpb=672, bsz=32, num_updates=62475, lr=0.25, gnorm=0.190, clip=57%, oom=0, sample_size=671.991\n",
      "| epoch 003:   8000 / 27737 loss=0.411, ppl=1.33, wps=8883, ups=13.2, wpb=672, bsz=32, num_updates=63475, lr=0.25, gnorm=0.189, clip=57%, oom=0, sample_size=671.992\n",
      "| epoch 003:   9000 / 27737 loss=0.404, ppl=1.32, wps=8886, ups=13.2, wpb=672, bsz=32, num_updates=64475, lr=0.25, gnorm=0.188, clip=57%, oom=0, sample_size=671.993\n",
      "| epoch 003:  10000 / 27737 loss=0.397, ppl=1.32, wps=8909, ups=13.3, wpb=672, bsz=32, num_updates=65475, lr=0.25, gnorm=0.187, clip=57%, oom=0, sample_size=671.994\n",
      "| epoch 003:  11000 / 27737 loss=0.396, ppl=1.32, wps=8950, ups=13.3, wpb=672, bsz=32, num_updates=66475, lr=0.25, gnorm=0.186, clip=56%, oom=0, sample_size=671.994\n",
      "| epoch 003:  12000 / 27737 loss=0.395, ppl=1.31, wps=8967, ups=13.3, wpb=672, bsz=32, num_updates=67475, lr=0.25, gnorm=0.185, clip=56%, oom=0, sample_size=671.995\n",
      "| epoch 003:  13000 / 27737 loss=0.398, ppl=1.32, wps=8965, ups=13.3, wpb=672, bsz=32, num_updates=68475, lr=0.25, gnorm=0.184, clip=56%, oom=0, sample_size=671.995\n",
      "| epoch 003:  14000 / 27737 loss=0.399, ppl=1.32, wps=8963, ups=13.3, wpb=672, bsz=32, num_updates=69475, lr=0.25, gnorm=0.183, clip=56%, oom=0, sample_size=671.996\n",
      "| epoch 003:  15000 / 27737 loss=0.400, ppl=1.32, wps=8964, ups=13.3, wpb=672, bsz=32, num_updates=70475, lr=0.25, gnorm=0.182, clip=55%, oom=0, sample_size=671.996\n",
      "| epoch 003:  16000 / 27737 loss=0.397, ppl=1.32, wps=8963, ups=13.3, wpb=672, bsz=32, num_updates=71475, lr=0.25, gnorm=0.181, clip=55%, oom=0, sample_size=671.996\n",
      "| epoch 003:  17000 / 27737 loss=0.397, ppl=1.32, wps=8964, ups=13.3, wpb=672, bsz=32, num_updates=72475, lr=0.25, gnorm=0.180, clip=55%, oom=0, sample_size=671.996\n",
      "| epoch 003:  18000 / 27737 loss=0.396, ppl=1.32, wps=8969, ups=13.3, wpb=672, bsz=32, num_updates=73475, lr=0.25, gnorm=0.179, clip=54%, oom=0, sample_size=671.997\n",
      "| epoch 003:  19000 / 27737 loss=0.396, ppl=1.32, wps=8968, ups=13.3, wpb=672, bsz=32, num_updates=74475, lr=0.25, gnorm=0.179, clip=54%, oom=0, sample_size=671.997\n",
      "| epoch 003:  20000 / 27737 loss=0.398, ppl=1.32, wps=8968, ups=13.3, wpb=672, bsz=32, num_updates=75475, lr=0.25, gnorm=0.178, clip=53%, oom=0, sample_size=671.997\n",
      "| epoch 003:  21000 / 27737 loss=0.401, ppl=1.32, wps=8969, ups=13.3, wpb=672, bsz=32, num_updates=76475, lr=0.25, gnorm=0.177, clip=53%, oom=0, sample_size=671.997\n",
      "| epoch 003:  22000 / 27737 loss=0.401, ppl=1.32, wps=8968, ups=13.3, wpb=672, bsz=32, num_updates=77475, lr=0.25, gnorm=0.176, clip=52%, oom=0, sample_size=671.997\n",
      "| epoch 003:  23000 / 27737 loss=0.401, ppl=1.32, wps=8971, ups=13.3, wpb=672, bsz=32, num_updates=78475, lr=0.25, gnorm=0.175, clip=52%, oom=0, sample_size=671.997\n",
      "| epoch 003:  24000 / 27737 loss=0.399, ppl=1.32, wps=8976, ups=13.4, wpb=672, bsz=32, num_updates=79475, lr=0.25, gnorm=0.175, clip=52%, oom=0, sample_size=671.997\n",
      "| epoch 003:  25000 / 27737 loss=0.398, ppl=1.32, wps=8975, ups=13.4, wpb=672, bsz=32, num_updates=80475, lr=0.25, gnorm=0.174, clip=51%, oom=0, sample_size=671.997\n",
      "| epoch 003:  26000 / 27737 loss=0.399, ppl=1.32, wps=8974, ups=13.4, wpb=672, bsz=32, num_updates=81475, lr=0.25, gnorm=0.173, clip=51%, oom=0, sample_size=671.998\n",
      "| epoch 003:  27000 / 27737 loss=0.396, ppl=1.32, wps=8976, ups=13.4, wpb=672, bsz=32, num_updates=82475, lr=0.25, gnorm=0.172, clip=51%, oom=0, sample_size=671.998\n",
      "| epoch 003 | loss 0.394 | ppl 1.31 | wps 8977 | ups 13.4 | wpb 672 | bsz 32 | num_updates 83211 | lr 0.25 | gnorm 0.172 | clip 50% | oom 0 | sample_size 671.998\n",
      "| epoch 003 | valid on 'valid' subset:   1000 / 6947 valid_loss=0.154586, valid_ppl=1.11, sample_size=669.273\n",
      "| epoch 003 | valid on 'valid' subset:   2000 / 6947 valid_loss=0.156351, valid_ppl=1.11, sample_size=670.636\n",
      "| epoch 003 | valid on 'valid' subset:   3000 / 6947 valid_loss=0.151495, valid_ppl=1.11, sample_size=671.09\n",
      "| epoch 003 | valid on 'valid' subset:   4000 / 6947 valid_loss=0.150034, valid_ppl=1.11, sample_size=671.318\n",
      "| epoch 003 | valid on 'valid' subset:   5000 / 6947 valid_loss=0.148677, valid_ppl=1.11, sample_size=671.454\n",
      "| epoch 003 | valid on 'valid' subset:   6000 / 6947 valid_loss=0.150093, valid_ppl=1.11, sample_size=671.545\n",
      "| epoch 003 | valid on 'valid' subset | valid_loss 0.391179 | valid_ppl 1.31 | sample_size 670.872\n",
      "| epoch 004:   1000 / 27737 loss=0.353, ppl=1.28, wps=8863, ups=13.2, wpb=672, bsz=32, num_updates=84212, lr=0.25, gnorm=0.171, clip=42%, oom=0, sample_size=672\n",
      "| epoch 004:   2000 / 27737 loss=0.367, ppl=1.29, wps=8922, ups=13.3, wpb=672, bsz=32, num_updates=85212, lr=0.25, gnorm=0.170, clip=42%, oom=0, sample_size=672\n",
      "| epoch 004:   3000 / 27737 loss=0.375, ppl=1.30, wps=8922, ups=13.3, wpb=672, bsz=32, num_updates=86212, lr=0.25, gnorm=0.170, clip=44%, oom=0, sample_size=672\n",
      "| epoch 004:   4000 / 27737 loss=0.361, ppl=1.28, wps=8923, ups=13.3, wpb=672, bsz=32, num_updates=87212, lr=0.25, gnorm=0.169, clip=44%, oom=0, sample_size=672\n",
      "| epoch 004:   5000 / 27737 loss=0.363, ppl=1.29, wps=8931, ups=13.3, wpb=672, bsz=32, num_updates=88212, lr=0.25, gnorm=0.168, clip=45%, oom=0, sample_size=672\n",
      "| epoch 004:   6000 / 27737 loss=0.367, ppl=1.29, wps=8923, ups=13.3, wpb=672, bsz=32, num_updates=89212, lr=0.25, gnorm=0.168, clip=44%, oom=0, sample_size=672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 004:   7000 / 27737 loss=0.367, ppl=1.29, wps=8905, ups=13.3, wpb=672, bsz=32, num_updates=90212, lr=0.25, gnorm=0.167, clip=45%, oom=0, sample_size=672\n",
      "| epoch 004:   8000 / 27737 loss=0.362, ppl=1.28, wps=8905, ups=13.3, wpb=672, bsz=32, num_updates=91212, lr=0.25, gnorm=0.167, clip=45%, oom=0, sample_size=672\n",
      "| epoch 004:   9000 / 27737 loss=0.364, ppl=1.29, wps=8900, ups=13.2, wpb=672, bsz=32, num_updates=92212, lr=0.25, gnorm=0.166, clip=45%, oom=0, sample_size=672\n",
      "| epoch 004:  10000 / 27737 loss=0.359, ppl=1.28, wps=8896, ups=13.2, wpb=672, bsz=32, num_updates=93212, lr=0.25, gnorm=0.166, clip=45%, oom=0, sample_size=672\n",
      "| epoch 004:  11000 / 27737 loss=0.353, ppl=1.28, wps=8895, ups=13.2, wpb=672, bsz=32, num_updates=94212, lr=0.25, gnorm=0.165, clip=45%, oom=0, sample_size=672\n",
      "| epoch 004:  12000 / 27737 loss=0.355, ppl=1.28, wps=8887, ups=13.2, wpb=672, bsz=32, num_updates=95212, lr=0.25, gnorm=0.164, clip=45%, oom=0, sample_size=672\n",
      "| epoch 004:  13000 / 27737 loss=0.353, ppl=1.28, wps=8891, ups=13.2, wpb=672, bsz=32, num_updates=96212, lr=0.25, gnorm=0.164, clip=44%, oom=0, sample_size=672\n",
      "| epoch 004:  14000 / 27737 loss=0.358, ppl=1.28, wps=8889, ups=13.2, wpb=672, bsz=32, num_updates=97212, lr=0.25, gnorm=0.163, clip=44%, oom=0, sample_size=672\n",
      "| epoch 004:  15000 / 27737 loss=0.355, ppl=1.28, wps=8892, ups=13.2, wpb=672, bsz=32, num_updates=98212, lr=0.25, gnorm=0.163, clip=44%, oom=0, sample_size=672\n",
      "| epoch 004:  16000 / 27737 loss=0.358, ppl=1.28, wps=8888, ups=13.2, wpb=672, bsz=32, num_updates=99212, lr=0.25, gnorm=0.162, clip=44%, oom=0, sample_size=672\n",
      "| epoch 004:  17000 / 27737 loss=0.361, ppl=1.28, wps=8892, ups=13.2, wpb=672, bsz=32, num_updates=100212, lr=0.25, gnorm=0.162, clip=44%, oom=0, sample_size=671.996\n",
      "| epoch 004:  18000 / 27737 loss=0.358, ppl=1.28, wps=8886, ups=13.2, wpb=672, bsz=32, num_updates=101212, lr=0.25, gnorm=0.161, clip=44%, oom=0, sample_size=671.997\n",
      "| epoch 004:  19000 / 27737 loss=0.357, ppl=1.28, wps=8888, ups=13.2, wpb=672, bsz=32, num_updates=102212, lr=0.25, gnorm=0.161, clip=43%, oom=0, sample_size=671.997\n",
      "| epoch 004:  20000 / 27737 loss=0.357, ppl=1.28, wps=8891, ups=13.2, wpb=672, bsz=32, num_updates=103212, lr=0.25, gnorm=0.160, clip=43%, oom=0, sample_size=671.997\n",
      "| epoch 004:  21000 / 27737 loss=0.358, ppl=1.28, wps=8890, ups=13.2, wpb=672, bsz=32, num_updates=104212, lr=0.25, gnorm=0.160, clip=43%, oom=0, sample_size=671.997\n",
      "| epoch 004:  22000 / 27737 loss=0.357, ppl=1.28, wps=8891, ups=13.2, wpb=672, bsz=32, num_updates=105212, lr=0.25, gnorm=0.159, clip=43%, oom=0, sample_size=671.997\n",
      "| epoch 004:  23000 / 27737 loss=0.357, ppl=1.28, wps=8895, ups=13.2, wpb=672, bsz=32, num_updates=106212, lr=0.25, gnorm=0.159, clip=43%, oom=0, sample_size=671.997\n",
      "| epoch 004:  24000 / 27737 loss=0.360, ppl=1.28, wps=8887, ups=13.2, wpb=672, bsz=32, num_updates=107212, lr=0.25, gnorm=0.158, clip=42%, oom=0, sample_size=671.997\n",
      "| epoch 004:  25000 / 27737 loss=0.357, ppl=1.28, wps=8889, ups=13.2, wpb=672, bsz=32, num_updates=108212, lr=0.25, gnorm=0.158, clip=42%, oom=0, sample_size=671.997\n",
      "| epoch 004:  26000 / 27737 loss=0.359, ppl=1.28, wps=8893, ups=13.2, wpb=672, bsz=32, num_updates=109212, lr=0.25, gnorm=0.157, clip=42%, oom=0, sample_size=671.998\n",
      "| epoch 004:  27000 / 27737 loss=0.358, ppl=1.28, wps=8908, ups=13.3, wpb=672, bsz=32, num_updates=110212, lr=0.25, gnorm=0.157, clip=42%, oom=0, sample_size=671.998\n",
      "| epoch 004 | loss 0.359 | ppl 1.28 | wps 8917 | ups 13.3 | wpb 672 | bsz 32 | num_updates 110948 | lr 0.25 | gnorm 0.157 | clip 41% | oom 0 | sample_size 671.998\n",
      "| epoch 004 | valid on 'valid' subset:   1000 / 6947 valid_loss=0.14679, valid_ppl=1.11, sample_size=669.273\n",
      "| epoch 004 | valid on 'valid' subset:   2000 / 6947 valid_loss=0.147342, valid_ppl=1.11, sample_size=670.636\n",
      "| epoch 004 | valid on 'valid' subset:   3000 / 6947 valid_loss=0.142514, valid_ppl=1.10, sample_size=671.09\n",
      "| epoch 004 | valid on 'valid' subset:   4000 / 6947 valid_loss=0.140872, valid_ppl=1.10, sample_size=671.318\n",
      "| epoch 004 | valid on 'valid' subset:   5000 / 6947 valid_loss=0.139433, valid_ppl=1.10, sample_size=671.454\n",
      "| epoch 004 | valid on 'valid' subset:   6000 / 6947 valid_loss=0.140678, valid_ppl=1.10, sample_size=671.545\n",
      "| epoch 004 | valid on 'valid' subset | valid_loss 0.373135 | valid_ppl 1.30 | sample_size 670.872\n",
      "| epoch 005:   1000 / 27737 loss=0.314, ppl=1.24, wps=9256, ups=13.8, wpb=672, bsz=32, num_updates=111949, lr=0.25, gnorm=0.156, clip=36%, oom=0, sample_size=672\n",
      "| epoch 005:   2000 / 27737 loss=0.327, ppl=1.25, wps=9109, ups=13.6, wpb=672, bsz=32, num_updates=112949, lr=0.25, gnorm=0.156, clip=37%, oom=0, sample_size=672\n",
      "| epoch 005:   3000 / 27737 loss=0.315, ppl=1.24, wps=9004, ups=13.4, wpb=672, bsz=32, num_updates=113949, lr=0.25, gnorm=0.156, clip=38%, oom=0, sample_size=672\n",
      "| epoch 005:   4000 / 27737 loss=0.301, ppl=1.23, wps=8951, ups=13.3, wpb=672, bsz=32, num_updates=114949, lr=0.25, gnorm=0.155, clip=39%, oom=0, sample_size=672\n",
      "| epoch 005:   5000 / 27737 loss=0.323, ppl=1.25, wps=8937, ups=13.3, wpb=672, bsz=32, num_updates=115949, lr=0.25, gnorm=0.155, clip=40%, oom=0, sample_size=672\n",
      "| epoch 005:   6000 / 27737 loss=0.325, ppl=1.25, wps=8977, ups=13.4, wpb=672, bsz=32, num_updates=116949, lr=0.25, gnorm=0.154, clip=41%, oom=0, sample_size=672\n",
      "| epoch 005:   7000 / 27737 loss=0.323, ppl=1.25, wps=9033, ups=13.4, wpb=672, bsz=32, num_updates=117949, lr=0.25, gnorm=0.154, clip=41%, oom=0, sample_size=672\n",
      "| epoch 005:   8000 / 27737 loss=0.325, ppl=1.25, wps=9014, ups=13.4, wpb=672, bsz=32, num_updates=118949, lr=0.25, gnorm=0.154, clip=41%, oom=0, sample_size=672\n",
      "| epoch 005:   9000 / 27737 loss=0.324, ppl=1.25, wps=9010, ups=13.4, wpb=672, bsz=32, num_updates=119949, lr=0.25, gnorm=0.153, clip=41%, oom=0, sample_size=672\n",
      "| epoch 005:  10000 / 27737 loss=0.327, ppl=1.25, wps=9002, ups=13.4, wpb=672, bsz=32, num_updates=120949, lr=0.25, gnorm=0.153, clip=41%, oom=0, sample_size=672\n",
      "| epoch 005:  11000 / 27737 loss=0.327, ppl=1.25, wps=8995, ups=13.4, wpb=672, bsz=32, num_updates=121949, lr=0.25, gnorm=0.153, clip=41%, oom=0, sample_size=671.994\n",
      "| epoch 005:  12000 / 27737 loss=0.327, ppl=1.25, wps=8986, ups=13.4, wpb=672, bsz=32, num_updates=122949, lr=0.25, gnorm=0.152, clip=41%, oom=0, sample_size=671.995\n",
      "| epoch 005:  13000 / 27737 loss=0.325, ppl=1.25, wps=8977, ups=13.4, wpb=672, bsz=32, num_updates=123949, lr=0.25, gnorm=0.152, clip=41%, oom=0, sample_size=671.995\n",
      "| epoch 005:  14000 / 27737 loss=0.333, ppl=1.26, wps=8972, ups=13.4, wpb=672, bsz=32, num_updates=124949, lr=0.25, gnorm=0.152, clip=40%, oom=0, sample_size=671.996\n",
      "| epoch 005:  15000 / 27737 loss=0.331, ppl=1.26, wps=8963, ups=13.3, wpb=672, bsz=32, num_updates=125949, lr=0.25, gnorm=0.151, clip=40%, oom=0, sample_size=671.996\n",
      "| epoch 005:  16000 / 27737 loss=0.331, ppl=1.26, wps=8956, ups=13.3, wpb=672, bsz=32, num_updates=126949, lr=0.25, gnorm=0.151, clip=40%, oom=0, sample_size=671.996\n",
      "| epoch 005:  17000 / 27737 loss=0.330, ppl=1.26, wps=8956, ups=13.3, wpb=672, bsz=32, num_updates=127949, lr=0.25, gnorm=0.151, clip=40%, oom=0, sample_size=671.996\n",
      "| epoch 005:  18000 / 27737 loss=0.331, ppl=1.26, wps=8948, ups=13.3, wpb=672, bsz=32, num_updates=128949, lr=0.25, gnorm=0.150, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 005:  19000 / 27737 loss=0.333, ppl=1.26, wps=8949, ups=13.3, wpb=672, bsz=32, num_updates=129949, lr=0.25, gnorm=0.150, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 005:  20000 / 27737 loss=0.337, ppl=1.26, wps=8945, ups=13.3, wpb=672, bsz=32, num_updates=130949, lr=0.25, gnorm=0.150, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 005:  21000 / 27737 loss=0.335, ppl=1.26, wps=8944, ups=13.3, wpb=672, bsz=32, num_updates=131949, lr=0.25, gnorm=0.149, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 005:  22000 / 27737 loss=0.337, ppl=1.26, wps=8950, ups=13.3, wpb=672, bsz=32, num_updates=132949, lr=0.25, gnorm=0.149, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 005:  23000 / 27737 loss=0.336, ppl=1.26, wps=8950, ups=13.3, wpb=672, bsz=32, num_updates=133949, lr=0.25, gnorm=0.149, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 005:  24000 / 27737 loss=0.335, ppl=1.26, wps=8948, ups=13.3, wpb=672, bsz=32, num_updates=134949, lr=0.25, gnorm=0.148, clip=39%, oom=0, sample_size=671.997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 005:  25000 / 27737 loss=0.337, ppl=1.26, wps=8949, ups=13.3, wpb=672, bsz=32, num_updates=135949, lr=0.25, gnorm=0.148, clip=39%, oom=0, sample_size=671.997\n",
      "| epoch 005:  26000 / 27737 loss=0.334, ppl=1.26, wps=8954, ups=13.3, wpb=672, bsz=32, num_updates=136949, lr=0.25, gnorm=0.148, clip=39%, oom=0, sample_size=671.998\n",
      "| epoch 005:  27000 / 27737 loss=0.333, ppl=1.26, wps=8957, ups=13.3, wpb=672, bsz=32, num_updates=137949, lr=0.25, gnorm=0.148, clip=39%, oom=0, sample_size=671.998\n",
      "| epoch 005 | loss 0.333 | ppl 1.26 | wps 8960 | ups 13.3 | wpb 672 | bsz 32 | num_updates 138685 | lr 0.25 | gnorm 0.147 | clip 39% | oom 0 | sample_size 671.998\n",
      "| epoch 005 | valid on 'valid' subset:   1000 / 6947 valid_loss=0.141679, valid_ppl=1.10, sample_size=669.273\n",
      "| epoch 005 | valid on 'valid' subset:   2000 / 6947 valid_loss=0.143171, valid_ppl=1.10, sample_size=670.636\n",
      "| epoch 005 | valid on 'valid' subset:   3000 / 6947 valid_loss=0.138675, valid_ppl=1.10, sample_size=671.09\n",
      "| epoch 005 | valid on 'valid' subset:   4000 / 6947 valid_loss=0.137521, valid_ppl=1.10, sample_size=671.318\n",
      "| epoch 005 | valid on 'valid' subset:   5000 / 6947 valid_loss=0.136268, valid_ppl=1.10, sample_size=671.454\n",
      "| epoch 005 | valid on 'valid' subset:   6000 / 6947 valid_loss=0.137562, valid_ppl=1.10, sample_size=671.545\n",
      "| epoch 005 | valid on 'valid' subset | valid_loss 0.363908 | valid_ppl 1.29 | sample_size 670.872\n",
      "| epoch 006:   1000 / 27737 loss=0.264, ppl=1.20, wps=9010, ups=13.4, wpb=672, bsz=32, num_updates=139686, lr=0.25, gnorm=0.147, clip=34%, oom=0, sample_size=672\n",
      "| epoch 006:   2000 / 27737 loss=0.286, ppl=1.22, wps=8914, ups=13.3, wpb=672, bsz=32, num_updates=140686, lr=0.25, gnorm=0.147, clip=36%, oom=0, sample_size=672\n",
      "| epoch 006:   3000 / 27737 loss=0.305, ppl=1.24, wps=8904, ups=13.2, wpb=672, bsz=32, num_updates=141686, lr=0.25, gnorm=0.146, clip=37%, oom=0, sample_size=672\n",
      "| epoch 006:   4000 / 27737 loss=0.299, ppl=1.23, wps=8933, ups=13.3, wpb=672, bsz=32, num_updates=142686, lr=0.25, gnorm=0.146, clip=38%, oom=0, sample_size=672\n",
      "| epoch 006:   5000 / 27737 loss=0.308, ppl=1.24, wps=8934, ups=13.3, wpb=672, bsz=32, num_updates=143686, lr=0.25, gnorm=0.146, clip=39%, oom=0, sample_size=672\n",
      "| epoch 006:   6000 / 27737 loss=0.306, ppl=1.24, wps=8925, ups=13.3, wpb=672, bsz=32, num_updates=144686, lr=0.25, gnorm=0.146, clip=39%, oom=0, sample_size=672\n",
      "| epoch 006:   7000 / 27737 loss=0.314, ppl=1.24, wps=8954, ups=13.3, wpb=672, bsz=32, num_updates=145686, lr=0.25, gnorm=0.146, clip=39%, oom=0, sample_size=672\n",
      "| epoch 006:   8000 / 27737 loss=0.317, ppl=1.25, wps=8978, ups=13.4, wpb=672, bsz=32, num_updates=146686, lr=0.25, gnorm=0.145, clip=39%, oom=0, sample_size=672\n",
      "| epoch 006:   9000 / 27737 loss=0.318, ppl=1.25, wps=8988, ups=13.4, wpb=672, bsz=32, num_updates=147686, lr=0.25, gnorm=0.145, clip=40%, oom=0, sample_size=672\n",
      "| epoch 006:  10000 / 27737 loss=0.317, ppl=1.25, wps=8972, ups=13.4, wpb=672, bsz=32, num_updates=148686, lr=0.25, gnorm=0.145, clip=40%, oom=0, sample_size=672\n",
      "| epoch 006:  11000 / 27737 loss=0.313, ppl=1.24, wps=8966, ups=13.3, wpb=672, bsz=32, num_updates=149686, lr=0.25, gnorm=0.145, clip=40%, oom=0, sample_size=672\n",
      "| epoch 006:  12000 / 27737 loss=0.312, ppl=1.24, wps=8952, ups=13.3, wpb=672, bsz=32, num_updates=150686, lr=0.25, gnorm=0.144, clip=40%, oom=0, sample_size=672\n",
      "| epoch 006:  13000 / 27737 loss=0.317, ppl=1.25, wps=8942, ups=13.3, wpb=672, bsz=32, num_updates=151686, lr=0.25, gnorm=0.144, clip=40%, oom=0, sample_size=672\n",
      "| epoch 006:  14000 / 27737 loss=0.315, ppl=1.24, wps=8927, ups=13.3, wpb=672, bsz=32, num_updates=152686, lr=0.25, gnorm=0.144, clip=40%, oom=0, sample_size=672\n",
      "| epoch 006:  15000 / 27737 loss=0.317, ppl=1.25, wps=8917, ups=13.3, wpb=672, bsz=32, num_updates=153686, lr=0.25, gnorm=0.144, clip=40%, oom=0, sample_size=671.996\n",
      "| epoch 006:  16000 / 27737 loss=0.315, ppl=1.24, wps=8916, ups=13.3, wpb=672, bsz=32, num_updates=154686, lr=0.25, gnorm=0.144, clip=40%, oom=0, sample_size=671.996\n",
      "| epoch 006:  17000 / 27737 loss=0.315, ppl=1.24, wps=8909, ups=13.3, wpb=672, bsz=32, num_updates=155686, lr=0.25, gnorm=0.143, clip=40%, oom=0, sample_size=671.996\n",
      "| epoch 006:  18000 / 27737 loss=0.314, ppl=1.24, wps=8898, ups=13.2, wpb=672, bsz=32, num_updates=156686, lr=0.25, gnorm=0.143, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 006:  19000 / 27737 loss=0.315, ppl=1.24, wps=8899, ups=13.2, wpb=672, bsz=32, num_updates=157686, lr=0.25, gnorm=0.143, clip=39%, oom=0, sample_size=671.997\n",
      "| epoch 006:  20000 / 27737 loss=0.317, ppl=1.25, wps=8899, ups=13.2, wpb=672, bsz=32, num_updates=158686, lr=0.25, gnorm=0.143, clip=39%, oom=0, sample_size=671.997\n",
      "| epoch 006:  21000 / 27737 loss=0.316, ppl=1.25, wps=8901, ups=13.2, wpb=672, bsz=32, num_updates=159686, lr=0.25, gnorm=0.142, clip=39%, oom=0, sample_size=671.997\n",
      "| epoch 006:  22000 / 27737 loss=0.316, ppl=1.25, wps=8900, ups=13.2, wpb=672, bsz=32, num_updates=160686, lr=0.25, gnorm=0.142, clip=39%, oom=0, sample_size=671.997\n",
      "| epoch 006:  23000 / 27737 loss=0.315, ppl=1.24, wps=8905, ups=13.3, wpb=672, bsz=32, num_updates=161686, lr=0.25, gnorm=0.142, clip=39%, oom=0, sample_size=671.997\n",
      "| epoch 006:  24000 / 27737 loss=0.314, ppl=1.24, wps=8903, ups=13.2, wpb=672, bsz=32, num_updates=162686, lr=0.25, gnorm=0.142, clip=39%, oom=0, sample_size=671.997\n",
      "| epoch 006:  25000 / 27737 loss=0.314, ppl=1.24, wps=8902, ups=13.2, wpb=672, bsz=32, num_updates=163686, lr=0.25, gnorm=0.142, clip=39%, oom=0, sample_size=671.997\n",
      "| epoch 006:  26000 / 27737 loss=0.314, ppl=1.24, wps=8905, ups=13.3, wpb=672, bsz=32, num_updates=164686, lr=0.25, gnorm=0.141, clip=39%, oom=0, sample_size=671.998\n",
      "| epoch 006:  27000 / 27737 loss=0.314, ppl=1.24, wps=8905, ups=13.3, wpb=672, bsz=32, num_updates=165686, lr=0.25, gnorm=0.141, clip=38%, oom=0, sample_size=671.998\n",
      "| epoch 006 | loss 0.314 | ppl 1.24 | wps 8905 | ups 13.3 | wpb 672 | bsz 32 | num_updates 166422 | lr 0.25 | gnorm 0.141 | clip 38% | oom 0 | sample_size 671.998\n",
      "| epoch 006 | valid on 'valid' subset:   1000 / 6947 valid_loss=0.140109, valid_ppl=1.10, sample_size=669.273\n",
      "| epoch 006 | valid on 'valid' subset:   2000 / 6947 valid_loss=0.141259, valid_ppl=1.10, sample_size=670.636\n",
      "| epoch 006 | valid on 'valid' subset:   3000 / 6947 valid_loss=0.136911, valid_ppl=1.10, sample_size=671.09\n",
      "| epoch 006 | valid on 'valid' subset:   4000 / 6947 valid_loss=0.13577, valid_ppl=1.10, sample_size=671.318\n",
      "| epoch 006 | valid on 'valid' subset:   5000 / 6947 valid_loss=0.134344, valid_ppl=1.10, sample_size=671.454\n",
      "| epoch 006 | valid on 'valid' subset:   6000 / 6947 valid_loss=0.135661, valid_ppl=1.10, sample_size=671.545\n",
      "| epoch 006 | valid on 'valid' subset | valid_loss 0.359395 | valid_ppl 1.28 | sample_size 670.872\n",
      "| epoch 007:   1000 / 27737 loss=0.289, ppl=1.22, wps=8967, ups=13.3, wpb=672, bsz=32, num_updates=167423, lr=0.25, gnorm=0.141, clip=34%, oom=0, sample_size=672\n",
      "| epoch 007:   2000 / 27737 loss=0.275, ppl=1.21, wps=8940, ups=13.3, wpb=672, bsz=32, num_updates=168423, lr=0.25, gnorm=0.141, clip=36%, oom=0, sample_size=672\n",
      "| epoch 007:   3000 / 27737 loss=0.280, ppl=1.21, wps=8919, ups=13.3, wpb=672, bsz=32, num_updates=169423, lr=0.25, gnorm=0.140, clip=37%, oom=0, sample_size=672\n",
      "| epoch 007:   4000 / 27737 loss=0.267, ppl=1.20, wps=8897, ups=13.2, wpb=672, bsz=32, num_updates=170423, lr=0.25, gnorm=0.140, clip=38%, oom=0, sample_size=672\n",
      "| epoch 007:   5000 / 27737 loss=0.279, ppl=1.21, wps=8935, ups=13.3, wpb=672, bsz=32, num_updates=171423, lr=0.25, gnorm=0.140, clip=38%, oom=0, sample_size=672\n",
      "| epoch 007:   6000 / 27737 loss=0.277, ppl=1.21, wps=9016, ups=13.4, wpb=672, bsz=32, num_updates=172423, lr=0.25, gnorm=0.140, clip=38%, oom=0, sample_size=672\n",
      "| epoch 007:   7000 / 27737 loss=0.286, ppl=1.22, wps=9059, ups=13.5, wpb=672, bsz=32, num_updates=173423, lr=0.25, gnorm=0.140, clip=39%, oom=0, sample_size=672\n",
      "| epoch 007:   8000 / 27737 loss=0.282, ppl=1.22, wps=9046, ups=13.5, wpb=672, bsz=32, num_updates=174423, lr=0.25, gnorm=0.139, clip=39%, oom=0, sample_size=672\n",
      "| epoch 007:   9000 / 27737 loss=0.284, ppl=1.22, wps=9032, ups=13.4, wpb=672, bsz=32, num_updates=175423, lr=0.25, gnorm=0.139, clip=39%, oom=0, sample_size=672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 007:  10000 / 27737 loss=0.283, ppl=1.22, wps=9007, ups=13.4, wpb=672, bsz=32, num_updates=176423, lr=0.25, gnorm=0.139, clip=39%, oom=0, sample_size=672\n",
      "| epoch 007:  11000 / 27737 loss=0.283, ppl=1.22, wps=8995, ups=13.4, wpb=672, bsz=32, num_updates=177423, lr=0.25, gnorm=0.139, clip=39%, oom=0, sample_size=672\n",
      "| epoch 007:  12000 / 27737 loss=0.290, ppl=1.22, wps=8976, ups=13.4, wpb=672, bsz=32, num_updates=178423, lr=0.25, gnorm=0.139, clip=40%, oom=0, sample_size=671.995\n",
      "| epoch 007:  13000 / 27737 loss=0.288, ppl=1.22, wps=8974, ups=13.4, wpb=672, bsz=32, num_updates=179423, lr=0.25, gnorm=0.139, clip=39%, oom=0, sample_size=671.995\n",
      "| epoch 007:  14000 / 27737 loss=0.291, ppl=1.22, wps=8965, ups=13.3, wpb=672, bsz=32, num_updates=180423, lr=0.25, gnorm=0.138, clip=39%, oom=0, sample_size=671.996\n",
      "| epoch 007:  15000 / 27737 loss=0.294, ppl=1.23, wps=8963, ups=13.3, wpb=672, bsz=32, num_updates=181423, lr=0.25, gnorm=0.138, clip=39%, oom=0, sample_size=671.996\n",
      "| epoch 007:  16000 / 27737 loss=0.297, ppl=1.23, wps=8967, ups=13.3, wpb=672, bsz=32, num_updates=182423, lr=0.25, gnorm=0.138, clip=39%, oom=0, sample_size=671.996\n",
      "| epoch 007:  17000 / 27737 loss=0.299, ppl=1.23, wps=8965, ups=13.3, wpb=672, bsz=32, num_updates=183423, lr=0.25, gnorm=0.138, clip=40%, oom=0, sample_size=671.996\n",
      "| epoch 007:  18000 / 27737 loss=0.299, ppl=1.23, wps=8962, ups=13.3, wpb=672, bsz=32, num_updates=184423, lr=0.25, gnorm=0.138, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 007:  19000 / 27737 loss=0.298, ppl=1.23, wps=8961, ups=13.3, wpb=672, bsz=32, num_updates=185423, lr=0.25, gnorm=0.138, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 007:  20000 / 27737 loss=0.297, ppl=1.23, wps=8961, ups=13.3, wpb=672, bsz=32, num_updates=186423, lr=0.25, gnorm=0.138, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 007:  21000 / 27737 loss=0.296, ppl=1.23, wps=8959, ups=13.3, wpb=672, bsz=32, num_updates=187423, lr=0.25, gnorm=0.137, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 007:  22000 / 27737 loss=0.295, ppl=1.23, wps=8959, ups=13.3, wpb=672, bsz=32, num_updates=188423, lr=0.25, gnorm=0.137, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 007:  23000 / 27737 loss=0.298, ppl=1.23, wps=8960, ups=13.3, wpb=672, bsz=32, num_updates=189423, lr=0.25, gnorm=0.137, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 007:  24000 / 27737 loss=0.297, ppl=1.23, wps=8963, ups=13.3, wpb=672, bsz=32, num_updates=190423, lr=0.25, gnorm=0.137, clip=39%, oom=0, sample_size=671.997\n",
      "| epoch 007:  25000 / 27737 loss=0.299, ppl=1.23, wps=8956, ups=13.3, wpb=672, bsz=32, num_updates=191423, lr=0.25, gnorm=0.137, clip=39%, oom=0, sample_size=671.997\n",
      "| epoch 007:  26000 / 27737 loss=0.297, ppl=1.23, wps=8956, ups=13.3, wpb=672, bsz=32, num_updates=192423, lr=0.25, gnorm=0.137, clip=39%, oom=0, sample_size=671.998\n",
      "| epoch 007:  27000 / 27737 loss=0.297, ppl=1.23, wps=8954, ups=13.3, wpb=672, bsz=32, num_updates=193423, lr=0.25, gnorm=0.136, clip=39%, oom=0, sample_size=671.998\n",
      "| epoch 007 | loss 0.298 | ppl 1.23 | wps 8953 | ups 13.3 | wpb 672 | bsz 32 | num_updates 194159 | lr 0.25 | gnorm 0.136 | clip 39% | oom 0 | sample_size 671.998\n",
      "| epoch 007 | valid on 'valid' subset:   1000 / 6947 valid_loss=0.137792, valid_ppl=1.10, sample_size=669.273\n",
      "| epoch 007 | valid on 'valid' subset:   2000 / 6947 valid_loss=0.139667, valid_ppl=1.10, sample_size=670.636\n",
      "| epoch 007 | valid on 'valid' subset:   3000 / 6947 valid_loss=0.134699, valid_ppl=1.10, sample_size=671.09\n",
      "| epoch 007 | valid on 'valid' subset:   4000 / 6947 valid_loss=0.133454, valid_ppl=1.10, sample_size=671.318\n",
      "| epoch 007 | valid on 'valid' subset:   5000 / 6947 valid_loss=0.132136, valid_ppl=1.10, sample_size=671.454\n",
      "| epoch 007 | valid on 'valid' subset:   6000 / 6947 valid_loss=0.133392, valid_ppl=1.10, sample_size=671.545\n",
      "| epoch 007 | valid on 'valid' subset | valid_loss 0.350286 | valid_ppl 1.27 | sample_size 670.872\n",
      "| epoch 008:   1000 / 27737 loss=0.247, ppl=1.19, wps=8968, ups=13.3, wpb=672, bsz=32, num_updates=195160, lr=0.25, gnorm=0.136, clip=34%, oom=0, sample_size=672\n",
      "| epoch 008:   2000 / 27737 loss=0.264, ppl=1.20, wps=8980, ups=13.4, wpb=672, bsz=32, num_updates=196160, lr=0.25, gnorm=0.136, clip=37%, oom=0, sample_size=672\n",
      "| epoch 008:   3000 / 27737 loss=0.244, ppl=1.18, wps=8960, ups=13.3, wpb=672, bsz=32, num_updates=197160, lr=0.25, gnorm=0.136, clip=38%, oom=0, sample_size=672\n",
      "| epoch 008:   4000 / 27737 loss=0.263, ppl=1.20, wps=8967, ups=13.3, wpb=672, bsz=32, num_updates=198160, lr=0.25, gnorm=0.136, clip=38%, oom=0, sample_size=672\n",
      "| epoch 008:   5000 / 27737 loss=0.275, ppl=1.21, wps=8960, ups=13.3, wpb=672, bsz=32, num_updates=199160, lr=0.25, gnorm=0.136, clip=39%, oom=0, sample_size=671.987\n",
      "| epoch 008:   6000 / 27737 loss=0.275, ppl=1.21, wps=8949, ups=13.3, wpb=672, bsz=32, num_updates=200160, lr=0.25, gnorm=0.136, clip=39%, oom=0, sample_size=671.99\n",
      "| epoch 008:   7000 / 27737 loss=0.276, ppl=1.21, wps=8943, ups=13.3, wpb=672, bsz=32, num_updates=201160, lr=0.25, gnorm=0.135, clip=40%, oom=0, sample_size=671.991\n",
      "| epoch 008:   8000 / 27737 loss=0.276, ppl=1.21, wps=8938, ups=13.3, wpb=672, bsz=32, num_updates=202160, lr=0.25, gnorm=0.135, clip=40%, oom=0, sample_size=671.992\n",
      "| epoch 008:   9000 / 27737 loss=0.284, ppl=1.22, wps=8933, ups=13.3, wpb=672, bsz=32, num_updates=203160, lr=0.25, gnorm=0.135, clip=40%, oom=0, sample_size=671.993\n",
      "| epoch 008:  10000 / 27737 loss=0.289, ppl=1.22, wps=8927, ups=13.3, wpb=672, bsz=32, num_updates=204160, lr=0.25, gnorm=0.135, clip=40%, oom=0, sample_size=671.994\n",
      "| epoch 008:  11000 / 27737 loss=0.286, ppl=1.22, wps=8965, ups=13.3, wpb=672, bsz=32, num_updates=205160, lr=0.25, gnorm=0.135, clip=40%, oom=0, sample_size=671.994\n",
      "| epoch 008:  12000 / 27737 loss=0.285, ppl=1.22, wps=8987, ups=13.4, wpb=672, bsz=32, num_updates=206160, lr=0.25, gnorm=0.135, clip=40%, oom=0, sample_size=671.995\n",
      "| epoch 008:  13000 / 27737 loss=0.282, ppl=1.22, wps=8998, ups=13.4, wpb=672, bsz=32, num_updates=207160, lr=0.25, gnorm=0.135, clip=40%, oom=0, sample_size=671.995\n",
      "| epoch 008:  14000 / 27737 loss=0.280, ppl=1.21, wps=9009, ups=13.4, wpb=672, bsz=32, num_updates=208160, lr=0.25, gnorm=0.135, clip=40%, oom=0, sample_size=671.996\n",
      "| epoch 008:  15000 / 27737 loss=0.281, ppl=1.21, wps=9011, ups=13.4, wpb=672, bsz=32, num_updates=209160, lr=0.25, gnorm=0.134, clip=40%, oom=0, sample_size=671.996\n",
      "| epoch 008:  16000 / 27737 loss=0.284, ppl=1.22, wps=9018, ups=13.4, wpb=672, bsz=32, num_updates=210160, lr=0.25, gnorm=0.134, clip=40%, oom=0, sample_size=671.996\n",
      "| epoch 008:  17000 / 27737 loss=0.283, ppl=1.22, wps=9019, ups=13.4, wpb=672, bsz=32, num_updates=211160, lr=0.25, gnorm=0.134, clip=41%, oom=0, sample_size=671.996\n",
      "| epoch 008:  18000 / 27737 loss=0.284, ppl=1.22, wps=9021, ups=13.4, wpb=672, bsz=32, num_updates=212160, lr=0.25, gnorm=0.134, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 008:  19000 / 27737 loss=0.280, ppl=1.21, wps=9030, ups=13.4, wpb=672, bsz=32, num_updates=213160, lr=0.25, gnorm=0.134, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 008:  20000 / 27737 loss=0.281, ppl=1.22, wps=9034, ups=13.4, wpb=672, bsz=32, num_updates=214160, lr=0.25, gnorm=0.134, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 008:  21000 / 27737 loss=0.283, ppl=1.22, wps=9034, ups=13.4, wpb=672, bsz=32, num_updates=215160, lr=0.25, gnorm=0.134, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 008:  22000 / 27737 loss=0.281, ppl=1.21, wps=9038, ups=13.4, wpb=672, bsz=32, num_updates=216160, lr=0.25, gnorm=0.134, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 008:  23000 / 27737 loss=0.284, ppl=1.22, wps=9036, ups=13.4, wpb=672, bsz=32, num_updates=217160, lr=0.25, gnorm=0.134, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 008:  24000 / 27737 loss=0.282, ppl=1.22, wps=9036, ups=13.4, wpb=672, bsz=32, num_updates=218160, lr=0.25, gnorm=0.133, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 008:  25000 / 27737 loss=0.283, ppl=1.22, wps=9037, ups=13.4, wpb=672, bsz=32, num_updates=219160, lr=0.25, gnorm=0.133, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 008:  26000 / 27737 loss=0.284, ppl=1.22, wps=9033, ups=13.4, wpb=672, bsz=32, num_updates=220160, lr=0.25, gnorm=0.133, clip=40%, oom=0, sample_size=671.998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 008:  27000 / 27737 loss=0.284, ppl=1.22, wps=9038, ups=13.4, wpb=672, bsz=32, num_updates=221160, lr=0.25, gnorm=0.133, clip=40%, oom=0, sample_size=671.998\n",
      "| epoch 008 | loss 0.285 | ppl 1.22 | wps 9048 | ups 13.5 | wpb 672 | bsz 32 | num_updates 221896 | lr 0.25 | gnorm 0.133 | clip 40% | oom 0 | sample_size 671.998\n",
      "| epoch 008 | valid on 'valid' subset:   1000 / 6947 valid_loss=0.13526, valid_ppl=1.10, sample_size=669.273\n",
      "| epoch 008 | valid on 'valid' subset:   2000 / 6947 valid_loss=0.138118, valid_ppl=1.10, sample_size=670.636\n",
      "| epoch 008 | valid on 'valid' subset:   3000 / 6947 valid_loss=0.133677, valid_ppl=1.10, sample_size=671.09\n",
      "| epoch 008 | valid on 'valid' subset:   4000 / 6947 valid_loss=0.132457, valid_ppl=1.10, sample_size=671.318\n",
      "| epoch 008 | valid on 'valid' subset:   5000 / 6947 valid_loss=0.131334, valid_ppl=1.10, sample_size=671.454\n",
      "| epoch 008 | valid on 'valid' subset:   6000 / 6947 valid_loss=0.13265, valid_ppl=1.10, sample_size=671.545\n",
      "| epoch 008 | valid on 'valid' subset | valid_loss 0.348742 | valid_ppl 1.27 | sample_size 670.872\n",
      "| epoch 009:   1000 / 27737 loss=0.253, ppl=1.19, wps=9271, ups=13.8, wpb=672, bsz=32, num_updates=222897, lr=0.25, gnorm=0.133, clip=36%, oom=0, sample_size=672\n",
      "| epoch 009:   2000 / 27737 loss=0.247, ppl=1.19, wps=9241, ups=13.8, wpb=672, bsz=32, num_updates=223897, lr=0.25, gnorm=0.133, clip=37%, oom=0, sample_size=672\n",
      "| epoch 009:   3000 / 27737 loss=0.255, ppl=1.19, wps=9270, ups=13.8, wpb=672, bsz=32, num_updates=224897, lr=0.25, gnorm=0.133, clip=38%, oom=0, sample_size=672\n",
      "| epoch 009:   4000 / 27737 loss=0.269, ppl=1.20, wps=9255, ups=13.8, wpb=672, bsz=32, num_updates=225897, lr=0.25, gnorm=0.133, clip=39%, oom=0, sample_size=672\n",
      "| epoch 009:   5000 / 27737 loss=0.275, ppl=1.21, wps=9262, ups=13.8, wpb=672, bsz=32, num_updates=226897, lr=0.25, gnorm=0.133, clip=39%, oom=0, sample_size=672\n",
      "| epoch 009:   6000 / 27737 loss=0.272, ppl=1.21, wps=9249, ups=13.8, wpb=672, bsz=32, num_updates=227897, lr=0.25, gnorm=0.132, clip=40%, oom=0, sample_size=672\n",
      "| epoch 009:   7000 / 27737 loss=0.268, ppl=1.20, wps=9246, ups=13.8, wpb=672, bsz=32, num_updates=228897, lr=0.25, gnorm=0.132, clip=40%, oom=0, sample_size=672\n",
      "| epoch 009:   8000 / 27737 loss=0.272, ppl=1.21, wps=9238, ups=13.7, wpb=672, bsz=32, num_updates=229897, lr=0.25, gnorm=0.132, clip=40%, oom=0, sample_size=672\n",
      "| epoch 009:   9000 / 27737 loss=0.267, ppl=1.20, wps=9248, ups=13.8, wpb=672, bsz=32, num_updates=230897, lr=0.25, gnorm=0.132, clip=40%, oom=0, sample_size=672\n",
      "| epoch 009:  10000 / 27737 loss=0.268, ppl=1.20, wps=9245, ups=13.8, wpb=672, bsz=32, num_updates=231897, lr=0.25, gnorm=0.132, clip=40%, oom=0, sample_size=672\n",
      "| epoch 009:  11000 / 27737 loss=0.265, ppl=1.20, wps=9244, ups=13.8, wpb=672, bsz=32, num_updates=232897, lr=0.25, gnorm=0.132, clip=40%, oom=0, sample_size=672\n",
      "| epoch 009:  12000 / 27737 loss=0.265, ppl=1.20, wps=9249, ups=13.8, wpb=672, bsz=32, num_updates=233897, lr=0.25, gnorm=0.132, clip=41%, oom=0, sample_size=672\n",
      "| epoch 009:  13000 / 27737 loss=0.267, ppl=1.20, wps=9257, ups=13.8, wpb=672, bsz=32, num_updates=234897, lr=0.25, gnorm=0.132, clip=41%, oom=0, sample_size=672\n",
      "| epoch 009:  14000 / 27737 loss=0.269, ppl=1.20, wps=9256, ups=13.8, wpb=672, bsz=32, num_updates=235897, lr=0.25, gnorm=0.132, clip=41%, oom=0, sample_size=672\n",
      "| epoch 009:  15000 / 27737 loss=0.269, ppl=1.21, wps=9258, ups=13.8, wpb=672, bsz=32, num_updates=236897, lr=0.25, gnorm=0.132, clip=41%, oom=0, sample_size=672\n",
      "| epoch 009:  16000 / 27737 loss=0.271, ppl=1.21, wps=9260, ups=13.8, wpb=672, bsz=32, num_updates=237897, lr=0.25, gnorm=0.132, clip=41%, oom=0, sample_size=672\n",
      "| epoch 009:  17000 / 27737 loss=0.268, ppl=1.20, wps=9262, ups=13.8, wpb=672, bsz=32, num_updates=238897, lr=0.25, gnorm=0.131, clip=41%, oom=0, sample_size=672\n",
      "| epoch 009:  18000 / 27737 loss=0.268, ppl=1.20, wps=9259, ups=13.8, wpb=672, bsz=32, num_updates=239897, lr=0.25, gnorm=0.131, clip=41%, oom=0, sample_size=672\n",
      "| epoch 009:  19000 / 27737 loss=0.270, ppl=1.21, wps=9264, ups=13.8, wpb=672, bsz=32, num_updates=240897, lr=0.25, gnorm=0.131, clip=41%, oom=0, sample_size=672\n",
      "| epoch 009:  20000 / 27737 loss=0.270, ppl=1.21, wps=9263, ups=13.8, wpb=672, bsz=32, num_updates=241897, lr=0.25, gnorm=0.131, clip=40%, oom=0, sample_size=672\n",
      "| epoch 009:  21000 / 27737 loss=0.271, ppl=1.21, wps=9266, ups=13.8, wpb=672, bsz=32, num_updates=242897, lr=0.25, gnorm=0.131, clip=40%, oom=0, sample_size=672\n",
      "| epoch 009:  22000 / 27737 loss=0.272, ppl=1.21, wps=9266, ups=13.8, wpb=672, bsz=32, num_updates=243897, lr=0.25, gnorm=0.131, clip=40%, oom=0, sample_size=672\n",
      "| epoch 009:  23000 / 27737 loss=0.271, ppl=1.21, wps=9266, ups=13.8, wpb=672, bsz=32, num_updates=244897, lr=0.25, gnorm=0.131, clip=40%, oom=0, sample_size=672\n",
      "| epoch 009:  24000 / 27737 loss=0.276, ppl=1.21, wps=9264, ups=13.8, wpb=672, bsz=32, num_updates=245897, lr=0.25, gnorm=0.131, clip=40%, oom=0, sample_size=672\n",
      "| epoch 009:  25000 / 27737 loss=0.274, ppl=1.21, wps=9264, ups=13.8, wpb=672, bsz=32, num_updates=246897, lr=0.25, gnorm=0.131, clip=40%, oom=0, sample_size=671.997\n",
      "| epoch 009:  26000 / 27737 loss=0.273, ppl=1.21, wps=9264, ups=13.8, wpb=672, bsz=32, num_updates=247897, lr=0.25, gnorm=0.131, clip=40%, oom=0, sample_size=671.998\n",
      "| epoch 009:  27000 / 27737 loss=0.274, ppl=1.21, wps=9262, ups=13.8, wpb=672, bsz=32, num_updates=248897, lr=0.25, gnorm=0.131, clip=40%, oom=0, sample_size=671.998\n",
      "| epoch 009 | loss 0.273 | ppl 1.21 | wps 9263 | ups 13.8 | wpb 672 | bsz 32 | num_updates 249633 | lr 0.25 | gnorm 0.130 | clip 40% | oom 0 | sample_size 671.998\n",
      "| epoch 009 | valid on 'valid' subset:   1000 / 6947 valid_loss=0.139494, valid_ppl=1.10, sample_size=669.273\n",
      "| epoch 009 | valid on 'valid' subset:   2000 / 6947 valid_loss=0.14195, valid_ppl=1.10, sample_size=670.636\n",
      "| epoch 009 | valid on 'valid' subset:   3000 / 6947 valid_loss=0.13742, valid_ppl=1.10, sample_size=671.09\n",
      "| epoch 009 | valid on 'valid' subset:   4000 / 6947 valid_loss=0.136415, valid_ppl=1.10, sample_size=671.318\n",
      "| epoch 009 | valid on 'valid' subset:   5000 / 6947 valid_loss=0.135256, valid_ppl=1.10, sample_size=671.454\n",
      "| epoch 009 | valid on 'valid' subset:   6000 / 6947 valid_loss=0.13673, valid_ppl=1.10, sample_size=671.545\n",
      "| epoch 009 | valid on 'valid' subset | valid_loss 0.351208 | valid_ppl 1.28 | sample_size 670.872\n",
      "| epoch 010:   1000 / 27737 loss=0.185, ppl=1.14, wps=9303, ups=13.8, wpb=672, bsz=32, num_updates=250634, lr=0.025, gnorm=0.130, clip=24%, oom=0, sample_size=672\n",
      "| epoch 010:   2000 / 27737 loss=0.231, ppl=1.17, wps=9326, ups=13.9, wpb=672, bsz=32, num_updates=251634, lr=0.025, gnorm=0.130, clip=23%, oom=0, sample_size=672\n",
      "| epoch 010:   3000 / 27737 loss=0.237, ppl=1.18, wps=9310, ups=13.9, wpb=672, bsz=32, num_updates=252634, lr=0.025, gnorm=0.130, clip=24%, oom=0, sample_size=672\n",
      "| epoch 010:   4000 / 27737 loss=0.246, ppl=1.19, wps=9310, ups=13.9, wpb=672, bsz=32, num_updates=253634, lr=0.025, gnorm=0.130, clip=24%, oom=0, sample_size=672\n",
      "| epoch 010:   5000 / 27737 loss=0.249, ppl=1.19, wps=9293, ups=13.8, wpb=672, bsz=32, num_updates=254634, lr=0.025, gnorm=0.130, clip=24%, oom=0, sample_size=672\n",
      "| epoch 010:   6000 / 27737 loss=0.247, ppl=1.19, wps=9294, ups=13.8, wpb=672, bsz=32, num_updates=255634, lr=0.025, gnorm=0.130, clip=24%, oom=0, sample_size=671.99\n",
      "| epoch 010:   7000 / 27737 loss=0.242, ppl=1.18, wps=9279, ups=13.8, wpb=672, bsz=32, num_updates=256634, lr=0.025, gnorm=0.129, clip=24%, oom=0, sample_size=671.991\n",
      "| epoch 010:   8000 / 27737 loss=0.242, ppl=1.18, wps=9295, ups=13.8, wpb=672, bsz=32, num_updates=257634, lr=0.025, gnorm=0.129, clip=24%, oom=0, sample_size=671.992\n",
      "| epoch 010:   9000 / 27737 loss=0.243, ppl=1.18, wps=9325, ups=13.9, wpb=672, bsz=32, num_updates=258634, lr=0.025, gnorm=0.129, clip=24%, oom=0, sample_size=671.993\n",
      "| epoch 010:  10000 / 27737 loss=0.244, ppl=1.18, wps=9348, ups=13.9, wpb=672, bsz=32, num_updates=259634, lr=0.025, gnorm=0.129, clip=24%, oom=0, sample_size=671.994\n",
      "| epoch 010:  11000 / 27737 loss=0.242, ppl=1.18, wps=9341, ups=13.9, wpb=672, bsz=32, num_updates=260634, lr=0.025, gnorm=0.129, clip=23%, oom=0, sample_size=671.994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 010:  12000 / 27737 loss=0.243, ppl=1.18, wps=9339, ups=13.9, wpb=672, bsz=32, num_updates=261634, lr=0.025, gnorm=0.129, clip=23%, oom=0, sample_size=671.995\n",
      "| epoch 010:  13000 / 27737 loss=0.243, ppl=1.18, wps=9333, ups=13.9, wpb=672, bsz=32, num_updates=262634, lr=0.025, gnorm=0.128, clip=23%, oom=0, sample_size=671.995\n",
      "| epoch 010:  14000 / 27737 loss=0.242, ppl=1.18, wps=9328, ups=13.9, wpb=672, bsz=32, num_updates=263634, lr=0.025, gnorm=0.128, clip=23%, oom=0, sample_size=671.996\n",
      "| epoch 010:  15000 / 27737 loss=0.242, ppl=1.18, wps=9325, ups=13.9, wpb=672, bsz=32, num_updates=264634, lr=0.025, gnorm=0.128, clip=23%, oom=0, sample_size=671.996\n",
      "| epoch 010:  16000 / 27737 loss=0.238, ppl=1.18, wps=9324, ups=13.9, wpb=672, bsz=32, num_updates=265634, lr=0.025, gnorm=0.128, clip=23%, oom=0, sample_size=671.996\n",
      "| epoch 010:  17000 / 27737 loss=0.236, ppl=1.18, wps=9320, ups=13.9, wpb=672, bsz=32, num_updates=266634, lr=0.025, gnorm=0.128, clip=23%, oom=0, sample_size=671.996\n",
      "| epoch 010:  18000 / 27737 loss=0.235, ppl=1.18, wps=9321, ups=13.9, wpb=672, bsz=32, num_updates=267634, lr=0.025, gnorm=0.128, clip=23%, oom=0, sample_size=671.997\n",
      "| epoch 010:  19000 / 27737 loss=0.234, ppl=1.18, wps=9320, ups=13.9, wpb=672, bsz=32, num_updates=268634, lr=0.025, gnorm=0.128, clip=23%, oom=0, sample_size=671.997\n",
      "| epoch 010:  20000 / 27737 loss=0.234, ppl=1.18, wps=9319, ups=13.9, wpb=672, bsz=32, num_updates=269634, lr=0.025, gnorm=0.127, clip=23%, oom=0, sample_size=671.997\n",
      "| epoch 010:  21000 / 27737 loss=0.232, ppl=1.17, wps=9318, ups=13.9, wpb=672, bsz=32, num_updates=270634, lr=0.025, gnorm=0.127, clip=23%, oom=0, sample_size=671.997\n",
      "| epoch 010:  22000 / 27737 loss=0.231, ppl=1.17, wps=9316, ups=13.9, wpb=672, bsz=32, num_updates=271634, lr=0.025, gnorm=0.127, clip=23%, oom=0, sample_size=671.997\n",
      "| epoch 010:  23000 / 27737 loss=0.230, ppl=1.17, wps=9314, ups=13.9, wpb=672, bsz=32, num_updates=272634, lr=0.025, gnorm=0.127, clip=23%, oom=0, sample_size=671.997\n",
      "| epoch 010:  24000 / 27737 loss=0.229, ppl=1.17, wps=9312, ups=13.9, wpb=672, bsz=32, num_updates=273634, lr=0.025, gnorm=0.127, clip=23%, oom=0, sample_size=671.997\n",
      "| epoch 010:  25000 / 27737 loss=0.228, ppl=1.17, wps=9308, ups=13.9, wpb=672, bsz=32, num_updates=274634, lr=0.025, gnorm=0.127, clip=23%, oom=0, sample_size=671.997\n",
      "| epoch 010:  26000 / 27737 loss=0.228, ppl=1.17, wps=9307, ups=13.9, wpb=672, bsz=32, num_updates=275634, lr=0.025, gnorm=0.126, clip=23%, oom=0, sample_size=671.998\n",
      "| epoch 010:  27000 / 27737 loss=0.227, ppl=1.17, wps=9306, ups=13.8, wpb=672, bsz=32, num_updates=276634, lr=0.025, gnorm=0.126, clip=23%, oom=0, sample_size=671.998\n",
      "| epoch 010 | loss 0.225 | ppl 1.17 | wps 9305 | ups 13.8 | wpb 672 | bsz 32 | num_updates 277370 | lr 0.025 | gnorm 0.126 | clip 22% | oom 0 | sample_size 671.998\n",
      "| epoch 010 | valid on 'valid' subset:   1000 / 6947 valid_loss=0.136045, valid_ppl=1.10, sample_size=669.273\n",
      "| epoch 010 | valid on 'valid' subset:   2000 / 6947 valid_loss=0.140115, valid_ppl=1.10, sample_size=670.636\n",
      "| epoch 010 | valid on 'valid' subset:   3000 / 6947 valid_loss=0.135622, valid_ppl=1.10, sample_size=671.09\n",
      "| epoch 010 | valid on 'valid' subset:   4000 / 6947 valid_loss=0.134989, valid_ppl=1.10, sample_size=671.318\n",
      "| epoch 010 | valid on 'valid' subset:   5000 / 6947 valid_loss=0.133677, valid_ppl=1.10, sample_size=671.454\n",
      "| epoch 010 | valid on 'valid' subset:   6000 / 6947 valid_loss=0.135192, valid_ppl=1.10, sample_size=671.545\n",
      "| epoch 010 | valid on 'valid' subset | valid_loss 0.339093 | valid_ppl 1.26 | sample_size 670.872\n",
      "| epoch 011:   1000 / 27737 loss=0.207, ppl=1.15, wps=9305, ups=13.8, wpb=672, bsz=32, num_updates=278371, lr=0.025, gnorm=0.126, clip=17%, oom=0, sample_size=671.937\n",
      "| epoch 011:   2000 / 27737 loss=0.216, ppl=1.16, wps=9345, ups=13.9, wpb=672, bsz=32, num_updates=279371, lr=0.025, gnorm=0.126, clip=19%, oom=0, sample_size=671.969\n",
      "| epoch 011:   3000 / 27737 loss=0.229, ppl=1.17, wps=9330, ups=13.9, wpb=672, bsz=32, num_updates=280371, lr=0.025, gnorm=0.126, clip=20%, oom=0, sample_size=671.979\n",
      "| epoch 011:   4000 / 27737 loss=0.220, ppl=1.16, wps=9329, ups=13.9, wpb=672, bsz=32, num_updates=281371, lr=0.025, gnorm=0.126, clip=20%, oom=0, sample_size=671.984\n",
      "| epoch 011:   5000 / 27737 loss=0.220, ppl=1.16, wps=9325, ups=13.9, wpb=672, bsz=32, num_updates=282371, lr=0.025, gnorm=0.125, clip=20%, oom=0, sample_size=671.987\n",
      "| epoch 011:   6000 / 27737 loss=0.215, ppl=1.16, wps=9332, ups=13.9, wpb=672, bsz=32, num_updates=283371, lr=0.025, gnorm=0.125, clip=20%, oom=0, sample_size=671.99\n",
      "| epoch 011:   7000 / 27737 loss=0.214, ppl=1.16, wps=9323, ups=13.9, wpb=672, bsz=32, num_updates=284371, lr=0.025, gnorm=0.125, clip=20%, oom=0, sample_size=671.991\n",
      "| epoch 011:   8000 / 27737 loss=0.215, ppl=1.16, wps=9323, ups=13.9, wpb=672, bsz=32, num_updates=285371, lr=0.025, gnorm=0.125, clip=20%, oom=0, sample_size=671.992\n",
      "| epoch 011:   9000 / 27737 loss=0.208, ppl=1.15, wps=9315, ups=13.9, wpb=672, bsz=32, num_updates=286371, lr=0.025, gnorm=0.125, clip=20%, oom=0, sample_size=671.993\n",
      "| epoch 011:  10000 / 27737 loss=0.211, ppl=1.16, wps=9318, ups=13.9, wpb=672, bsz=32, num_updates=287371, lr=0.025, gnorm=0.125, clip=21%, oom=0, sample_size=671.994\n",
      "| epoch 011:  11000 / 27737 loss=0.213, ppl=1.16, wps=9309, ups=13.9, wpb=672, bsz=32, num_updates=288371, lr=0.025, gnorm=0.125, clip=21%, oom=0, sample_size=671.994\n",
      "| epoch 011:  12000 / 27737 loss=0.214, ppl=1.16, wps=9307, ups=13.9, wpb=672, bsz=32, num_updates=289371, lr=0.025, gnorm=0.125, clip=21%, oom=0, sample_size=671.995\n",
      "| epoch 011:  13000 / 27737 loss=0.218, ppl=1.16, wps=9305, ups=13.8, wpb=672, bsz=32, num_updates=290371, lr=0.025, gnorm=0.124, clip=21%, oom=0, sample_size=671.995\n",
      "| epoch 011:  14000 / 27737 loss=0.215, ppl=1.16, wps=9301, ups=13.8, wpb=672, bsz=32, num_updates=291371, lr=0.025, gnorm=0.124, clip=21%, oom=0, sample_size=671.996\n",
      "| epoch 011:  15000 / 27737 loss=0.216, ppl=1.16, wps=9303, ups=13.8, wpb=672, bsz=32, num_updates=292371, lr=0.025, gnorm=0.124, clip=21%, oom=0, sample_size=671.996\n",
      "| epoch 011:  16000 / 27737 loss=0.219, ppl=1.16, wps=9302, ups=13.8, wpb=672, bsz=32, num_updates=293371, lr=0.025, gnorm=0.124, clip=21%, oom=0, sample_size=671.996\n",
      "| epoch 011:  17000 / 27737 loss=0.219, ppl=1.16, wps=9297, ups=13.8, wpb=672, bsz=32, num_updates=294371, lr=0.025, gnorm=0.124, clip=21%, oom=0, sample_size=671.996\n",
      "| epoch 011:  18000 / 27737 loss=0.219, ppl=1.16, wps=9298, ups=13.8, wpb=672, bsz=32, num_updates=295371, lr=0.025, gnorm=0.124, clip=21%, oom=0, sample_size=671.997\n",
      "| epoch 011:  19000 / 27737 loss=0.218, ppl=1.16, wps=9293, ups=13.8, wpb=672, bsz=32, num_updates=296371, lr=0.025, gnorm=0.124, clip=21%, oom=0, sample_size=671.997\n",
      "| epoch 011:  20000 / 27737 loss=0.217, ppl=1.16, wps=9294, ups=13.8, wpb=672, bsz=32, num_updates=297371, lr=0.025, gnorm=0.124, clip=21%, oom=0, sample_size=671.997\n",
      "| epoch 011:  21000 / 27737 loss=0.217, ppl=1.16, wps=9304, ups=13.8, wpb=672, bsz=32, num_updates=298371, lr=0.025, gnorm=0.123, clip=21%, oom=0, sample_size=671.997\n",
      "| epoch 011:  22000 / 27737 loss=0.216, ppl=1.16, wps=9317, ups=13.9, wpb=672, bsz=32, num_updates=299371, lr=0.025, gnorm=0.123, clip=22%, oom=0, sample_size=671.997\n",
      "| epoch 011:  23000 / 27737 loss=0.215, ppl=1.16, wps=9317, ups=13.9, wpb=672, bsz=32, num_updates=300371, lr=0.025, gnorm=0.123, clip=22%, oom=0, sample_size=671.997\n",
      "| epoch 011:  24000 / 27737 loss=0.215, ppl=1.16, wps=9315, ups=13.9, wpb=672, bsz=32, num_updates=301371, lr=0.025, gnorm=0.123, clip=22%, oom=0, sample_size=671.997\n",
      "| epoch 011:  25000 / 27737 loss=0.215, ppl=1.16, wps=9315, ups=13.9, wpb=672, bsz=32, num_updates=302371, lr=0.025, gnorm=0.123, clip=22%, oom=0, sample_size=671.997\n",
      "| epoch 011:  26000 / 27737 loss=0.216, ppl=1.16, wps=9317, ups=13.9, wpb=672, bsz=32, num_updates=303371, lr=0.025, gnorm=0.123, clip=22%, oom=0, sample_size=671.998\n",
      "| epoch 011:  27000 / 27737 loss=0.216, ppl=1.16, wps=9317, ups=13.9, wpb=672, bsz=32, num_updates=304371, lr=0.025, gnorm=0.123, clip=22%, oom=0, sample_size=671.998\n",
      "| epoch 011 | loss 0.215 | ppl 1.16 | wps 9315 | ups 13.9 | wpb 672 | bsz 32 | num_updates 305107 | lr 0.025 | gnorm 0.123 | clip 22% | oom 0 | sample_size 671.998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 011 | valid on 'valid' subset:   1000 / 6947 valid_loss=0.140539, valid_ppl=1.10, sample_size=669.273\n",
      "| epoch 011 | valid on 'valid' subset:   2000 / 6947 valid_loss=0.144593, valid_ppl=1.11, sample_size=670.636\n",
      "| epoch 011 | valid on 'valid' subset:   3000 / 6947 valid_loss=0.140041, valid_ppl=1.10, sample_size=671.09\n",
      "| epoch 011 | valid on 'valid' subset:   4000 / 6947 valid_loss=0.13935, valid_ppl=1.10, sample_size=671.318\n",
      "| epoch 011 | valid on 'valid' subset:   5000 / 6947 valid_loss=0.13805, valid_ppl=1.10, sample_size=671.454\n",
      "| epoch 011 | valid on 'valid' subset:   6000 / 6947 valid_loss=0.139605, valid_ppl=1.10, sample_size=671.545\n",
      "| epoch 011 | valid on 'valid' subset | valid_loss 0.342945 | valid_ppl 1.27 | sample_size 670.872\n",
      "| epoch 012:   1000 / 27737 loss=0.225, ppl=1.17, wps=9383, ups=14.0, wpb=672, bsz=32, num_updates=306108, lr=0.0025, gnorm=0.122, clip=18%, oom=0, sample_size=672\n",
      "| epoch 012:   2000 / 27737 loss=0.251, ppl=1.19, wps=9323, ups=13.9, wpb=672, bsz=32, num_updates=307108, lr=0.0025, gnorm=0.122, clip=20%, oom=0, sample_size=672\n",
      "| epoch 012:   3000 / 27737 loss=0.232, ppl=1.17, wps=9312, ups=13.9, wpb=672, bsz=32, num_updates=308108, lr=0.0025, gnorm=0.122, clip=19%, oom=0, sample_size=672\n",
      "| epoch 012:   4000 / 27737 loss=0.231, ppl=1.17, wps=9297, ups=13.8, wpb=672, bsz=32, num_updates=309108, lr=0.0025, gnorm=0.122, clip=19%, oom=0, sample_size=672\n",
      "| epoch 012:   5000 / 27737 loss=0.228, ppl=1.17, wps=9298, ups=13.8, wpb=672, bsz=32, num_updates=310108, lr=0.0025, gnorm=0.122, clip=20%, oom=0, sample_size=672\n",
      "| epoch 012:   6000 / 27737 loss=0.219, ppl=1.16, wps=9288, ups=13.8, wpb=672, bsz=32, num_updates=311108, lr=0.0025, gnorm=0.122, clip=20%, oom=0, sample_size=672\n",
      "| epoch 012:   7000 / 27737 loss=0.214, ppl=1.16, wps=9281, ups=13.8, wpb=672, bsz=32, num_updates=312108, lr=0.0025, gnorm=0.122, clip=20%, oom=0, sample_size=672\n",
      "| epoch 012:   8000 / 27737 loss=0.212, ppl=1.16, wps=9288, ups=13.8, wpb=672, bsz=32, num_updates=313108, lr=0.0025, gnorm=0.122, clip=20%, oom=0, sample_size=672\n",
      "| epoch 012:   9000 / 27737 loss=0.209, ppl=1.16, wps=9281, ups=13.8, wpb=672, bsz=32, num_updates=314108, lr=0.0025, gnorm=0.121, clip=20%, oom=0, sample_size=672\n",
      "| epoch 012:  10000 / 27737 loss=0.213, ppl=1.16, wps=9283, ups=13.8, wpb=672, bsz=32, num_updates=315108, lr=0.0025, gnorm=0.121, clip=20%, oom=0, sample_size=672\n",
      "| epoch 012:  11000 / 27737 loss=0.211, ppl=1.16, wps=9279, ups=13.8, wpb=672, bsz=32, num_updates=316108, lr=0.0025, gnorm=0.121, clip=20%, oom=0, sample_size=671.994\n",
      "| epoch 012:  12000 / 27737 loss=0.212, ppl=1.16, wps=9280, ups=13.8, wpb=672, bsz=32, num_updates=317108, lr=0.0025, gnorm=0.121, clip=20%, oom=0, sample_size=671.995\n",
      "| epoch 012:  13000 / 27737 loss=0.210, ppl=1.16, wps=9277, ups=13.8, wpb=672, bsz=32, num_updates=318108, lr=0.0025, gnorm=0.121, clip=20%, oom=0, sample_size=671.995\n",
      "| epoch 012:  14000 / 27737 loss=0.206, ppl=1.15, wps=9274, ups=13.8, wpb=672, bsz=32, num_updates=319108, lr=0.0025, gnorm=0.121, clip=20%, oom=0, sample_size=671.996\n",
      "| epoch 012:  15000 / 27737 loss=0.204, ppl=1.15, wps=9277, ups=13.8, wpb=672, bsz=32, num_updates=320108, lr=0.0025, gnorm=0.121, clip=20%, oom=0, sample_size=671.996\n",
      "| epoch 012:  16000 / 27737 loss=0.205, ppl=1.15, wps=9273, ups=13.8, wpb=672, bsz=32, num_updates=321108, lr=0.0025, gnorm=0.121, clip=20%, oom=0, sample_size=671.996\n",
      "| epoch 012:  17000 / 27737 loss=0.205, ppl=1.15, wps=9270, ups=13.8, wpb=672, bsz=32, num_updates=322108, lr=0.0025, gnorm=0.121, clip=20%, oom=0, sample_size=671.996\n",
      "| epoch 012:  18000 / 27737 loss=0.204, ppl=1.15, wps=9266, ups=13.8, wpb=672, bsz=32, num_updates=323108, lr=0.0025, gnorm=0.120, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 012:  19000 / 27737 loss=0.202, ppl=1.15, wps=9265, ups=13.8, wpb=672, bsz=32, num_updates=324108, lr=0.0025, gnorm=0.120, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 012:  20000 / 27737 loss=0.203, ppl=1.15, wps=9262, ups=13.8, wpb=672, bsz=32, num_updates=325108, lr=0.0025, gnorm=0.120, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 012:  21000 / 27737 loss=0.204, ppl=1.15, wps=9262, ups=13.8, wpb=672, bsz=32, num_updates=326108, lr=0.0025, gnorm=0.120, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 012:  22000 / 27737 loss=0.204, ppl=1.15, wps=9259, ups=13.8, wpb=672, bsz=32, num_updates=327108, lr=0.0025, gnorm=0.120, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 012:  23000 / 27737 loss=0.207, ppl=1.15, wps=9259, ups=13.8, wpb=672, bsz=32, num_updates=328108, lr=0.0025, gnorm=0.120, clip=21%, oom=0, sample_size=671.997\n",
      "| epoch 012:  24000 / 27737 loss=0.207, ppl=1.15, wps=9257, ups=13.8, wpb=672, bsz=32, num_updates=329108, lr=0.0025, gnorm=0.120, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 012:  25000 / 27737 loss=0.208, ppl=1.16, wps=9256, ups=13.8, wpb=672, bsz=32, num_updates=330108, lr=0.0025, gnorm=0.120, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 012:  26000 / 27737 loss=0.209, ppl=1.16, wps=9257, ups=13.8, wpb=672, bsz=32, num_updates=331108, lr=0.0025, gnorm=0.120, clip=20%, oom=0, sample_size=671.998\n",
      "| epoch 012:  27000 / 27737 loss=0.207, ppl=1.15, wps=9257, ups=13.8, wpb=672, bsz=32, num_updates=332108, lr=0.0025, gnorm=0.119, clip=20%, oom=0, sample_size=671.998\n",
      "| epoch 012 | loss 0.208 | ppl 1.16 | wps 9261 | ups 13.8 | wpb 672 | bsz 32 | num_updates 332844 | lr 0.0025 | gnorm 0.119 | clip 20% | oom 0 | sample_size 671.998\n",
      "| epoch 012 | valid on 'valid' subset:   1000 / 6947 valid_loss=0.142262, valid_ppl=1.10, sample_size=669.273\n",
      "| epoch 012 | valid on 'valid' subset:   2000 / 6947 valid_loss=0.14651, valid_ppl=1.11, sample_size=670.636\n",
      "| epoch 012 | valid on 'valid' subset:   3000 / 6947 valid_loss=0.141861, valid_ppl=1.10, sample_size=671.09\n",
      "| epoch 012 | valid on 'valid' subset:   4000 / 6947 valid_loss=0.141211, valid_ppl=1.10, sample_size=671.318\n",
      "| epoch 012 | valid on 'valid' subset:   5000 / 6947 valid_loss=0.139909, valid_ppl=1.10, sample_size=671.454\n",
      "| epoch 012 | valid on 'valid' subset:   6000 / 6947 valid_loss=0.141505, valid_ppl=1.10, sample_size=671.545\n",
      "| epoch 012 | valid on 'valid' subset | valid_loss 0.344696 | valid_ppl 1.27 | sample_size 670.872\n",
      "| epoch 013:   1000 / 27737 loss=0.219, ppl=1.16, wps=9279, ups=13.8, wpb=672, bsz=32, num_updates=333845, lr=0.00025, gnorm=0.119, clip=21%, oom=0, sample_size=672\n",
      "| epoch 013:   2000 / 27737 loss=0.196, ppl=1.15, wps=9279, ups=13.8, wpb=672, bsz=32, num_updates=334845, lr=0.00025, gnorm=0.119, clip=19%, oom=0, sample_size=672\n",
      "| epoch 013:   3000 / 27737 loss=0.202, ppl=1.15, wps=9279, ups=13.8, wpb=672, bsz=32, num_updates=335845, lr=0.00025, gnorm=0.119, clip=19%, oom=0, sample_size=672\n",
      "| epoch 013:   4000 / 27737 loss=0.205, ppl=1.15, wps=9299, ups=13.8, wpb=672, bsz=32, num_updates=336845, lr=0.00025, gnorm=0.119, clip=19%, oom=0, sample_size=672\n",
      "| epoch 013:   5000 / 27737 loss=0.196, ppl=1.15, wps=9281, ups=13.8, wpb=672, bsz=32, num_updates=337845, lr=0.00025, gnorm=0.119, clip=20%, oom=0, sample_size=672\n",
      "| epoch 013:   6000 / 27737 loss=0.198, ppl=1.15, wps=9289, ups=13.8, wpb=672, bsz=32, num_updates=338845, lr=0.00025, gnorm=0.119, clip=20%, oom=0, sample_size=672\n",
      "| epoch 013:   7000 / 27737 loss=0.201, ppl=1.15, wps=9328, ups=13.9, wpb=672, bsz=32, num_updates=339845, lr=0.00025, gnorm=0.119, clip=20%, oom=0, sample_size=672\n",
      "| epoch 013:   8000 / 27737 loss=0.209, ppl=1.16, wps=9365, ups=13.9, wpb=672, bsz=32, num_updates=340845, lr=0.00025, gnorm=0.119, clip=20%, oom=0, sample_size=672\n",
      "| epoch 013:   9000 / 27737 loss=0.207, ppl=1.15, wps=9362, ups=13.9, wpb=672, bsz=32, num_updates=341845, lr=0.00025, gnorm=0.118, clip=20%, oom=0, sample_size=672\n",
      "| epoch 013:  10000 / 27737 loss=0.206, ppl=1.15, wps=9354, ups=13.9, wpb=672, bsz=32, num_updates=342845, lr=0.00025, gnorm=0.118, clip=20%, oom=0, sample_size=672\n",
      "| epoch 013:  11000 / 27737 loss=0.208, ppl=1.16, wps=9349, ups=13.9, wpb=672, bsz=32, num_updates=343845, lr=0.00025, gnorm=0.118, clip=20%, oom=0, sample_size=672\n",
      "| epoch 013:  12000 / 27737 loss=0.211, ppl=1.16, wps=9344, ups=13.9, wpb=672, bsz=32, num_updates=344845, lr=0.00025, gnorm=0.118, clip=20%, oom=0, sample_size=672\n",
      "| epoch 013:  13000 / 27737 loss=0.211, ppl=1.16, wps=9342, ups=13.9, wpb=672, bsz=32, num_updates=345845, lr=0.00025, gnorm=0.118, clip=20%, oom=0, sample_size=671.995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 013:  14000 / 27737 loss=0.210, ppl=1.16, wps=9336, ups=13.9, wpb=672, bsz=32, num_updates=346845, lr=0.00025, gnorm=0.118, clip=20%, oom=0, sample_size=671.996\n",
      "| epoch 013:  15000 / 27737 loss=0.212, ppl=1.16, wps=9327, ups=13.9, wpb=672, bsz=32, num_updates=347845, lr=0.00025, gnorm=0.118, clip=20%, oom=0, sample_size=671.996\n",
      "| epoch 013:  16000 / 27737 loss=0.213, ppl=1.16, wps=9325, ups=13.9, wpb=672, bsz=32, num_updates=348845, lr=0.00025, gnorm=0.118, clip=20%, oom=0, sample_size=671.996\n",
      "| epoch 013:  17000 / 27737 loss=0.211, ppl=1.16, wps=9322, ups=13.9, wpb=672, bsz=32, num_updates=349845, lr=0.00025, gnorm=0.118, clip=20%, oom=0, sample_size=671.996\n",
      "| epoch 013:  18000 / 27737 loss=0.208, ppl=1.16, wps=9320, ups=13.9, wpb=672, bsz=32, num_updates=350845, lr=0.00025, gnorm=0.118, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 013:  19000 / 27737 loss=0.209, ppl=1.16, wps=9320, ups=13.9, wpb=672, bsz=32, num_updates=351845, lr=0.00025, gnorm=0.117, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 013:  20000 / 27737 loss=0.209, ppl=1.16, wps=9317, ups=13.9, wpb=672, bsz=32, num_updates=352845, lr=0.00025, gnorm=0.117, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 013:  21000 / 27737 loss=0.209, ppl=1.16, wps=9315, ups=13.9, wpb=672, bsz=32, num_updates=353845, lr=0.00025, gnorm=0.117, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 013:  22000 / 27737 loss=0.208, ppl=1.16, wps=9314, ups=13.9, wpb=672, bsz=32, num_updates=354845, lr=0.00025, gnorm=0.117, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 013:  23000 / 27737 loss=0.209, ppl=1.16, wps=9309, ups=13.9, wpb=672, bsz=32, num_updates=355845, lr=0.00025, gnorm=0.117, clip=21%, oom=0, sample_size=671.997\n",
      "| epoch 013:  24000 / 27737 loss=0.208, ppl=1.15, wps=9306, ups=13.8, wpb=672, bsz=32, num_updates=356845, lr=0.00025, gnorm=0.117, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 013:  25000 / 27737 loss=0.207, ppl=1.15, wps=9301, ups=13.8, wpb=672, bsz=32, num_updates=357845, lr=0.00025, gnorm=0.117, clip=20%, oom=0, sample_size=671.997\n",
      "| epoch 013:  26000 / 27737 loss=0.208, ppl=1.16, wps=9301, ups=13.8, wpb=672, bsz=32, num_updates=358845, lr=0.00025, gnorm=0.117, clip=20%, oom=0, sample_size=671.998\n",
      "| epoch 013:  27000 / 27737 loss=0.208, ppl=1.16, wps=9298, ups=13.8, wpb=672, bsz=32, num_updates=359845, lr=0.00025, gnorm=0.117, clip=20%, oom=0, sample_size=671.998\n",
      "| epoch 013 | loss 0.207 | ppl 1.15 | wps 9296 | ups 13.8 | wpb 672 | bsz 32 | num_updates 360581 | lr 0.00025 | gnorm 0.117 | clip 20% | oom 0 | sample_size 671.998\n",
      "| epoch 013 | valid on 'valid' subset:   1000 / 6947 valid_loss=0.142443, valid_ppl=1.10, sample_size=669.273\n",
      "| epoch 013 | valid on 'valid' subset:   2000 / 6947 valid_loss=0.146705, valid_ppl=1.11, sample_size=670.636\n",
      "| epoch 013 | valid on 'valid' subset:   3000 / 6947 valid_loss=0.142056, valid_ppl=1.10, sample_size=671.09\n",
      "| epoch 013 | valid on 'valid' subset:   4000 / 6947 valid_loss=0.141408, valid_ppl=1.10, sample_size=671.318\n",
      "| epoch 013 | valid on 'valid' subset:   5000 / 6947 valid_loss=0.140102, valid_ppl=1.10, sample_size=671.454\n",
      "| epoch 013 | valid on 'valid' subset:   6000 / 6947 valid_loss=0.141701, valid_ppl=1.10, sample_size=671.545\n",
      "| epoch 013 | valid on 'valid' subset | valid_loss 0.344893 | valid_ppl 1.27 | sample_size 670.872\n",
      "| done training in 28903.8 seconds\n"
     ]
    }
   ],
   "source": [
    "!head -n 4 /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train_no_bpe.tok.src\n",
    "print('-'*50)\n",
    "!head -n 4 /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train_no_bpe.tok.trg\n",
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training && \\\n",
    "    chmod +x preprocess_no_bpe.sh && ./preprocess_no_bpe.sh && \\\n",
    "    chmod +x train_no_bpe.sh && ./train_no_bpe.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/autograd/function.py:41: UserWarning: mark_shared_storage is deprecated. Tensors with shared storages are automatically tracked. Note that calls to `set_()` are not tracked\n",
      "  'mark_shared_storage is deprecated. '\n"
     ]
    }
   ],
   "source": [
    "!FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py \\\n",
    "    && root=$FAIRSEQPY/../.. \\\n",
    "    && env PYTHONPATH=$FAIRSEQPY:$PYTHONPATH CUDA_VISIBLE_DEVICES=\"3\" \\\n",
    "    python $FAIRSEQPY/interactive.py \\\n",
    "    --no-progress-bar \\\n",
    "    --path $root/training/models/mlconv_embed/model1/checkpoint_best.pt \\\n",
    "    --beam 12 $root/training/processed/bin/ < \\\n",
    "    $root/training/processed/dev.src > $root/training/processed/dev.trg.pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
      "res from CRNN\n",
      "罩 注 樊 金 按 注 均 分 ， ！ 注 最 高 限 额 封 顶 5 0 0\n",
      "并 回 吐 其 近 期 的 涨 幅 。 若 该 数 据 意 外 上 扬 · 或\n",
      "这 将 是 近 年 来 “ 最 令 人 激 勤 ” 的 历 史 发 现 之 一\n",
      "这 是 一 部 写 实 的 严 肃 题 材 。 张 家 辉 · 鲍 起 静 ·\n",
      "央 行 或 考 虑 再 度 升 息 。 若 加 拿 大 6 月 通 服 压 力\n",
      "postprocessed by CS2S:\n",
      "O\t罩 注 樊 金 按 注 均 分 ， ！ 注 最 高 限 额 封 顶 5 0 0\n",
      "H\t-3.400041532586329e-05\t单 注 奖 金 按 注 均 分 ， 单 注 最 高 限 额 封 顶 5 0 0\n",
      "A\t0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 14\n",
      "O\t并 回 吐 其 近 期 的 涨 幅 。 若 该 数 据 意 外 上 扬 · 或\n",
      "H\t-0.0009221668005920947\t并 回 吐 其 近 期 的 涨 幅 。 若 该 数 据 意 外 上 扬 ， 或\n",
      "A\t0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 1\n",
      "O\t这 将 是 近 年 来 “ 最 令 人 激 勤 ” 的 历 史 发 现 之 一\n",
      "H\t-3.2322233892045915e-05\t这 将 是 近 年 来 “ 最 令 人 激 动 ” 的 历 史 发 现 之 一\n",
      "A\t0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 0\n",
      "O\t这 是 一 部 写 实 的 严 肃 题 材 。 张 家 辉 · 鲍 起 静 ·\n",
      "H\t-0.002021280350163579\t这 是 一 部 写 实 的 严 肃 题 材 。 张 家 辉 、 鲍 起 静 、\n",
      "A\t0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 16\n",
      "O\t央 行 或 考 虑 再 度 升 息 。 若 加 拿 大 6 月 通 服 压 力\n",
      "H\t-0.0010641333647072315\t央 行 或 考 虑 再 度 升 息 。 若 加 拿 大 6 月 通 胀 压 力\n",
      "A\t0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 18\n",
      "ground truth:\n",
      "单 注 奖 金 按 注 均 分 ， 单 注 最 高 限 额 封 顶 5 0 0\n",
      "并 回 吐 其 近 期 的 涨 幅 。 若 该 数 据 意 外 上 扬 ， 或\n",
      "这 将 是 近 年 来 “ 最 令 人 激 动 ” 的 历 史 发 现 之 一\n",
      "这 是 一 部 写 实 的 严 肃 题 材 。 张 家 辉 、 鲍 起 静 、\n",
      "央 行 或 考 虑 再 度 升 息 。 若 加 拿 大 5 月 通 胀 压 力\n"
     ]
    }
   ],
   "source": [
    "%cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
    "print('res from CRNN')\n",
    "!head -n 5 dev.src\n",
    "print('postprocessed by CS2S:')\n",
    "!head -n 20 dev.trg.pred | tail -n 15\n",
    "print('ground truth:')\n",
    "!head -n 5 dev.trg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#val:221931\n",
      "cnt_em_crnn: 104954, cnt_em_cs2s: 174259\n",
      "em_crnn: 0.4729, em_cs2s: 0.7852\n",
      "dist_crnn: 91.2853, dist_cs2s: 91.6581\n"
     ]
    }
   ],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "with open('dev.src') as f:\n",
    "    dev_src = [''.join(l.strip().split()) for l in f.readlines()]\n",
    "with open('dev.trg') as f:\n",
    "    dev_trg = [''.join(l.strip().split()) for l in f.readlines()]\n",
    "with open('dev.trg.pred') as f:\n",
    "    dev_pred = [''.join(l.strip().split()[2:]) for i, l in enumerate(\n",
    "        f.readlines()[5:]) if (i-1) % 3 == 0]\n",
    "assert len(dev_src) == len(dev_trg) == len(dev_pred)\n",
    "cnt_em_crnn = sum([p == g for p, g in zip(dev_src, dev_trg)])\n",
    "cnt_em_cs2s = sum([p == g for p, g in zip(dev_pred, dev_trg)])\n",
    "dist_crnn = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_trg)]) / len(dev_src)\n",
    "dist_cs2s = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_pred)]) / len(dev_src)\n",
    "print(f'#val:{len(dev_src)}')\n",
    "print(f'cnt_em_crnn: {cnt_em_crnn}, cnt_em_cs2s: {cnt_em_cs2s}')\n",
    "print('em_crnn: {:.4f}, em_cs2s: {:.4f}'.format(\n",
    "    cnt_em_crnn / len(dev_src), cnt_em_cs2s / len(dev_src)))\n",
    "print('dist_crnn: {:.4f}, dist_cs2s: {:.4f}'.format(\n",
    "    100 * (1 - dist_crnn), 100 * (1 - dist_cs2s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#dev=327403\n",
      "EM_dev:210426, dev_acc:0.6427124980528583\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "from random import random\n",
    "\n",
    "prefix = '/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data'\n",
    "# !cp -v {prefix}/train_no_bpe.tok.src {prefix}/train_no_bpe.tok.src.old\n",
    "# !cp -v {prefix}/train_no_bpe.tok.trg {prefix}/train_no_bpe.tok.trg.old\n",
    "# !cp -v {prefix}/dev_no_bpe.tok.src {prefix}/dev_no_bpe.tok.src.old\n",
    "# !cp -v {prefix}/dev_no_bpe.tok.trg {prefix}/dev_no_bpe.tok.trg.old\n",
    "with open(prefix + '/dev_no_bpe.tok.src.old', 'r') as f:\n",
    "    val_src = [' '.join(l.strip()) for l in f.readlines()]\n",
    "with open(prefix + '/dev_no_bpe.tok.trg.old', 'r') as f:\n",
    "    val_trg = [' '.join(l.strip()) for l in f.readlines()]\n",
    "print(f'#dev={len(val_src)}')\n",
    "assert len(val_src) == len(val_trg)\n",
    "cnt_em_val = sum([s == t for s, t in zip(val_src, val_trg)])\n",
    "print(f'EM_dev:{cnt_em_val}, dev_acc:{cnt_em_val / len(val_src)}')\n",
    "\n",
    "with open(prefix + '/dev_no_bpe_no_ds.tok.src', 'w') as f:\n",
    "    f.writelines([l + '\\n' for l in val_src])\n",
    "with open(prefix + '/dev_no_bpe_no_ds.tok.trg', 'w') as f:\n",
    "    f.writelines([l + '\\n' for l in val_trg])\n",
    "print('done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ source ../paths.sh\n",
      "++++ dirname ../paths.sh\n",
      "+++ cd ..\n",
      "+++ pwd\n",
      "++ BASE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb\n",
      "++ DATA_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data\n",
      "++ MODEL_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/models\n",
      "++ SCRIPTS_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts\n",
      "++ SOFTWARE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software\n",
      "++ TRAINING_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training\n",
      "+ src_ext=src\n",
      "+ trg_ext=trg\n",
      "+ train_data_prefix=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train_no_bpe_no_ds.tok\n",
      "+ dev_data_prefix=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe_no_ds.tok\n",
      "+ SUBWORD_NMT=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/subword-nmt\n",
      "+ FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py\n",
      "+ mkdir -p processed/\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe_no_ds.tok.src processed/dev.src_no_ds\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe_no_ds.tok.trg processed/dev.trg_no_ds\n",
      "+ less processed/train.all.src\n",
      "+ less processed/train.all.trg\n",
      "+ python /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py/preprocess.py --source-lang src_no_ds --target-lang trg_no_ds --trainpref processed/train --validpref processed/dev --testpref processed/dev --destdir processed/bin\n",
      "Namespace(alignfile=None, destdir='processed/bin', joined_dictionary=False, nwordssrc=-1, nwordstgt=-1, only_source=False, output_format='binary', source_lang='src_no_ds', srcdict=None, target_lang='trg_no_ds', testpref='processed/dev', tgtdict=None, thresholdsrc=0, thresholdtgt=0, trainpref='processed/train', validpref='processed/dev')\n",
      "| [src_no_ds] Dictionary: 5417 types\n",
      "| [src_no_ds] processed/train.src_no_ds: 887581 sents, 18404200 tokens, 0.0% replaced by <unk>\n",
      "| [src_no_ds] Dictionary: 5417 types\n",
      "| [src_no_ds] processed/dev.src_no_ds: 327403 sents, 6816512 tokens, 0.00166% replaced by <unk>\n",
      "| [src_no_ds] Dictionary: 5417 types\n",
      "| [src_no_ds] processed/dev.src_no_ds: 327403 sents, 6816512 tokens, 0.00166% replaced by <unk>\n",
      "| [trg_no_ds] Dictionary: 5324 types\n",
      "| [trg_no_ds] processed/train.trg_no_ds: 887581 sents, 18639201 tokens, 0.0% replaced by <unk>\n",
      "| [trg_no_ds] Dictionary: 5324 types\n",
      "| [trg_no_ds] processed/dev.trg_no_ds: 327403 sents, 6875463 tokens, 0.00739% replaced by <unk>\n",
      "| [trg_no_ds] Dictionary: 5324 types\n",
      "| [trg_no_ds] processed/dev.trg_no_ds: 327403 sents, 6875463 tokens, 0.00739% replaced by <unk>\n",
      "| Wrote preprocessed data to processed/bin\n",
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/autograd/function.py:41: UserWarning: mark_shared_storage is deprecated. Tensors with shared storages are automatically tracked. Note that calls to `set_()` are not tracked\n",
      "  'mark_shared_storage is deprecated. '\n"
     ]
    }
   ],
   "source": [
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training && \\\n",
    "    chmod +x preprocess_no_bpe_no_ds.sh && ./preprocess_no_bpe_no_ds.sh\n",
    "!FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py \\\n",
    "    && root=$FAIRSEQPY/../.. \\\n",
    "    && env PYTHONPATH=$FAIRSEQPY:$PYTHONPATH CUDA_VISIBLE_DEVICES=\"2,3\" \\\n",
    "    python $FAIRSEQPY/interactive.py \\\n",
    "    --no-progress-bar \\\n",
    "    --path $root/training/models/mlconv_embed/model1/checkpoint_best.pt \\\n",
    "    --beam 12 $root/training/processed/bin/ < \\\n",
    "    $root/training/processed/dev.src_no_ds > $root/training/processed/dev.trg_no_ds.pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/autograd/function.py:41: UserWarning: mark_shared_storage is deprecated. Tensors with shared storages are automatically tracked. Note that calls to `set_()` are not tracked\r\n",
      "  'mark_shared_storage is deprecated. '\r\n"
     ]
    }
   ],
   "source": [
    "#     chmod +x preprocess_no_bpe_no_ds.sh && ./preprocess_no_bpe_no_ds.sh && \\\n",
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training && \\\n",
    "    FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq-py \\\n",
    "    && root=$FAIRSEQPY/../.. \\\n",
    "    && env PYTHONPATH=$FAIRSEQPY:$PYTHONPATH CUDA_VISIBLE_DEVICES=\"1,2,3\" \\\n",
    "    python $FAIRSEQPY/generate.py \\\n",
    "    ./processed/bin_cs2s_no_ds \\\n",
    "    --batch-size 384 \\\n",
    "    -s src_no_ds -t trg_no_ds \\\n",
    "    --no-progress-bar \\\n",
    "    --path $root/training/models/mlconv_embed/model1/checkpoint_best.pt \\\n",
    "    --beam 5 \\\n",
    "    > $root/training/processed/dev.trg_no_ds.pred_cs2s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
      "#val:327403\n",
      "cnt_em_crnn: 210205, cnt_em_cs2s: 277027\n",
      "em_crnn: 0.6420, em_cs2s: 0.8461\n",
      "dist_crnn: 94.0595, dist_cs2s: 94.3130\n"
     ]
    }
   ],
   "source": [
    "%cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
    "\n",
    "from Levenshtein import distance\n",
    "\n",
    "with open('dev.trg_no_ds.pred_cs2s') as f:\n",
    "    lines = f.readlines()\n",
    "    dev_src = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[5:-2]) if i % 5 == 0]\n",
    "    dev_trg = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[5:-2]) if (i-1) % 5 == 0]\n",
    "    dev_pred = [''.join(l.strip().split()[2:]) for i, l in enumerate(\n",
    "        lines[5:-2]) if (i-2) % 5 == 0]\n",
    "assert len(dev_src) == len(dev_trg) == len(dev_pred)\n",
    "cnt_em_crnn = sum([p == g for p, g in zip(dev_src, dev_trg)])\n",
    "cnt_em_cs2s = sum([p == g for p, g in zip(dev_pred, dev_trg)])\n",
    "dist_crnn = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_trg)]) / len(dev_src)\n",
    "dist_cs2s = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_pred)]) / len(dev_src)\n",
    "print(f'#val:{len(dev_src)}')\n",
    "print(f'cnt_em_crnn: {cnt_em_crnn}, cnt_em_cs2s: {cnt_em_cs2s}')\n",
    "print('em_crnn: {:.4f}, em_cs2s: {:.4f}'.format(\n",
    "    cnt_em_crnn / len(dev_src), cnt_em_cs2s / len(dev_src)))\n",
    "print('dist_crnn: {:.4f}, dist_cs2s: {:.4f}'.format(\n",
    "    100 * (1 - dist_crnn), 100 * (1 - dist_cs2s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
      "#val:327403\n",
      "cnt_em_crnn: 210426, cnt_em_cs2s: 277035\n",
      "em_crnn: 0.6427, em_cs2s: 0.8462\n",
      "dist_crnn: 94.0927, dist_cs2s: 94.3008\n"
     ]
    }
   ],
   "source": [
    "%cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
    "\n",
    "from Levenshtein import distance\n",
    "\n",
    "with open('dev.src_no_ds') as f:\n",
    "    dev_src = [''.join(l.strip().split()) for l in f.readlines()]\n",
    "with open('dev.trg_no_ds') as f:\n",
    "    dev_trg = [''.join(l.strip().split()) for l in f.readlines()]\n",
    "with open('dev.trg_no_ds.pred') as f:\n",
    "    dev_pred = [''.join(l.strip().split()[2:]) for i, l in enumerate(\n",
    "        f.readlines()[5:]) if (i-1) % 3 == 0]\n",
    "assert len(dev_src) == len(dev_trg) == len(dev_pred)\n",
    "cnt_em_crnn = sum([p == g for p, g in zip(dev_src, dev_trg)])\n",
    "cnt_em_cs2s = sum([p == g for p, g in zip(dev_pred, dev_trg)])\n",
    "dist_crnn = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_trg)]) / len(dev_src)\n",
    "dist_cs2s = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_pred)]) / len(dev_src)\n",
    "print(f'#val:{len(dev_src)}')\n",
    "print(f'cnt_em_crnn: {cnt_em_crnn}, cnt_em_cs2s: {cnt_em_cs2s}')\n",
    "print('em_crnn: {:.4f}, em_cs2s: {:.4f}'.format(\n",
    "    cnt_em_crnn / len(dev_src), cnt_em_cs2s / len(dev_src)))\n",
    "print('dist_crnn: {:.4f}, dist_cs2s: {:.4f}'.format(\n",
    "    100 * (1 - dist_crnn), 100 * (1 - dist_cs2s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n"
     ]
    }
   ],
   "source": [
    "%cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
    "with open('train.src') as f, open('train.bert.src_bert_nmt', 'w') as f2:\n",
    "    res = [(''.join(l.strip().split()) + '\\n') for l in f.readlines()]\n",
    "    f2.writelines(res)\n",
    "with open('dev.src_no_ds') as f, open('dev.bert.src_bert_nmt', 'w') as f2:\n",
    "    res = [(''.join(l.strip().split()) + '\\n') for l in f.readlines()]\n",
    "    f2.writelines(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ source ../paths.sh\n",
      "++++ dirname ../paths.sh\n",
      "+++ cd ..\n",
      "+++ pwd\n",
      "++ BASE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb\n",
      "++ DATA_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data\n",
      "++ MODEL_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/models\n",
      "++ SCRIPTS_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts\n",
      "++ SOFTWARE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software\n",
      "++ TRAINING_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training\n",
      "+ src_ext=src\n",
      "+ trg_ext=trg\n",
      "+ train_data_prefix=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train_no_bpe.tok\n",
      "+ dev_data_prefix=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe.tok\n",
      "+ FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt\n",
      "+ mkdir -p processed/\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe.tok.src processed/dev.src_bert_nmt\n",
      "+ cp /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe.tok.trg processed/dev.trg_bert_nmt\n",
      "+ less processed/train.all.src\n",
      "+ less processed/train.all.trg\n",
      "+ python /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt/preprocess.py --source-lang src_bert_nmt --target-lang trg_bert_nmt --trainpref processed/train --validpref processed/dev --testpref processed/dev --destdir processed/bin_bert_nmt --joined-dictionary --bert-model-name voidful/albert_chinese_tiny --workers 32\n",
      "Namespace(alignfile=None, bert_model_name='voidful/albert_chinese_tiny', cpu=False, criterion='cross_entropy', dataset_impl='cached', destdir='processed/bin_bert_nmt', fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, seed=1, source_lang='src_bert_nmt', srcdict=None, target_lang='trg_bert_nmt', task='translation', tbmf_wrapper=False, tensorboard_logdir='', testpref='processed/dev', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, trainpref='processed/train', user_dir=None, validpref='processed/dev', workers=32)\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt/preprocess.py\", line 277, in <module>\n",
      "    cli_main()\n",
      "  File \"/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt/preprocess.py\", line 273, in cli_main\n",
      "    main(args)\n",
      "  File \"/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt/preprocess.py\", line 61, in main\n",
      "    raise FileExistsError(dict_path(args.source_lang))\n",
      "FileExistsError: processed/bin_bert_nmt/dict.src_bert_nmt.txt\n"
     ]
    }
   ],
   "source": [
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training && \\\n",
    "    chmod +x preprocess_bert_nmt_no_bpe.sh && ./preprocess_bert_nmt_no_bpe.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ set -e\n",
      "+ source ../paths.sh\n",
      "++++ dirname ../paths.sh\n",
      "+++ cd ..\n",
      "+++ pwd\n",
      "++ BASE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb\n",
      "++ DATA_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data\n",
      "++ MODEL_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/models\n",
      "++ SCRIPTS_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts\n",
      "++ SOFTWARE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software\n",
      "++ TRAINING_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training\n",
      "+ FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq\n",
      "+ src=src_bert_nmt\n",
      "+ tgt=trg_bert_nmt\n",
      "+ DATAPATH=processed/bin_bert_nmt\n",
      "+ SAVEDIR=checkpoints/pretrained_nmt\n",
      "+ mkdir -p checkpoints/pretrained_nmt\n",
      "+ env PYTHONPATH=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq: CUDA_VISIBLE_DEVICES=1,2,3 python /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq/train.py processed/bin_bert_nmt --arch transformer_iwslt_de_en --share-decoder-input-output-embed --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 --dropout 0.3 --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --label-smoothing 0.1 --save-dir checkpoints/pretrained_nmt -s src_bert_nmt -t trg_bert_nmt --fp16 --batch-size 256 --max-tokens 4096\n",
      "+ tee -a checkpoints/pretrained_nmt/training.log\n",
      "| distributed init (rank 0): tcp://localhost:15330\n",
      "| distributed init (rank 1): tcp://localhost:15330\n",
      "| initialized host 199 as rank 1\n",
      "| distributed init (rank 2): tcp://localhost:15330\n",
      "| initialized host 199 as rank 2\n",
      "| initialized host 199 as rank 0\n",
      "Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, all_gather_list_size=16384, arch='transformer_iwslt_de_en', attention_dropout=0.0, best_checkpoint_metric='loss', bpe=None, bucket_cap_mb=25, clip_norm=0.0, cpu=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='processed/bin_bert_nmt', dataset_impl=None, ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:15330', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=3, dropout=0.3, empty_cache_freq=0, encoder_attention_heads=4, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, fast_stat_sync=False, find_unused_parameters=False, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, layer_wise_attention=False, layernorm_embedding=False, left_pad_source='True', left_pad_target='False', load_alignments=False, log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_sentences=256, max_sentences_valid=256, max_source_positions=1024, max_target_positions=1024, max_tokens=4096, max_tokens_valid=4096, max_update=0, maximize_best_checkpoint_metric=False, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=-1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_token_positional_embeddings=False, num_workers=1, optimizer='adam', optimizer_overrides='{}', patience=-1, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/pretrained_nmt', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=True, skip_invalid_size_inputs_valid_test=False, source_lang='src_bert_nmt', target_lang='trg_bert_nmt', task='translation', tensorboard_logdir='', threshold_loss_scale=None, tokenizer=None, train_subset='train', truncate_source=False, update_freq=[1], upsample_primary=1, use_bmuf=False, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0001)\n",
      "| [src_bert_nmt] dictionary: 5544 types\n",
      "| [trg_bert_nmt] dictionary: 5544 types\n",
      "| loaded 221931 examples from: processed/bin_bert_nmt/valid.src_bert_nmt-trg_bert_nmt.src_bert_nmt\n",
      "| loaded 221931 examples from: processed/bin_bert_nmt/valid.src_bert_nmt-trg_bert_nmt.trg_bert_nmt\n",
      "| processed/bin_bert_nmt valid src_bert_nmt-trg_bert_nmt 221931 examples\n",
      "TransformerModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embed_tokens): Embedding(5544, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed_tokens): Embedding(5544, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 001:   0%|                                     | 0/1542 [00:00<?, ?it/s]\n",
      "| model transformer_iwslt_de_en, criterion LabelSmoothedCrossEntropyCriterion\n",
      "| num. model params: 37220352 (num. trained: 37220352)\n",
      "| training on 3 GPUs\n",
      "| max tokens per GPU = 4096 and max sentences per GPU = 256\n",
      "| no existing checkpoint found checkpoints/pretrained_nmt/checkpoint_last.pt\n",
      "| loading train data for epoch 0\n",
      "| loaded 887581 examples from: processed/bin_bert_nmt/train.src_bert_nmt-trg_bert_nmt.src_bert_nmt\n",
      "| loaded 887581 examples from: processed/bin_bert_nmt/train.src_bert_nmt-trg_bert_nmt.trg_bert_nmt\n",
      "| processed/bin_bert_nmt train src_bert_nmt-trg_bert_nmt 887581 examples\n",
      "| epoch 001 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 64.0\n",
      "| WARNING: overflow detected, setting loss scale to: 32.0\n",
      "| WARNING: overflow detected, setting loss scale to: 16.0\n",
      "| epoch 001 | loss 7.036 | nll_loss 6.147 | ppl 70.85 | wps 109857 | ups 9 | wpb 12087.663 | bsz 575.603 | num_updates 1539 | lr 0.000192375 | gnorm 1.970 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 183 | train_wall 163\n",
      "| epoch 001 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 001 | loss 7.036 | nll_loss 6.147 | ppl 70.85 | wps 109856 | ups 9 | wpb 12087.663 | bsz 575.603 | num_updates 1539 | lr 0.000192375 | gnorm 1.970 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 183 | train_wall 163\n",
      "| epoch 001 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 001 | loss 7.036 | nll_loss 6.147 | ppl 70.85 | wps 109848 | ups 9 | wpb 12087.663 | bsz 575.603 | num_updates 1539 | lr 0.000192375 | gnorm 1.970 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 183 | train_wall 162\n",
      "| epoch 002:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 001 | valid on 'valid' subset | loss 3.365 | nll_loss 1.343 | ppl 2.54 | num_updates 1539\n",
      "| epoch 002:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 001 | valid on 'valid' subset | loss 3.365 | nll_loss 1.343 | ppl 2.54 | num_updates 1539\n",
      "| epoch 002:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 001 | valid on 'valid' subset | loss 3.365 | nll_loss 1.343 | ppl 2.54 | num_updates 1539\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint1.pt (epoch 1 @ 1539 updates) (writing took 1.0407726764678955 seconds)\n",
      "| epoch 002 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 002 | loss 2.579 | nll_loss 1.024 | ppl 2.03 | wps 104105 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 3081 | lr 0.000385125 | gnorm 1.256 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 385 | train_wall 334\n",
      "| epoch 002 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 002 | loss 2.579 | nll_loss 1.024 | ppl 2.03 | wps 104076 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 3081 | lr 0.000385125 | gnorm 1.256 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 385 | train_wall 334\n",
      "| epoch 002 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 002 | loss 2.579 | nll_loss 1.024 | ppl 2.03 | wps 104067 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 3081 | lr 0.000385125 | gnorm 1.256 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 384 | train_wall 335\n",
      "| epoch 003:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 002 | valid on 'valid' subset | loss 2.525 | nll_loss 0.639 | ppl 1.56 | num_updates 3081 | best_loss 2.5253\n",
      "| epoch 003:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 002 | valid on 'valid' subset | loss 2.525 | nll_loss 0.639 | ppl 1.56 | num_updates 3081 | best_loss 2.5253\n",
      "| epoch 003:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 002 | valid on 'valid' subset | loss 2.525 | nll_loss 0.639 | ppl 1.56 | num_updates 3081 | best_loss 2.5253\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint2.pt (epoch 2 @ 3081 updates) (writing took 5.31175422668457 seconds)\n",
      "| epoch 003 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 003 | loss 2.315 | nll_loss 0.760 | ppl 1.69 | wps 108861 | ups 9 | wpb 12087.668 | bsz 575.603 | num_updates 4621 | lr 0.000465192 | gnorm 0.543 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 583 | train_wall 503\n",
      "| epoch 003 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 003 | loss 2.315 | nll_loss 0.760 | ppl 1.69 | wps 108860 | ups 9 | wpb 12087.668 | bsz 575.603 | num_updates 4621 | lr 0.000465192 | gnorm 0.543 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 583 | train_wall 504\n",
      "| WARNING: overflow detected, setting loss scale to: 8.0\n",
      "| WARNING: overflow detected, setting loss scale to: 4.0\n",
      "| epoch 003 | loss 2.315 | nll_loss 0.760 | ppl 1.69 | wps 108860 | ups 9 | wpb 12087.668 | bsz 575.603 | num_updates 4621 | lr 0.000465192 | gnorm 0.543 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 583 | train_wall 498\n",
      "| epoch 004:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 003 | valid on 'valid' subset | loss 2.347 | nll_loss 0.559 | ppl 1.47 | num_updates 4621 | best_loss 2.34683\n",
      "| epoch 004:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 003 | valid on 'valid' subset | loss 2.347 | nll_loss 0.559 | ppl 1.47 | num_updates 4621 | best_loss 2.34683\n",
      "| epoch 004:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 003 | valid on 'valid' subset | loss 2.347 | nll_loss 0.559 | ppl 1.47 | num_updates 4621 | best_loss 2.34683\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint3.pt (epoch 3 @ 4621 updates) (writing took 5.791347026824951 seconds)\n",
      "| epoch 004 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 004 | loss 2.202 | nll_loss 0.654 | ppl 1.57 | wps 115123 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 6163 | lr 0.000402813 | gnorm 0.400 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 770 | train_wall 664\n",
      "| epoch 004 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 004 | loss 2.202 | nll_loss 0.654 | ppl 1.57 | wps 115106 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 6163 | lr 0.000402813 | gnorm 0.400 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 770 | train_wall 663\n",
      "| epoch 004 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 004 | loss 2.202 | nll_loss 0.654 | ppl 1.57 | wps 115106 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 6163 | lr 0.000402813 | gnorm 0.400 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 771 | train_wall 652\n",
      "| epoch 005:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 004 | valid on 'valid' subset | loss 2.260 | nll_loss 0.492 | ppl 1.41 | num_updates 6163 | best_loss 2.26002\n",
      "| epoch 005:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 004 | valid on 'valid' subset | loss 2.260 | nll_loss 0.492 | ppl 1.41 | num_updates 6163 | best_loss 2.26002\n",
      "| epoch 005:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 004 | valid on 'valid' subset | loss 2.260 | nll_loss 0.492 | ppl 1.41 | num_updates 6163 | best_loss 2.26002\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint4.pt (epoch 4 @ 6163 updates) (writing took 5.65480637550354 seconds)\n",
      "| epoch 005 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 005 | loss 2.140 | nll_loss 0.594 | ppl 1.51 | wps 105148 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 7705 | lr 0.000360258 | gnorm 0.329 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 974 | train_wall 822\n",
      "| epoch 005 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 005 | loss 2.140 | nll_loss 0.594 | ppl 1.51 | wps 105146 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 7705 | lr 0.000360258 | gnorm 0.329 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 974 | train_wall 839\n",
      "| epoch 005 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 005 | loss 2.140 | nll_loss 0.594 | ppl 1.51 | wps 105130 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 7705 | lr 0.000360258 | gnorm 0.329 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 974 | train_wall 840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 006:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 005 | valid on 'valid' subset | loss 2.206 | nll_loss 0.457 | ppl 1.37 | num_updates 7705 | best_loss 2.20572\n",
      "| epoch 006:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 005 | valid on 'valid' subset | loss 2.206 | nll_loss 0.457 | ppl 1.37 | num_updates 7705 | best_loss 2.20572\n",
      "| epoch 006:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 005 | valid on 'valid' subset | loss 2.206 | nll_loss 0.457 | ppl 1.37 | num_updates 7705 | best_loss 2.20572\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint5.pt (epoch 5 @ 7705 updates) (writing took 5.656879663467407 seconds)\n",
      "| epoch 006 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 006 | loss 2.101 | nll_loss 0.554 | ppl 1.47 | wps 104253 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 9247 | lr 0.000328851 | gnorm 0.296 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 1181 | train_wall 993\n",
      "| epoch 006 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 006 | loss 2.101 | nll_loss 0.554 | ppl 1.47 | wps 104236 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 9247 | lr 0.000328851 | gnorm 0.296 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 1180 | train_wall 1017\n",
      "| epoch 006 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 006 | loss 2.101 | nll_loss 0.554 | ppl 1.47 | wps 104232 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 9247 | lr 0.000328851 | gnorm 0.296 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 1180 | train_wall 1016\n",
      "| epoch 007:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 006 | valid on 'valid' subset | loss 2.175 | nll_loss 0.430 | ppl 1.35 | num_updates 9247 | best_loss 2.17489\n",
      "| epoch 007:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 006 | valid on 'valid' subset | loss 2.175 | nll_loss 0.430 | ppl 1.35 | num_updates 9247 | best_loss 2.17489\n",
      "| epoch 007:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 006 | valid on 'valid' subset | loss 2.175 | nll_loss 0.430 | ppl 1.35 | num_updates 9247 | best_loss 2.17489\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint6.pt (epoch 6 @ 9247 updates) (writing took 5.69322395324707 seconds)\n",
      "| epoch 007 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 007 | loss 2.072 | nll_loss 0.525 | ppl 1.44 | wps 105095 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 10789 | lr 0.000304445 | gnorm 0.265 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 1385 | train_wall 1191\n",
      "| epoch 007 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 007 | loss 2.072 | nll_loss 0.525 | ppl 1.44 | wps 105106 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 10789 | lr 0.000304445 | gnorm 0.265 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 1385 | train_wall 1163\n",
      "| epoch 007 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 007 | loss 2.072 | nll_loss 0.525 | ppl 1.44 | wps 105094 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 10789 | lr 0.000304445 | gnorm 0.265 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 1385 | train_wall 1192\n",
      "| epoch 008:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 007 | valid on 'valid' subset | loss 2.153 | nll_loss 0.414 | ppl 1.33 | num_updates 10789 | best_loss 2.15295\n",
      "| epoch 008:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 007 | valid on 'valid' subset | loss 2.153 | nll_loss 0.414 | ppl 1.33 | num_updates 10789 | best_loss 2.15295\n",
      "| epoch 008:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 007 | valid on 'valid' subset | loss 2.153 | nll_loss 0.414 | ppl 1.33 | num_updates 10789 | best_loss 2.15295\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint7.pt (epoch 7 @ 10789 updates) (writing took 5.823526382446289 seconds)\n",
      "| epoch 008 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 008 | loss 2.052 | nll_loss 0.504 | ppl 1.42 | wps 106880 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 12331 | lr 0.000284774 | gnorm 0.241 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 1586 | train_wall 1365\n",
      "| epoch 008 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 008 | loss 2.052 | nll_loss 0.504 | ppl 1.42 | wps 106865 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 12331 | lr 0.000284774 | gnorm 0.241 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 1586 | train_wall 1364\n",
      "| epoch 008 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 008 | loss 2.052 | nll_loss 0.504 | ppl 1.42 | wps 106863 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 12331 | lr 0.000284774 | gnorm 0.241 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 1587 | train_wall 1330\n",
      "| epoch 009:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 008 | valid on 'valid' subset | loss 2.132 | nll_loss 0.396 | ppl 1.32 | num_updates 12331 | best_loss 2.13183\n",
      "| epoch 009:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 008 | valid on 'valid' subset | loss 2.132 | nll_loss 0.396 | ppl 1.32 | num_updates 12331 | best_loss 2.13183\n",
      "| epoch 009:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 008 | valid on 'valid' subset | loss 2.132 | nll_loss 0.396 | ppl 1.32 | num_updates 12331 | best_loss 2.13183\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint8.pt (epoch 8 @ 12331 updates) (writing took 5.785054683685303 seconds)\n",
      "| epoch 009 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 009 | loss 2.035 | nll_loss 0.486 | ppl 1.4 | wps 104597 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 13873 | lr 0.000268482 | gnorm 0.226 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 1793 | train_wall 1541\n",
      "| epoch 009 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 009 | loss 2.035 | nll_loss 0.486 | ppl 1.4 | wps 104590 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 13873 | lr 0.000268482 | gnorm 0.226 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 1792 | train_wall 1541\n",
      "| epoch 009 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 009 | loss 2.035 | nll_loss 0.486 | ppl 1.4 | wps 104574 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 13873 | lr 0.000268482 | gnorm 0.226 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 1793 | train_wall 1500\n",
      "| epoch 010:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 009 | valid on 'valid' subset | loss 2.109 | nll_loss 0.388 | ppl 1.31 | num_updates 13873 | best_loss 2.10946\n",
      "| epoch 010:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 009 | valid on 'valid' subset | loss 2.109 | nll_loss 0.388 | ppl 1.31 | num_updates 13873 | best_loss 2.10946\n",
      "| epoch 010:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 009 | valid on 'valid' subset | loss 2.109 | nll_loss 0.388 | ppl 1.31 | num_updates 13873 | best_loss 2.10946\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint9.pt (epoch 9 @ 13873 updates) (writing took 5.583969831466675 seconds)\n",
      "| epoch 010 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 010 | loss 2.022 | nll_loss 0.473 | ppl 1.39 | wps 104158 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 15415 | lr 0.0002547 | gnorm 0.222 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 2000 | train_wall 1718\n",
      "| epoch 010 | loss 2.022 | nll_loss 0.473 | ppl 1.39 | wps 104159 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 15415 | lr 0.0002547 | gnorm 0.222 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 2000 | train_wall 1671\n",
      "| epoch 010 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 010 | loss 2.022 | nll_loss 0.473 | ppl 1.39 | wps 104162 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 15415 | lr 0.0002547 | gnorm 0.222 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 1999 | train_wall 1718\n",
      "| epoch 011:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 010 | valid on 'valid' subset | loss 2.103 | nll_loss 0.382 | ppl 1.3 | num_updates 15415 | best_loss 2.10345\n",
      "| epoch 011:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 010 | valid on 'valid' subset | loss 2.103 | nll_loss 0.382 | ppl 1.3 | num_updates 15415 | best_loss 2.10345\n",
      "| epoch 011:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 010 | valid on 'valid' subset | loss 2.103 | nll_loss 0.382 | ppl 1.3 | num_updates 15415 | best_loss 2.10345\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint10.pt (epoch 10 @ 15415 updates) (writing took 5.514180421829224 seconds)\n",
      "| epoch 011 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 011 | loss 2.012 | nll_loss 0.462 | ppl 1.38 | wps 111486 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 16956 | lr 0.00024285 | gnorm 0.205 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 2196 | train_wall 1885\n",
      "| epoch 011 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 8.0\n",
      "| epoch 011 | loss 2.012 | nll_loss 0.462 | ppl 1.38 | wps 111457 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 16956 | lr 0.00024285 | gnorm 0.205 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 2196 | train_wall 1831\n",
      "| epoch 011 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 011 | loss 2.012 | nll_loss 0.462 | ppl 1.38 | wps 111454 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 16956 | lr 0.00024285 | gnorm 0.205 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 2195 | train_wall 1884\n",
      "| epoch 012:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 011 | valid on 'valid' subset | loss 2.087 | nll_loss 0.373 | ppl 1.3 | num_updates 16956 | best_loss 2.08656\n",
      "| epoch 012:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 011 | valid on 'valid' subset | loss 2.087 | nll_loss 0.373 | ppl 1.3 | num_updates 16956 | best_loss 2.08656\n",
      "| epoch 012:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 011 | valid on 'valid' subset | loss 2.087 | nll_loss 0.373 | ppl 1.3 | num_updates 16956 | best_loss 2.08656\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint11.pt (epoch 11 @ 16956 updates) (writing took 5.717398166656494 seconds)\n",
      "| epoch 012 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 012 | loss 2.002 | nll_loss 0.452 | ppl 1.37 | wps 116310 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 18498 | lr 0.000232508 | gnorm 0.200 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 2382 | train_wall 2045\n",
      "| epoch 012 | loss 2.002 | nll_loss 0.452 | ppl 1.37 | wps 116298 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 18498 | lr 0.000232508 | gnorm 0.200 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 2382 | train_wall 2044\n",
      "| epoch 012 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 012 | loss 2.002 | nll_loss 0.452 | ppl 1.37 | wps 116275 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 18498 | lr 0.000232508 | gnorm 0.200 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 2382 | train_wall 1984\n",
      "| epoch 013:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 012 | valid on 'valid' subset | loss 2.081 | nll_loss 0.367 | ppl 1.29 | num_updates 18498 | best_loss 2.08078\n",
      "| epoch 013:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 012 | valid on 'valid' subset | loss 2.081 | nll_loss 0.367 | ppl 1.29 | num_updates 18498 | best_loss 2.08078\n",
      "| epoch 013:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 012 | valid on 'valid' subset | loss 2.081 | nll_loss 0.367 | ppl 1.29 | num_updates 18498 | best_loss 2.08078\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint12.pt (epoch 12 @ 18498 updates) (writing took 5.261749505996704 seconds)\n",
      "| epoch 013 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 013 | loss 1.994 | nll_loss 0.444 | ppl 1.36 | wps 106344 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 20040 | lr 0.000223384 | gnorm 0.199 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 2583 | train_wall 2152\n",
      "| epoch 013 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 013 | loss 1.994 | nll_loss 0.444 | ppl 1.36 | wps 106329 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 20040 | lr 0.000223384 | gnorm 0.199 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 2582 | train_wall 2218\n",
      "| epoch 013 | loss 1.994 | nll_loss 0.444 | ppl 1.36 | wps 106330 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 20040 | lr 0.000223384 | gnorm 0.199 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 2582 | train_wall 2216\n",
      "| epoch 014:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 013 | valid on 'valid' subset | loss 2.075 | nll_loss 0.364 | ppl 1.29 | num_updates 20040 | best_loss 2.07547\n",
      "| epoch 014:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 013 | valid on 'valid' subset | loss 2.075 | nll_loss 0.364 | ppl 1.29 | num_updates 20040 | best_loss 2.07547\n",
      "| epoch 014:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 013 | valid on 'valid' subset | loss 2.075 | nll_loss 0.364 | ppl 1.29 | num_updates 20040 | best_loss 2.07547\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint13.pt (epoch 13 @ 20040 updates) (writing took 5.375784397125244 seconds)\n",
      "| epoch 014 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 4.0\n",
      "| epoch 014 | loss 1.987 | nll_loss 0.437 | ppl 1.35 | wps 105563 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 21581 | lr 0.00021526 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 2787 | train_wall 2321\n",
      "| epoch 014 | loss 1.987 | nll_loss 0.437 | ppl 1.35 | wps 105565 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 21581 | lr 0.00021526 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 2786 | train_wall 2391\n",
      "| epoch 014 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 014 | loss 1.987 | nll_loss 0.437 | ppl 1.35 | wps 105559 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 21581 | lr 0.00021526 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 2786 | train_wall 2392\n",
      "| epoch 015:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 014 | valid on 'valid' subset | loss 2.070 | nll_loss 0.362 | ppl 1.28 | num_updates 21581 | best_loss 2.07001\n",
      "| epoch 015:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 014 | valid on 'valid' subset | loss 2.070 | nll_loss 0.362 | ppl 1.28 | num_updates 21581 | best_loss 2.07001\n",
      "| epoch 015:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 014 | valid on 'valid' subset | loss 2.070 | nll_loss 0.362 | ppl 1.28 | num_updates 21581 | best_loss 2.07001\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint14.pt (epoch 14 @ 21581 updates) (writing took 5.475023984909058 seconds)\n",
      "| epoch 015 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 015 | loss 1.981 | nll_loss 0.430 | ppl 1.35 | wps 105215 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 23123 | lr 0.000207959 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 2991 | train_wall 2490\n",
      "| epoch 015 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 015 | loss 1.981 | nll_loss 0.430 | ppl 1.35 | wps 105203 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 23123 | lr 0.000207959 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 2990 | train_wall 2566\n",
      "| epoch 015 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 015 | loss 1.981 | nll_loss 0.430 | ppl 1.35 | wps 105189 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 23123 | lr 0.000207959 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 2991 | train_wall 2567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 016:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 015 | valid on 'valid' subset | loss 2.064 | nll_loss 0.360 | ppl 1.28 | num_updates 23123 | best_loss 2.06442\n",
      "| epoch 016:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 015 | valid on 'valid' subset | loss 2.064 | nll_loss 0.360 | ppl 1.28 | num_updates 23123 | best_loss 2.06442\n",
      "| epoch 016:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 015 | valid on 'valid' subset | loss 2.064 | nll_loss 0.360 | ppl 1.28 | num_updates 23123 | best_loss 2.06442\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint15.pt (epoch 15 @ 23123 updates) (writing took 5.361370801925659 seconds)\n",
      "| epoch 016 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 016 | loss 1.975 | nll_loss 0.424 | ppl 1.34 | wps 106654 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 24665 | lr 0.000201354 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 3192 | train_wall 2741\n",
      "| epoch 016 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 016 | loss 1.975 | nll_loss 0.424 | ppl 1.34 | wps 106643 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 24665 | lr 0.000201354 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 3192 | train_wall 2739\n",
      "| epoch 016 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 016 | loss 1.975 | nll_loss 0.424 | ppl 1.34 | wps 106648 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 24665 | lr 0.000201354 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 3192 | train_wall 2657\n",
      "| epoch 017:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 016 | valid on 'valid' subset | loss 2.062 | nll_loss 0.360 | ppl 1.28 | num_updates 24665 | best_loss 2.06169\n",
      "| epoch 017:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 016 | valid on 'valid' subset | loss 2.062 | nll_loss 0.360 | ppl 1.28 | num_updates 24665 | best_loss 2.06169\n",
      "| epoch 017:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 016 | valid on 'valid' subset | loss 2.062 | nll_loss 0.360 | ppl 1.28 | num_updates 24665 | best_loss 2.06169\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint16.pt (epoch 16 @ 24665 updates) (writing took 5.5586323738098145 seconds)\n",
      "| epoch 017 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 017 | loss 1.970 | nll_loss 0.418 | ppl 1.34 | wps 103361 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 26207 | lr 0.00019534 | gnorm 0.190 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 3400 | train_wall 2830\n",
      "| epoch 017 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 017 | loss 1.970 | nll_loss 0.418 | ppl 1.34 | wps 103358 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 26207 | lr 0.00019534 | gnorm 0.190 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 3399 | train_wall 2917\n",
      "| epoch 017 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 017 | loss 1.970 | nll_loss 0.418 | ppl 1.34 | wps 103350 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 26207 | lr 0.00019534 | gnorm 0.190 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 3399 | train_wall 2918\n",
      "| epoch 018:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 017 | valid on 'valid' subset | loss 2.056 | nll_loss 0.355 | ppl 1.28 | num_updates 26207 | best_loss 2.05644\n",
      "| epoch 018:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 017 | valid on 'valid' subset | loss 2.056 | nll_loss 0.355 | ppl 1.28 | num_updates 26207 | best_loss 2.05644\n",
      "| epoch 018:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 017 | valid on 'valid' subset | loss 2.056 | nll_loss 0.355 | ppl 1.28 | num_updates 26207 | best_loss 2.05644\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint17.pt (epoch 17 @ 26207 updates) (writing took 5.656672239303589 seconds)\n",
      "| epoch 018 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 018 | loss 1.965 | nll_loss 0.414 | ppl 1.33 | wps 106427 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 27749 | lr 0.000189835 | gnorm 0.184 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 3602 | train_wall 3092\n",
      "| epoch 018 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 018 | loss 1.965 | nll_loss 0.414 | ppl 1.33 | wps 106431 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 27749 | lr 0.000189835 | gnorm 0.184 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 3602 | train_wall 2997\n",
      "| epoch 018 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 018 | loss 1.965 | nll_loss 0.414 | ppl 1.33 | wps 106409 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 27749 | lr 0.000189835 | gnorm 0.184 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 3602 | train_wall 3090\n",
      "| epoch 019:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 018 | valid on 'valid' subset | loss 2.054 | nll_loss 0.354 | ppl 1.28 | num_updates 27749 | best_loss 2.05391\n",
      "| epoch 019:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 018 | valid on 'valid' subset | loss 2.054 | nll_loss 0.354 | ppl 1.28 | num_updates 27749 | best_loss 2.05391\n",
      "| epoch 019:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 018 | valid on 'valid' subset | loss 2.054 | nll_loss 0.354 | ppl 1.28 | num_updates 27749 | best_loss 2.05391\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint18.pt (epoch 18 @ 27749 updates) (writing took 5.5681397914886475 seconds)\n",
      "| epoch 019 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 019 | loss 1.961 | nll_loss 0.410 | ppl 1.33 | wps 110915 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 29291 | lr 0.000184771 | gnorm 0.178 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 3797 | train_wall 3256\n",
      "| epoch 019 | loss 1.961 | nll_loss 0.410 | ppl 1.33 | wps 110918 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 29291 | lr 0.000184771 | gnorm 0.178 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 3797 | train_wall 3158\n",
      "| epoch 019 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 019 | loss 1.961 | nll_loss 0.410 | ppl 1.33 | wps 110906 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 29291 | lr 0.000184771 | gnorm 0.178 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 3797 | train_wall 3258\n",
      "| epoch 020:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 019 | valid on 'valid' subset | loss 2.053 | nll_loss 0.354 | ppl 1.28 | num_updates 29291 | best_loss 2.05265\n",
      "| epoch 020:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 019 | valid on 'valid' subset | loss 2.053 | nll_loss 0.354 | ppl 1.28 | num_updates 29291 | best_loss 2.05265\n",
      "| epoch 020:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 019 | valid on 'valid' subset | loss 2.053 | nll_loss 0.354 | ppl 1.28 | num_updates 29291 | best_loss 2.05265\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint19.pt (epoch 19 @ 29291 updates) (writing took 6.9772255420684814 seconds)\n",
      "| epoch 020 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 020 | loss 1.957 | nll_loss 0.405 | ppl 1.32 | wps 115204 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 30833 | lr 0.000180091 | gnorm 0.184 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 3988 | train_wall 3413\n",
      "| epoch 020 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 020 | loss 1.957 | nll_loss 0.405 | ppl 1.32 | wps 115203 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 30833 | lr 0.000180091 | gnorm 0.184 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 3988 | train_wall 3410\n",
      "| epoch 020 | loss 1.957 | nll_loss 0.405 | ppl 1.32 | wps 115202 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 30833 | lr 0.000180091 | gnorm 0.184 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 3989 | train_wall 3312\n",
      "| epoch 021:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 020 | valid on 'valid' subset | loss 2.049 | nll_loss 0.352 | ppl 1.28 | num_updates 30833 | best_loss 2.0488\n",
      "| epoch 021:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 020 | valid on 'valid' subset | loss 2.049 | nll_loss 0.352 | ppl 1.28 | num_updates 30833 | best_loss 2.0488\n",
      "| epoch 021:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 020 | valid on 'valid' subset | loss 2.049 | nll_loss 0.352 | ppl 1.28 | num_updates 30833 | best_loss 2.0488\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint20.pt (epoch 20 @ 30833 updates) (writing took 5.813119173049927 seconds)\n",
      "| epoch 021 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 021 | loss 1.953 | nll_loss 0.401 | ppl 1.32 | wps 105945 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 32375 | lr 0.00017575 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 4191 | train_wall 3481\n",
      "| epoch 021 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 021 | loss 1.953 | nll_loss 0.401 | ppl 1.32 | wps 105934 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 32375 | lr 0.00017575 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 4190 | train_wall 3579\n",
      "| epoch 021 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 021 | loss 1.953 | nll_loss 0.401 | ppl 1.32 | wps 105923 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 32375 | lr 0.00017575 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 4191 | train_wall 3581\n",
      "| epoch 022:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 021 | valid on 'valid' subset | loss 2.047 | nll_loss 0.351 | ppl 1.28 | num_updates 32375 | best_loss 2.0471\n",
      "| epoch 022:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 021 | valid on 'valid' subset | loss 2.047 | nll_loss 0.351 | ppl 1.28 | num_updates 32375 | best_loss 2.0471\n",
      "| epoch 022:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 021 | valid on 'valid' subset | loss 2.047 | nll_loss 0.351 | ppl 1.28 | num_updates 32375 | best_loss 2.0471\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint21.pt (epoch 21 @ 32375 updates) (writing took 5.9866814613342285 seconds)\n",
      "| epoch 022 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 022 | loss 1.950 | nll_loss 0.397 | ppl 1.32 | wps 104416 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 33917 | lr 0.000171708 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 4397 | train_wall 3758\n",
      "| epoch 022 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 022 | loss 1.950 | nll_loss 0.397 | ppl 1.32 | wps 104404 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 33917 | lr 0.000171708 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 4398 | train_wall 3651\n",
      "| epoch 022 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 022 | loss 1.950 | nll_loss 0.397 | ppl 1.32 | wps 104405 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 33917 | lr 0.000171708 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 4397 | train_wall 3756\n",
      "| epoch 023:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 022 | valid on 'valid' subset | loss 2.044 | nll_loss 0.352 | ppl 1.28 | num_updates 33917 | best_loss 2.04427\n",
      "| epoch 023:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 022 | valid on 'valid' subset | loss 2.044 | nll_loss 0.352 | ppl 1.28 | num_updates 33917 | best_loss 2.04427\n",
      "| epoch 023:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 022 | valid on 'valid' subset | loss 2.044 | nll_loss 0.352 | ppl 1.28 | num_updates 33917 | best_loss 2.04427\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint22.pt (epoch 22 @ 33917 updates) (writing took 5.87335729598999 seconds)\n",
      "| epoch 023 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 023 | loss 1.946 | nll_loss 0.394 | ppl 1.31 | wps 103710 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 35458 | lr 0.000167936 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 4606 | train_wall 3936\n",
      "| epoch 023 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 023 | loss 1.946 | nll_loss 0.394 | ppl 1.31 | wps 103697 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 35458 | lr 0.000167936 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 4606 | train_wall 3933\n",
      "| epoch 023 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 8.0\n",
      "| epoch 023 | loss 1.946 | nll_loss 0.394 | ppl 1.31 | wps 103688 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 35458 | lr 0.000167936 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 4606 | train_wall 3823\n",
      "| epoch 024:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 023 | valid on 'valid' subset | loss 2.044 | nll_loss 0.350 | ppl 1.27 | num_updates 35458 | best_loss 2.04355\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint23.pt (epoch 23 @ 35458 updates) (writing took 13.788389682769775 seconds)\n",
      "| epoch 024:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 023 | valid on 'valid' subset | loss 2.044 | nll_loss 0.350 | ppl 1.27 | num_updates 35458 | best_loss 2.04355\n",
      "| epoch 024:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 023 | valid on 'valid' subset | loss 2.044 | nll_loss 0.350 | ppl 1.27 | num_updates 35458 | best_loss 2.04355\n",
      "| epoch 024 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 024 | loss 1.943 | nll_loss 0.390 | ppl 1.31 | wps 103727 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 37000 | lr 0.000164399 | gnorm 0.173 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 4852 | train_wall 4108\n",
      "| epoch 024 | loss 1.943 | nll_loss 0.390 | ppl 1.31 | wps 103730 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 37000 | lr 0.000164399 | gnorm 0.173 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 4852 | train_wall 4106\n",
      "| epoch 024 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 024 | loss 1.943 | nll_loss 0.390 | ppl 1.31 | wps 103715 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 37000 | lr 0.000164399 | gnorm 0.173 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 4852 | train_wall 3995\n",
      "| epoch 025:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 024 | valid on 'valid' subset | loss 2.044 | nll_loss 0.350 | ppl 1.27 | num_updates 37000 | best_loss 2.04355\n",
      "| epoch 025:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 024 | valid on 'valid' subset | loss 2.044 | nll_loss 0.350 | ppl 1.27 | num_updates 37000 | best_loss 2.04355\n",
      "| epoch 025:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 024 | valid on 'valid' subset | loss 2.044 | nll_loss 0.350 | ppl 1.27 | num_updates 37000 | best_loss 2.04355\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint24.pt (epoch 24 @ 37000 updates) (writing took 3.674582004547119 seconds)\n",
      "| epoch 025 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 025 | loss 1.940 | nll_loss 0.387 | ppl 1.31 | wps 104823 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 38542 | lr 0.000161077 | gnorm 0.178 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 5056 | train_wall 4282\n",
      "| epoch 025 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 025 | loss 1.940 | nll_loss 0.387 | ppl 1.31 | wps 104815 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 38542 | lr 0.000161077 | gnorm 0.178 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 5056 | train_wall 4280\n",
      "| epoch 025 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 025 | loss 1.940 | nll_loss 0.387 | ppl 1.31 | wps 104809 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 38542 | lr 0.000161077 | gnorm 0.178 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 5056 | train_wall 4165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 026:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 025 | valid on 'valid' subset | loss 2.042 | nll_loss 0.351 | ppl 1.28 | num_updates 38542 | best_loss 2.04248\n",
      "| epoch 026:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 025 | valid on 'valid' subset | loss 2.042 | nll_loss 0.351 | ppl 1.28 | num_updates 38542 | best_loss 2.04248\n",
      "| epoch 026:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 025 | valid on 'valid' subset | loss 2.042 | nll_loss 0.351 | ppl 1.28 | num_updates 38542 | best_loss 2.04248\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint25.pt (epoch 25 @ 38542 updates) (writing took 5.629516363143921 seconds)\n",
      "| epoch 026 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 026 | loss 1.937 | nll_loss 0.384 | ppl 1.3 | wps 104206 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 40084 | lr 0.000157948 | gnorm 0.178 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 5263 | train_wall 4336\n",
      "| epoch 026 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 026 | loss 1.937 | nll_loss 0.384 | ppl 1.3 | wps 104197 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 40084 | lr 0.000157948 | gnorm 0.178 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 5263 | train_wall 4456\n",
      "| epoch 026 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 026 | loss 1.937 | nll_loss 0.384 | ppl 1.3 | wps 104186 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 40084 | lr 0.000157948 | gnorm 0.178 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 5263 | train_wall 4458\n",
      "| epoch 027:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 026 | valid on 'valid' subset | loss 2.043 | nll_loss 0.349 | ppl 1.27 | num_updates 40084 | best_loss 2.04248\n",
      "| epoch 027:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 026 | valid on 'valid' subset | loss 2.043 | nll_loss 0.349 | ppl 1.27 | num_updates 40084 | best_loss 2.04248\n",
      "| epoch 027:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 026 | valid on 'valid' subset | loss 2.043 | nll_loss 0.349 | ppl 1.27 | num_updates 40084 | best_loss 2.04248\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint26.pt (epoch 26 @ 40084 updates) (writing took 3.7060413360595703 seconds)\n",
      "| epoch 027 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 8.0\n",
      "| epoch 027 | loss 1.934 | nll_loss 0.381 | ppl 1.3 | wps 116003 | ups 10 | wpb 12087.674 | bsz 575.604 | num_updates 41625 | lr 0.000154997 | gnorm 0.180 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 5450 | train_wall 4489\n",
      "| epoch 027 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 027 | loss 1.934 | nll_loss 0.381 | ppl 1.3 | wps 116000 | ups 10 | wpb 12087.674 | bsz 575.604 | num_updates 41625 | lr 0.000154997 | gnorm 0.180 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 5450 | train_wall 4616\n",
      "| epoch 027 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 027 | loss 1.934 | nll_loss 0.381 | ppl 1.3 | wps 115997 | ups 10 | wpb 12087.674 | bsz 575.604 | num_updates 41625 | lr 0.000154997 | gnorm 0.180 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 5450 | train_wall 4613\n",
      "| epoch 028:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 027 | valid on 'valid' subset | loss 2.043 | nll_loss 0.350 | ppl 1.27 | num_updates 41625 | best_loss 2.04248\n",
      "| epoch 028:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 027 | valid on 'valid' subset | loss 2.043 | nll_loss 0.350 | ppl 1.27 | num_updates 41625 | best_loss 2.04248\n",
      "| epoch 028:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 027 | valid on 'valid' subset | loss 2.043 | nll_loss 0.350 | ppl 1.27 | num_updates 41625 | best_loss 2.04248\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint27.pt (epoch 27 @ 41625 updates) (writing took 3.487349510192871 seconds)\n",
      "| epoch 028 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 028 | loss 1.932 | nll_loss 0.378 | ppl 1.3 | wps 115482 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 43167 | lr 0.000152203 | gnorm 0.177 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 5636 | train_wall 4774\n",
      "| epoch 028 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 028 | loss 1.932 | nll_loss 0.378 | ppl 1.3 | wps 115449 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 43167 | lr 0.000152203 | gnorm 0.177 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 5636 | train_wall 4643\n",
      "| epoch 028 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 028 | loss 1.932 | nll_loss 0.378 | ppl 1.3 | wps 115422 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 43167 | lr 0.000152203 | gnorm 0.177 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 5636 | train_wall 4771\n",
      "| epoch 029:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 028 | valid on 'valid' subset | loss 2.044 | nll_loss 0.349 | ppl 1.27 | num_updates 43167 | best_loss 2.04248\n",
      "| epoch 029:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 028 | valid on 'valid' subset | loss 2.044 | nll_loss 0.349 | ppl 1.27 | num_updates 43167 | best_loss 2.04248\n",
      "| epoch 029:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 028 | valid on 'valid' subset | loss 2.044 | nll_loss 0.349 | ppl 1.27 | num_updates 43167 | best_loss 2.04248\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint28.pt (epoch 28 @ 43167 updates) (writing took 3.2862088680267334 seconds)\n",
      "| epoch 029 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 4.0\n",
      "| epoch 029 | loss 1.930 | nll_loss 0.376 | ppl 1.3 | wps 103425 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 44708 | lr 0.000149557 | gnorm 0.180 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 5842 | train_wall 4815\n",
      "| epoch 029 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 029 | loss 1.930 | nll_loss 0.376 | ppl 1.3 | wps 103412 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 44708 | lr 0.000149557 | gnorm 0.180 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 5841 | train_wall 4946\n",
      "| epoch 029 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 029 | loss 1.930 | nll_loss 0.376 | ppl 1.3 | wps 103408 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 44708 | lr 0.000149557 | gnorm 0.180 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 5841 | train_wall 4950\n",
      "| epoch 030:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 029 | valid on 'valid' subset | loss 2.040 | nll_loss 0.348 | ppl 1.27 | num_updates 44708 | best_loss 2.04027\n",
      "| epoch 030:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 029 | valid on 'valid' subset | loss 2.040 | nll_loss 0.348 | ppl 1.27 | num_updates 44708 | best_loss 2.04027\n",
      "| epoch 030:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 029 | valid on 'valid' subset | loss 2.040 | nll_loss 0.348 | ppl 1.27 | num_updates 44708 | best_loss 2.04027\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint29.pt (epoch 29 @ 44708 updates) (writing took 6.264145135879517 seconds)\n",
      "| epoch 030 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 030 | loss 1.927 | nll_loss 0.373 | ppl 1.29 | wps 104778 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 46250 | lr 0.000147043 | gnorm 0.175 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 6064 | train_wall 5117\n",
      "| epoch 030 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 030 | loss 1.927 | nll_loss 0.373 | ppl 1.29 | wps 104773 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 46250 | lr 0.000147043 | gnorm 0.175 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 6064 | train_wall 5120\n",
      "| epoch 030 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 030 | loss 1.927 | nll_loss 0.373 | ppl 1.29 | wps 104769 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 46250 | lr 0.000147043 | gnorm 0.175 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 6064 | train_wall 4985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 031:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 030 | valid on 'valid' subset | loss 2.043 | nll_loss 0.350 | ppl 1.27 | num_updates 46250 | best_loss 2.04027\n",
      "| epoch 031:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 030 | valid on 'valid' subset | loss 2.043 | nll_loss 0.350 | ppl 1.27 | num_updates 46250 | best_loss 2.04027\n",
      "| epoch 031:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 030 | valid on 'valid' subset | loss 2.043 | nll_loss 0.350 | ppl 1.27 | num_updates 46250 | best_loss 2.04027\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint30.pt (epoch 30 @ 46250 updates) (writing took 3.533159017562866 seconds)\n",
      "| epoch 031 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 031 | loss 1.925 | nll_loss 0.370 | ppl 1.29 | wps 103556 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 47792 | lr 0.000144651 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 6270 | train_wall 5293\n",
      "| epoch 031 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 031 | loss 1.925 | nll_loss 0.370 | ppl 1.29 | wps 103553 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 47792 | lr 0.000144651 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 6270 | train_wall 5157\n",
      "| epoch 031 | loss 1.925 | nll_loss 0.370 | ppl 1.29 | wps 103561 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 47792 | lr 0.000144651 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 6270 | train_wall 5296\n",
      "| epoch 032:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 031 | valid on 'valid' subset | loss 2.040 | nll_loss 0.349 | ppl 1.27 | num_updates 47792 | best_loss 2.04018\n",
      "| epoch 032:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 031 | valid on 'valid' subset | loss 2.040 | nll_loss 0.349 | ppl 1.27 | num_updates 47792 | best_loss 2.04018\n",
      "| epoch 032:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 031 | valid on 'valid' subset | loss 2.040 | nll_loss 0.349 | ppl 1.27 | num_updates 47792 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint31.pt (epoch 31 @ 47792 updates) (writing took 5.91075587272644 seconds)\n",
      "| epoch 032 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 032 | loss 1.922 | nll_loss 0.368 | ppl 1.29 | wps 105894 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 49334 | lr 0.000142373 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 6475 | train_wall 5472\n",
      "| epoch 032 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 032 | loss 1.922 | nll_loss 0.368 | ppl 1.29 | wps 105861 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 49334 | lr 0.000142373 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 6475 | train_wall 5326\n",
      "| epoch 032 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 032 | loss 1.922 | nll_loss 0.368 | ppl 1.29 | wps 105860 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 49334 | lr 0.000142373 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 6474 | train_wall 5468\n",
      "| epoch 033:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 032 | valid on 'valid' subset | loss 2.043 | nll_loss 0.349 | ppl 1.27 | num_updates 49334 | best_loss 2.04018\n",
      "| epoch 033:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 032 | valid on 'valid' subset | loss 2.043 | nll_loss 0.349 | ppl 1.27 | num_updates 49334 | best_loss 2.04018\n",
      "| epoch 033:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 032 | valid on 'valid' subset | loss 2.043 | nll_loss 0.349 | ppl 1.27 | num_updates 49334 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint32.pt (epoch 32 @ 49334 updates) (writing took 3.182955503463745 seconds)\n",
      "| epoch 033 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 033 | loss 1.920 | nll_loss 0.365 | ppl 1.29 | wps 104786 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 50876 | lr 0.000140199 | gnorm 0.183 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 6678 | train_wall 5496\n",
      "| epoch 033 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 033 | loss 1.920 | nll_loss 0.365 | ppl 1.29 | wps 104780 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 50876 | lr 0.000140199 | gnorm 0.183 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 6678 | train_wall 5645\n",
      "| epoch 033 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 033 | loss 1.920 | nll_loss 0.365 | ppl 1.29 | wps 104774 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 50876 | lr 0.000140199 | gnorm 0.183 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 6678 | train_wall 5642\n",
      "| epoch 034:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 033 | valid on 'valid' subset | loss 2.042 | nll_loss 0.350 | ppl 1.27 | num_updates 50876 | best_loss 2.04018\n",
      "| epoch 034:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 033 | valid on 'valid' subset | loss 2.042 | nll_loss 0.350 | ppl 1.27 | num_updates 50876 | best_loss 2.04018\n",
      "| epoch 034:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 033 | valid on 'valid' subset | loss 2.042 | nll_loss 0.350 | ppl 1.27 | num_updates 50876 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint33.pt (epoch 33 @ 50876 updates) (writing took 3.7448673248291016 seconds)\n",
      "| epoch 034 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 4.0\n",
      "| epoch 034 | loss 1.918 | nll_loss 0.363 | ppl 1.29 | wps 102569 | ups 8 | wpb 12087.674 | bsz 575.604 | num_updates 52417 | lr 0.000138122 | gnorm 0.176 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 6887 | train_wall 5669\n",
      "| epoch 034 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 034 | loss 1.918 | nll_loss 0.363 | ppl 1.29 | wps 102571 | ups 8 | wpb 12087.674 | bsz 575.604 | num_updates 52417 | lr 0.000138122 | gnorm 0.176 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 6886 | train_wall 5822\n",
      "| epoch 034 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 034 | loss 1.918 | nll_loss 0.363 | ppl 1.29 | wps 102565 | ups 8 | wpb 12087.674 | bsz 575.604 | num_updates 52417 | lr 0.000138122 | gnorm 0.176 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 6886 | train_wall 5819\n",
      "| epoch 035:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 034 | valid on 'valid' subset | loss 2.041 | nll_loss 0.349 | ppl 1.27 | num_updates 52417 | best_loss 2.04018\n",
      "| epoch 035:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 034 | valid on 'valid' subset | loss 2.041 | nll_loss 0.349 | ppl 1.27 | num_updates 52417 | best_loss 2.04018\n",
      "| epoch 035:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 034 | valid on 'valid' subset | loss 2.041 | nll_loss 0.349 | ppl 1.27 | num_updates 52417 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint34.pt (epoch 34 @ 52417 updates) (writing took 3.7245171070098877 seconds)\n",
      "| epoch 035 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 035 | loss 1.916 | nll_loss 0.361 | ppl 1.28 | wps 116616 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 53959 | lr 0.000136134 | gnorm 0.182 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 7073 | train_wall 5975\n",
      "| epoch 035 | loss 1.916 | nll_loss 0.361 | ppl 1.28 | wps 116622 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 53959 | lr 0.000136134 | gnorm 0.182 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 7073 | train_wall 5822\n",
      "| epoch 035 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 035 | loss 1.916 | nll_loss 0.361 | ppl 1.28 | wps 116600 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 53959 | lr 0.000136134 | gnorm 0.182 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 7073 | train_wall 5978\n",
      "| epoch 036:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 035 | valid on 'valid' subset | loss 2.043 | nll_loss 0.351 | ppl 1.28 | num_updates 53959 | best_loss 2.04018\n",
      "| epoch 036:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 035 | valid on 'valid' subset | loss 2.043 | nll_loss 0.351 | ppl 1.28 | num_updates 53959 | best_loss 2.04018\n",
      "| epoch 036:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 035 | valid on 'valid' subset | loss 2.043 | nll_loss 0.351 | ppl 1.28 | num_updates 53959 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint35.pt (epoch 35 @ 53959 updates) (writing took 3.4866080284118652 seconds)\n",
      "| epoch 036 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 036 | loss 1.914 | nll_loss 0.358 | ppl 1.28 | wps 116060 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 55501 | lr 0.00013423 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 7259 | train_wall 6133\n",
      "| epoch 036 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 036 | loss 1.914 | nll_loss 0.358 | ppl 1.28 | wps 116037 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 55501 | lr 0.00013423 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 7259 | train_wall 5975\n",
      "| epoch 036 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 036 | loss 1.914 | nll_loss 0.358 | ppl 1.28 | wps 116012 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 55501 | lr 0.00013423 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 7259 | train_wall 6129\n",
      "| epoch 037:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 036 | valid on 'valid' subset | loss 2.042 | nll_loss 0.351 | ppl 1.28 | num_updates 55501 | best_loss 2.04018\n",
      "| epoch 037:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 036 | valid on 'valid' subset | loss 2.042 | nll_loss 0.351 | ppl 1.28 | num_updates 55501 | best_loss 2.04018\n",
      "| epoch 037:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 036 | valid on 'valid' subset | loss 2.042 | nll_loss 0.351 | ppl 1.28 | num_updates 55501 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint36.pt (epoch 36 @ 55501 updates) (writing took 3.642728567123413 seconds)\n",
      "| epoch 037 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 037 | loss 1.912 | nll_loss 0.357 | ppl 1.28 | wps 103486 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 57043 | lr 0.000132403 | gnorm 0.180 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 7466 | train_wall 6148\n",
      "| epoch 037 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 037 | loss 1.912 | nll_loss 0.357 | ppl 1.28 | wps 103482 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 57043 | lr 0.000132403 | gnorm 0.180 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 7466 | train_wall 6306\n",
      "| epoch 037 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 037 | loss 1.912 | nll_loss 0.357 | ppl 1.28 | wps 103467 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 57043 | lr 0.000132403 | gnorm 0.180 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 7466 | train_wall 6309\n",
      "| epoch 038:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 037 | valid on 'valid' subset | loss 2.044 | nll_loss 0.351 | ppl 1.28 | num_updates 57043 | best_loss 2.04018\n",
      "| epoch 037 | valid on 'valid' subset | loss 2.044 | nll_loss 0.351 | ppl 1.28 | num_updates 57043 | best_loss 2.04018\n",
      "| epoch 037 | valid on 'valid' subset | loss 2.044 | nll_loss 0.351 | ppl 1.28 | num_updates 57043 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint37.pt (epoch 37 @ 57043 updates) (writing took 3.4680583477020264 seconds)\n",
      "| epoch 038 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 038 | loss 1.910 | nll_loss 0.355 | ppl 1.28 | wps 104354 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 58585 | lr 0.000130649 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 7676 | train_wall 6319\n",
      "| epoch 038 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 038 | loss 1.910 | nll_loss 0.355 | ppl 1.28 | wps 104343 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 58585 | lr 0.000130649 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 7675 | train_wall 6477\n",
      "| epoch 038 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 038 | loss 1.910 | nll_loss 0.355 | ppl 1.28 | wps 104337 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 58585 | lr 0.000130649 | gnorm 0.179 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 7676 | train_wall 6480\n",
      "| epoch 039:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 038 | valid on 'valid' subset | loss 2.044 | nll_loss 0.353 | ppl 1.28 | num_updates 58585 | best_loss 2.04018\n",
      "| epoch 039:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 038 | valid on 'valid' subset | loss 2.044 | nll_loss 0.353 | ppl 1.28 | num_updates 58585 | best_loss 2.04018\n",
      "| epoch 039:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 038 | valid on 'valid' subset | loss 2.044 | nll_loss 0.353 | ppl 1.28 | num_updates 58585 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint38.pt (epoch 38 @ 58585 updates) (writing took 3.6078689098358154 seconds)\n",
      "| epoch 039 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 039 | loss 1.909 | nll_loss 0.353 | ppl 1.28 | wps 105388 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 60127 | lr 0.000128963 | gnorm 0.182 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 7879 | train_wall 6649\n",
      "| epoch 039 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 039 | loss 1.909 | nll_loss 0.353 | ppl 1.28 | wps 105382 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 60127 | lr 0.000128963 | gnorm 0.182 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 7879 | train_wall 6652\n",
      "| epoch 039 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 039 | loss 1.909 | nll_loss 0.353 | ppl 1.28 | wps 105373 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 60127 | lr 0.000128963 | gnorm 0.182 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 7879 | train_wall 6488\n",
      "| epoch 040:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 039 | valid on 'valid' subset | loss 2.043 | nll_loss 0.350 | ppl 1.27 | num_updates 60127 | best_loss 2.04018\n",
      "| epoch 040:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 039 | valid on 'valid' subset | loss 2.043 | nll_loss 0.350 | ppl 1.27 | num_updates 60127 | best_loss 2.04018\n",
      "| epoch 040:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 039 | valid on 'valid' subset | loss 2.043 | nll_loss 0.350 | ppl 1.27 | num_updates 60127 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint39.pt (epoch 39 @ 60127 updates) (writing took 3.4905285835266113 seconds)\n",
      "| epoch 040 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 040 | loss 1.907 | nll_loss 0.351 | ppl 1.28 | wps 101016 | ups 8 | wpb 12087.679 | bsz 575.604 | num_updates 61669 | lr 0.00012734 | gnorm 0.191 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 8089 | train_wall 6664\n",
      "| epoch 040 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 040 | loss 1.907 | nll_loss 0.351 | ppl 1.28 | wps 101036 | ups 8 | wpb 12087.679 | bsz 575.604 | num_updates 61669 | lr 0.00012734 | gnorm 0.191 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 8089 | train_wall 6832\n",
      "| epoch 040 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 040 | loss 1.907 | nll_loss 0.351 | ppl 1.28 | wps 101033 | ups 8 | wpb 12087.679 | bsz 575.604 | num_updates 61669 | lr 0.00012734 | gnorm 0.191 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 8089 | train_wall 6829\n",
      "| epoch 041:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 040 | valid on 'valid' subset | loss 2.044 | nll_loss 0.352 | ppl 1.28 | num_updates 61669 | best_loss 2.04018\n",
      "| epoch 041:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 040 | valid on 'valid' subset | loss 2.044 | nll_loss 0.352 | ppl 1.28 | num_updates 61669 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint40.pt (epoch 40 @ 61669 updates) (writing took 4.299867630004883 seconds)\n",
      "| epoch 041:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 040 | valid on 'valid' subset | loss 2.044 | nll_loss 0.352 | ppl 1.28 | num_updates 61669 | best_loss 2.04018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 041 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 8.023, nll_loss=0.36 041:  23%|▏| 360/1542 [01:06<04:17,  4.60it/s, loss=1.919, nll_loss=0.36 041:  41%|▍| 629/1542 [01:45<01:43,  8.79it/s, loss=1.899, nll_loss=0.34\n",
      "| epoch 041 | loss 1.905 | nll_loss 0.349 | ppl 1.27 | wps 72315 | ups 6 | wpb 12087.674 | bsz 575.604 | num_updates 63210 | lr 0.000125779 | gnorm 0.183 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 8392 | train_wall 6912\n",
      "| epoch 041 | loss 1.905 | nll_loss 0.349 | ppl 1.27 | wps 72313 | ups 6 | wpb 12087.674 | bsz 575.604 | num_updates 63210 | lr 0.000125779 | gnorm 0.183 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 8392 | train_wall 7078\n",
      "| epoch 041 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 041 | loss 1.905 | nll_loss 0.349 | ppl 1.27 | wps 72318 | ups 6 | wpb 12087.674 | bsz 575.604 | num_updates 63210 | lr 0.000125779 | gnorm 0.183 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 8392 | train_wall 7081\n",
      "| epoch 042:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 041 | valid on 'valid' subset | loss 2.045 | nll_loss 0.352 | ppl 1.28 | num_updates 63210 | best_loss 2.04018\n",
      "| epoch 042:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 041 | valid on 'valid' subset | loss 2.045 | nll_loss 0.352 | ppl 1.28 | num_updates 63210 | best_loss 2.04018\n",
      "| epoch 042:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 041 | valid on 'valid' subset | loss 2.045 | nll_loss 0.352 | ppl 1.28 | num_updates 63210 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint41.pt (epoch 41 @ 63210 updates) (writing took 4.934092998504639 seconds)\n",
      "| epoch 042 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 042 | loss 1.904 | nll_loss 0.348 | ppl 1.27 | wps 74269 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 64752 | lr 0.000124272 | gnorm 0.186 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 8687 | train_wall 7155\n",
      "| epoch 042 | loss 1.904 | nll_loss 0.348 | ppl 1.27 | wps 74270 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 64752 | lr 0.000124272 | gnorm 0.186 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 8686 | train_wall 7324\n",
      "| epoch 042 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 042 | loss 1.904 | nll_loss 0.348 | ppl 1.27 | wps 74269 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 64752 | lr 0.000124272 | gnorm 0.186 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 8686 | train_wall 7321\n",
      "| epoch 043:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 042 | valid on 'valid' subset | loss 2.044 | nll_loss 0.353 | ppl 1.28 | num_updates 64752 | best_loss 2.04018\n",
      "| epoch 043:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 042 | valid on 'valid' subset | loss 2.044 | nll_loss 0.353 | ppl 1.28 | num_updates 64752 | best_loss 2.04018\n",
      "| epoch 043:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 042 | valid on 'valid' subset | loss 2.044 | nll_loss 0.353 | ppl 1.28 | num_updates 64752 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint42.pt (epoch 42 @ 64752 updates) (writing took 3.502845048904419 seconds)\n",
      "| epoch 043 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 043 | loss 1.902 | nll_loss 0.346 | ppl 1.27 | wps 74906 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 66294 | lr 0.000122818 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 8969 | train_wall 7562\n",
      "| epoch 043 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 043 | loss 1.902 | nll_loss 0.346 | ppl 1.27 | wps 74891 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 66294 | lr 0.000122818 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 8970 | train_wall 7565\n",
      "| epoch 043 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 043 | loss 1.902 | nll_loss 0.346 | ppl 1.27 | wps 74876 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 66294 | lr 0.000122818 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 8970 | train_wall 7396\n",
      "| epoch 044:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 043 | valid on 'valid' subset | loss 2.046 | nll_loss 0.353 | ppl 1.28 | num_updates 66294 | best_loss 2.04018\n",
      "| epoch 044:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 043 | valid on 'valid' subset | loss 2.046 | nll_loss 0.353 | ppl 1.28 | num_updates 66294 | best_loss 2.04018\n",
      "| epoch 044:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 043 | valid on 'valid' subset | loss 2.046 | nll_loss 0.353 | ppl 1.28 | num_updates 66294 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint43.pt (epoch 43 @ 66294 updates) (writing took 4.3489766120910645 seconds)\n",
      "| epoch 044 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 044 | loss 1.901 | nll_loss 0.344 | ppl 1.27 | wps 69331 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 67836 | lr 0.000121414 | gnorm 0.180 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 9278 | train_wall 7826\n",
      "| epoch 044 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 044 | loss 1.901 | nll_loss 0.344 | ppl 1.27 | wps 69329 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 67836 | lr 0.000121414 | gnorm 0.180 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 9278 | train_wall 7828\n",
      "| epoch 044 | loss 1.901 | nll_loss 0.344 | ppl 1.27 | wps 69328 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 67836 | lr 0.000121414 | gnorm 0.180 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 9279 | train_wall 7655\n",
      "| epoch 045:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 044 | valid on 'valid' subset | loss 2.045 | nll_loss 0.353 | ppl 1.28 | num_updates 67836 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint44.pt (epoch 44 @ 67836 updates) (writing took 3.2215025424957275 seconds)\n",
      "| epoch 045:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 044 | valid on 'valid' subset | loss 2.045 | nll_loss 0.353 | ppl 1.28 | num_updates 67836 | best_loss 2.04018\n",
      "| epoch 045:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 044 | valid on 'valid' subset | loss 2.045 | nll_loss 0.353 | ppl 1.28 | num_updates 67836 | best_loss 2.04018\n",
      "| epoch 045 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 045 | loss 1.899 | nll_loss 0.343 | ppl 1.27 | wps 87799 | ups 7 | wpb 12087.668 | bsz 575.603 | num_updates 69376 | lr 0.000120059 | gnorm 0.182 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 9527 | train_wall 8030\n",
      "| epoch 045 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 8.0\n",
      "| WARNING: overflow detected, setting loss scale to: 4.0\n",
      "| epoch 045 | loss 1.899 | nll_loss 0.343 | ppl 1.27 | wps 87787 | ups 7 | wpb 12087.668 | bsz 575.603 | num_updates 69376 | lr 0.000120059 | gnorm 0.182 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 9528 | train_wall 7859\n",
      "| epoch 045 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 045 | loss 1.899 | nll_loss 0.343 | ppl 1.27 | wps 87789 | ups 7 | wpb 12087.668 | bsz 575.603 | num_updates 69376 | lr 0.000120059 | gnorm 0.182 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 9527 | train_wall 8033\n",
      "| epoch 046:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 045 | valid on 'valid' subset | loss 2.046 | nll_loss 0.354 | ppl 1.28 | num_updates 69376 | best_loss 2.04018\n",
      "| epoch 046:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 045 | valid on 'valid' subset | loss 2.046 | nll_loss 0.354 | ppl 1.28 | num_updates 69376 | best_loss 2.04018\n",
      "| epoch 046:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 045 | valid on 'valid' subset | loss 2.046 | nll_loss 0.354 | ppl 1.28 | num_updates 69376 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint45.pt (epoch 45 @ 69376 updates) (writing took 3.567594289779663 seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 046 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 046 | loss 1.897 | nll_loss 0.340 | ppl 1.27 | wps 96017 | ups 8 | wpb 12087.679 | bsz 575.604 | num_updates 70918 | lr 0.000118747 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 9749 | train_wall 8220\n",
      "| epoch 046 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 046 | loss 1.897 | nll_loss 0.340 | ppl 1.27 | wps 96010 | ups 8 | wpb 12087.679 | bsz 575.604 | num_updates 70918 | lr 0.000118747 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 9749 | train_wall 8045\n",
      "| epoch 046 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 046 | loss 1.897 | nll_loss 0.340 | ppl 1.27 | wps 96003 | ups 8 | wpb 12087.679 | bsz 575.604 | num_updates 70918 | lr 0.000118747 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 9748 | train_wall 8217\n",
      "| epoch 047:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 046 | valid on 'valid' subset | loss 2.047 | nll_loss 0.357 | ppl 1.28 | num_updates 70918 | best_loss 2.04018\n",
      "| epoch 047:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 046 | valid on 'valid' subset | loss 2.047 | nll_loss 0.357 | ppl 1.28 | num_updates 70918 | best_loss 2.04018\n",
      "| epoch 047:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 046 | valid on 'valid' subset | loss 2.047 | nll_loss 0.357 | ppl 1.28 | num_updates 70918 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint46.pt (epoch 46 @ 70918 updates) (writing took 4.977202653884888 seconds)\n",
      "| epoch 047 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 047 | loss 1.897 | nll_loss 0.340 | ppl 1.27 | wps 71245 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 72460 | lr 0.000117476 | gnorm 0.186 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 10051 | train_wall 8472\n",
      "| epoch 047 | loss 1.897 | nll_loss 0.340 | ppl 1.27 | wps 71246 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 72460 | lr 0.000117476 | gnorm 0.186 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 10051 | train_wall 8470\n",
      "| epoch 047 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 047 | loss 1.897 | nll_loss 0.340 | ppl 1.27 | wps 71247 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 72460 | lr 0.000117476 | gnorm 0.186 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 10051 | train_wall 8298\n",
      "| epoch 048:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 047 | valid on 'valid' subset | loss 2.046 | nll_loss 0.356 | ppl 1.28 | num_updates 72460 | best_loss 2.04018\n",
      "| epoch 048:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 047 | valid on 'valid' subset | loss 2.046 | nll_loss 0.356 | ppl 1.28 | num_updates 72460 | best_loss 2.04018\n",
      "| epoch 048:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 047 | valid on 'valid' subset | loss 2.046 | nll_loss 0.356 | ppl 1.28 | num_updates 72460 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint47.pt (epoch 47 @ 72460 updates) (writing took 3.871777296066284 seconds)\n",
      "| epoch 048 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 048 | loss 1.895 | nll_loss 0.337 | ppl 1.26 | wps 107793 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 74002 | lr 0.000116246 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 10253 | train_wall 8636\n",
      "| epoch 048 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 048 | loss 1.895 | nll_loss 0.337 | ppl 1.26 | wps 107790 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 74002 | lr 0.000116246 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 10254 | train_wall 8639\n",
      "| epoch 048 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 048 | loss 1.895 | nll_loss 0.337 | ppl 1.26 | wps 107791 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 74002 | lr 0.000116246 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 10254 | train_wall 8463\n",
      "| epoch 049:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 048 | valid on 'valid' subset | loss 2.049 | nll_loss 0.358 | ppl 1.28 | num_updates 74002 | best_loss 2.04018\n",
      "| epoch 049:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 048 | valid on 'valid' subset | loss 2.049 | nll_loss 0.358 | ppl 1.28 | num_updates 74002 | best_loss 2.04018\n",
      "| epoch 049:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 048 | valid on 'valid' subset | loss 2.049 | nll_loss 0.358 | ppl 1.28 | num_updates 74002 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint48.pt (epoch 48 @ 74002 updates) (writing took 3.1527841091156006 seconds)\n",
      "| epoch 049 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 049 | loss 1.894 | nll_loss 0.336 | ppl 1.26 | wps 86476 | ups 7 | wpb 12087.679 | bsz 575.604 | num_updates 75544 | lr 0.000115054 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 10493 | train_wall 8847\n",
      "| epoch 049 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 049 | loss 1.894 | nll_loss 0.336 | ppl 1.26 | wps 86478 | ups 7 | wpb 12087.679 | bsz 575.604 | num_updates 75544 | lr 0.000115054 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 10493 | train_wall 8849\n",
      "| epoch 049 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 049 | loss 1.894 | nll_loss 0.336 | ppl 1.26 | wps 86474 | ups 7 | wpb 12087.679 | bsz 575.604 | num_updates 75544 | lr 0.000115054 | gnorm 0.181 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 10494 | train_wall 8671\n",
      "| epoch 050:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 049 | valid on 'valid' subset | loss 2.048 | nll_loss 0.355 | ppl 1.28 | num_updates 75544 | best_loss 2.04018\n",
      "| epoch 050:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 049 | valid on 'valid' subset | loss 2.048 | nll_loss 0.355 | ppl 1.28 | num_updates 75544 | best_loss 2.04018\n",
      "| epoch 050:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 049 | valid on 'valid' subset | loss 2.048 | nll_loss 0.355 | ppl 1.28 | num_updates 75544 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint49.pt (epoch 49 @ 75544 updates) (writing took 3.213543653488159 seconds)\n",
      "| epoch 050 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 050 | loss 1.892 | nll_loss 0.335 | ppl 1.26 | wps 75906 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 77086 | lr 0.000113897 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 10768 | train_wall 8907\n",
      "| epoch 050 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 050 | loss 1.892 | nll_loss 0.335 | ppl 1.26 | wps 75898 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 77086 | lr 0.000113897 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 10767 | train_wall 9087\n",
      "| epoch 050 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 050 | loss 1.892 | nll_loss 0.335 | ppl 1.26 | wps 75885 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 77086 | lr 0.000113897 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 10768 | train_wall 9089\n",
      "| epoch 051:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 050 | valid on 'valid' subset | loss 2.050 | nll_loss 0.359 | ppl 1.28 | num_updates 77086 | best_loss 2.04018\n",
      "| epoch 051:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 050 | valid on 'valid' subset | loss 2.050 | nll_loss 0.359 | ppl 1.28 | num_updates 77086 | best_loss 2.04018\n",
      "| epoch 051:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 050 | valid on 'valid' subset | loss 2.050 | nll_loss 0.359 | ppl 1.28 | num_updates 77086 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint50.pt (epoch 50 @ 77086 updates) (writing took 3.7054696083068848 seconds)\n",
      "| epoch 051 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 051 | loss 1.891 | nll_loss 0.333 | ppl 1.26 | wps 75755 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 78628 | lr 0.000112775 | gnorm 0.186 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 11046 | train_wall 9144\n",
      "| epoch 051 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 051 | loss 1.891 | nll_loss 0.333 | ppl 1.26 | wps 75756 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 78628 | lr 0.000112775 | gnorm 0.186 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 11045 | train_wall 9332\n",
      "| epoch 051 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 051 | loss 1.891 | nll_loss 0.333 | ppl 1.26 | wps 75748 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 78628 | lr 0.000112775 | gnorm 0.186 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 11045 | train_wall 9330\n",
      "| epoch 052:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 051 | valid on 'valid' subset | loss 2.048 | nll_loss 0.359 | ppl 1.28 | num_updates 78628 | best_loss 2.04018\n",
      "| epoch 052:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 051 | valid on 'valid' subset | loss 2.048 | nll_loss 0.359 | ppl 1.28 | num_updates 78628 | best_loss 2.04018\n",
      "| epoch 052:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 051 | valid on 'valid' subset | loss 2.048 | nll_loss 0.359 | ppl 1.28 | num_updates 78628 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint51.pt (epoch 51 @ 78628 updates) (writing took 3.770097017288208 seconds)\n",
      "| epoch 052 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 052 | loss 1.890 | nll_loss 0.332 | ppl 1.26 | wps 79652 | ups 7 | wpb 12087.679 | bsz 575.604 | num_updates 80170 | lr 0.000111685 | gnorm 0.191 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 11309 | train_wall 9370\n",
      "| epoch 052 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 052 | loss 1.890 | nll_loss 0.332 | ppl 1.26 | wps 79652 | ups 7 | wpb 12087.679 | bsz 575.604 | num_updates 80170 | lr 0.000111685 | gnorm 0.191 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 11309 | train_wall 9558\n",
      "| epoch 052 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 052 | loss 1.890 | nll_loss 0.332 | ppl 1.26 | wps 79632 | ups 7 | wpb 12087.679 | bsz 575.604 | num_updates 80170 | lr 0.000111685 | gnorm 0.191 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 11309 | train_wall 9561\n",
      "| epoch 053:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 052 | valid on 'valid' subset | loss 2.051 | nll_loss 0.358 | ppl 1.28 | num_updates 80170 | best_loss 2.04018\n",
      "| epoch 053:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 052 | valid on 'valid' subset | loss 2.051 | nll_loss 0.358 | ppl 1.28 | num_updates 80170 | best_loss 2.04018\n",
      "| epoch 053:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 052 | valid on 'valid' subset | loss 2.051 | nll_loss 0.358 | ppl 1.28 | num_updates 80170 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint52.pt (epoch 52 @ 80170 updates) (writing took 3.650301933288574 seconds)\n",
      "| epoch 053 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 053 | loss 1.889 | nll_loss 0.331 | ppl 1.26 | wps 88063 | ups 7 | wpb 12087.674 | bsz 575.604 | num_updates 81711 | lr 0.000110627 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 11548 | train_wall 9768\n",
      "| epoch 053 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 8.0\n",
      "| epoch 053 | loss 1.889 | nll_loss 0.331 | ppl 1.26 | wps 88069 | ups 7 | wpb 12087.674 | bsz 575.604 | num_updates 81711 | lr 0.000110627 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 11548 | train_wall 9573\n",
      "| epoch 053 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 053 | loss 1.889 | nll_loss 0.331 | ppl 1.26 | wps 88056 | ups 7 | wpb 12087.674 | bsz 575.604 | num_updates 81711 | lr 0.000110627 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 11548 | train_wall 9766\n",
      "| epoch 054:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 053 | valid on 'valid' subset | loss 2.050 | nll_loss 0.359 | ppl 1.28 | num_updates 81711 | best_loss 2.04018\n",
      "| epoch 054:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 053 | valid on 'valid' subset | loss 2.050 | nll_loss 0.359 | ppl 1.28 | num_updates 81711 | best_loss 2.04018\n",
      "| epoch 054:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 053 | valid on 'valid' subset | loss 2.050 | nll_loss 0.359 | ppl 1.28 | num_updates 81711 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint53.pt (epoch 53 @ 81711 updates) (writing took 4.015733957290649 seconds)\n",
      "| epoch 054 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 054 | loss 1.887 | nll_loss 0.329 | ppl 1.26 | wps 74304 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 83253 | lr 0.000109597 | gnorm 0.183 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 11838 | train_wall 10010\n",
      "| epoch 054 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 054 | loss 1.887 | nll_loss 0.329 | ppl 1.26 | wps 74293 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 83253 | lr 0.000109597 | gnorm 0.183 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 11838 | train_wall 10008\n",
      "| epoch 054 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 054 | loss 1.887 | nll_loss 0.329 | ppl 1.26 | wps 74293 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 83253 | lr 0.000109597 | gnorm 0.183 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 11838 | train_wall 9815\n",
      "| epoch 055:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 054 | valid on 'valid' subset | loss 2.053 | nll_loss 0.361 | ppl 1.28 | num_updates 83253 | best_loss 2.04018\n",
      "| epoch 055:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 054 | valid on 'valid' subset | loss 2.053 | nll_loss 0.361 | ppl 1.28 | num_updates 83253 | best_loss 2.04018\n",
      "| epoch 055:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 054 | valid on 'valid' subset | loss 2.053 | nll_loss 0.361 | ppl 1.28 | num_updates 83253 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint54.pt (epoch 54 @ 83253 updates) (writing took 3.704969644546509 seconds)\n",
      "| epoch 055 | loss 1.886 | nll_loss 0.328 | ppl 1.26 | wps 86567 | ups 7 | wpb 12087.679 | bsz 575.604 | num_updates 84795 | lr 0.000108596 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 12089 | train_wall 10218\n",
      "| epoch 055 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 055 | loss 1.886 | nll_loss 0.328 | ppl 1.26 | wps 86560 | ups 7 | wpb 12087.679 | bsz 575.604 | num_updates 84795 | lr 0.000108596 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 12089 | train_wall 10023\n",
      "| epoch 055 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 055 | loss 1.886 | nll_loss 0.328 | ppl 1.26 | wps 86550 | ups 7 | wpb 12087.679 | bsz 575.604 | num_updates 84795 | lr 0.000108596 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 12089 | train_wall 10216\n",
      "| epoch 056:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 055 | valid on 'valid' subset | loss 2.052 | nll_loss 0.361 | ppl 1.28 | num_updates 84795 | best_loss 2.04018\n",
      "| epoch 056:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 055 | valid on 'valid' subset | loss 2.052 | nll_loss 0.361 | ppl 1.28 | num_updates 84795 | best_loss 2.04018\n",
      "| epoch 056:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 055 | valid on 'valid' subset | loss 2.052 | nll_loss 0.361 | ppl 1.28 | num_updates 84795 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint55.pt (epoch 55 @ 84795 updates) (writing took 3.7975358963012695 seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 056 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 8.0\n",
      "| epoch 056 | loss 1.885 | nll_loss 0.327 | ppl 1.25 | wps 78151 | ups 6 | wpb 12087.674 | bsz 575.604 | num_updates 86336 | lr 0.000107623 | gnorm 0.187 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 12364 | train_wall 10253\n",
      "| epoch 056 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 056 | loss 1.885 | nll_loss 0.327 | ppl 1.25 | wps 78141 | ups 6 | wpb 12087.674 | bsz 575.604 | num_updates 86336 | lr 0.000107623 | gnorm 0.187 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 12364 | train_wall 10447\n",
      "| epoch 056 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 056 | loss 1.885 | nll_loss 0.327 | ppl 1.25 | wps 78142 | ups 6 | wpb 12087.674 | bsz 575.604 | num_updates 86336 | lr 0.000107623 | gnorm 0.187 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 12364 | train_wall 10449\n",
      "| epoch 057:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 056 | valid on 'valid' subset | loss 2.052 | nll_loss 0.362 | ppl 1.29 | num_updates 86336 | best_loss 2.04018\n",
      "| epoch 057:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 056 | valid on 'valid' subset | loss 2.052 | nll_loss 0.362 | ppl 1.29 | num_updates 86336 | best_loss 2.04018\n",
      "| epoch 057:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 056 | valid on 'valid' subset | loss 2.052 | nll_loss 0.362 | ppl 1.29 | num_updates 86336 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint56.pt (epoch 56 @ 86336 updates) (writing took 4.564258575439453 seconds)\n",
      "| epoch 057 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 057 | loss 1.884 | nll_loss 0.326 | ppl 1.25 | wps 68719 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 87878 | lr 0.000106674 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 12666 | train_wall 10712\n",
      "| epoch 057 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 057 | loss 1.884 | nll_loss 0.326 | ppl 1.25 | wps 68706 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 87878 | lr 0.000106674 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 12667 | train_wall 10713\n",
      "| epoch 057 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 057 | loss 1.884 | nll_loss 0.326 | ppl 1.25 | wps 68701 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 87878 | lr 0.000106674 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 12667 | train_wall 10513\n",
      "| epoch 058:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 057 | valid on 'valid' subset | loss 2.054 | nll_loss 0.362 | ppl 1.29 | num_updates 87878 | best_loss 2.04018\n",
      "| epoch 058:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 057 | valid on 'valid' subset | loss 2.054 | nll_loss 0.362 | ppl 1.29 | num_updates 87878 | best_loss 2.04018\n",
      "| epoch 058:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 057 | valid on 'valid' subset | loss 2.054 | nll_loss 0.362 | ppl 1.29 | num_updates 87878 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint57.pt (epoch 57 @ 87878 updates) (writing took 4.083003044128418 seconds)\n",
      "| epoch 058 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 058 | loss 1.883 | nll_loss 0.324 | ppl 1.25 | wps 105630 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 89419 | lr 0.000105751 | gnorm 0.191 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 12884 | train_wall 10882\n",
      "| epoch 058 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 058 | loss 1.883 | nll_loss 0.324 | ppl 1.25 | wps 105627 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 89419 | lr 0.000105751 | gnorm 0.191 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 12884 | train_wall 10884\n",
      "| epoch 058 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 4.0\n",
      "| epoch 058 | loss 1.883 | nll_loss 0.324 | ppl 1.25 | wps 105622 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 89419 | lr 0.000105751 | gnorm 0.191 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 12885 | train_wall 10682\n",
      "| epoch 059:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 058 | valid on 'valid' subset | loss 2.052 | nll_loss 0.362 | ppl 1.28 | num_updates 89419 | best_loss 2.04018\n",
      "| epoch 059:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 058 | valid on 'valid' subset | loss 2.052 | nll_loss 0.362 | ppl 1.28 | num_updates 89419 | best_loss 2.04018\n",
      "| epoch 059:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 058 | valid on 'valid' subset | loss 2.052 | nll_loss 0.362 | ppl 1.28 | num_updates 89419 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint58.pt (epoch 58 @ 89419 updates) (writing took 3.3142168521881104 seconds)\n",
      "| epoch 059 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 059 | loss 1.882 | nll_loss 0.323 | ppl 1.25 | wps 70810 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 90961 | lr 0.000104851 | gnorm 0.194 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 13175 | train_wall 10936|▊| 1316/1542 [03:48<00:47,  4.73it/s, loss=1.881, nll_loss=0.36%|▊| 1324/1542 [03:47<00:50,  4.31it/s, loss=1.880, nll_loss=0.3\n",
      "| epoch 059 | loss 1.882 | nll_loss 0.323 | ppl 1.25 | wps 70811 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 90961 | lr 0.000104851 | gnorm 0.194 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 13175 | train_wall 11141\n",
      "| epoch 059 | loss 1.882 | nll_loss 0.323 | ppl 1.25 | wps 70811 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 90961 | lr 0.000104851 | gnorm 0.194 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 13175 | train_wall 11140\n",
      "| epoch 060:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 059 | valid on 'valid' subset | loss 2.055 | nll_loss 0.363 | ppl 1.29 | num_updates 90961 | best_loss 2.04018\n",
      "| epoch 060:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 059 | valid on 'valid' subset | loss 2.055 | nll_loss 0.363 | ppl 1.29 | num_updates 90961 | best_loss 2.04018\n",
      "| epoch 060:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 059 | valid on 'valid' subset | loss 2.055 | nll_loss 0.363 | ppl 1.29 | num_updates 90961 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint59.pt (epoch 59 @ 90961 updates) (writing took 4.905341625213623 seconds)\n",
      "| epoch 060 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 060 | loss 1.881 | nll_loss 0.322 | ppl 1.25 | wps 64555 | ups 5 | wpb 12087.679 | bsz 575.604 | num_updates 92503 | lr 0.000103973 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 13510 | train_wall 112159, nll_loss=0.3 060:  99%|▉| 1531/1542 [04:55<00:02,  4.90it/s, loss=1.881, nll_loss=0.3\n",
      "| epoch 060 | loss 1.881 | nll_loss 0.322 | ppl 1.25 | wps 64556 | ups 5 | wpb 12087.679 | bsz 575.604 | num_updates 92503 | lr 0.000103973 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 13510 | train_wall 11419\n",
      "| epoch 060 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 060 | loss 1.881 | nll_loss 0.322 | ppl 1.25 | wps 64547 | ups 5 | wpb 12087.679 | bsz 575.604 | num_updates 92503 | lr 0.000103973 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 13510 | train_wall 11421\n",
      "| epoch 061:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 060 | valid on 'valid' subset | loss 2.056 | nll_loss 0.364 | ppl 1.29 | num_updates 92503 | best_loss 2.04018\n",
      "| epoch 061:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 060 | valid on 'valid' subset | loss 2.056 | nll_loss 0.364 | ppl 1.29 | num_updates 92503 | best_loss 2.04018\n",
      "| epoch 061:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 060 | valid on 'valid' subset | loss 2.056 | nll_loss 0.364 | ppl 1.29 | num_updates 92503 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint60.pt (epoch 60 @ 92503 updates) (writing took 4.43066668510437 seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 061 | loss 1.880 | nll_loss 0.321 | ppl 1.25 | wps 68125 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 94045 | lr 0.000103117 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 13828 | train_wall 114803s=0.33 061:  40%|▍| 611/1542 [01:58<01:37,  9.51it/s, loss=1.875, nll_loss=0.31\n",
      "| epoch 061 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 061 | loss 1.880 | nll_loss 0.321 | ppl 1.25 | wps 68126 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 94045 | lr 0.000103117 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 13828 | train_wall 11686\n",
      "| epoch 061 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 061 | loss 1.880 | nll_loss 0.321 | ppl 1.25 | wps 68116 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 94045 | lr 0.000103117 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 13828 | train_wall 11684\n",
      "| epoch 062:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 061 | valid on 'valid' subset | loss 2.054 | nll_loss 0.362 | ppl 1.29 | num_updates 94045 | best_loss 2.04018\n",
      "| epoch 062:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 061 | valid on 'valid' subset | loss 2.054 | nll_loss 0.362 | ppl 1.29 | num_updates 94045 | best_loss 2.04018\n",
      "| epoch 062:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 061 | valid on 'valid' subset | loss 2.054 | nll_loss 0.362 | ppl 1.29 | num_updates 94045 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint61.pt (epoch 61 @ 94045 updates) (writing took 4.693138360977173 seconds)\n",
      "| epoch 062 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 062 | loss 1.879 | nll_loss 0.320 | ppl 1.25 | wps 76315 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 95587 | lr 0.000102282 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 14119 | train_wall 11925\n",
      "| epoch 062 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 062 | loss 1.879 | nll_loss 0.320 | ppl 1.25 | wps 76311 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 95587 | lr 0.000102282 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 14119 | train_wall 11922\n",
      "| epoch 062 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 062 | loss 1.879 | nll_loss 0.320 | ppl 1.25 | wps 76305 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 95587 | lr 0.000102282 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 14119 | train_wall 11716\n",
      "| epoch 063:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 062 | valid on 'valid' subset | loss 2.057 | nll_loss 0.365 | ppl 1.29 | num_updates 95587 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint62.pt (epoch 62 @ 95587 updates) (writing took 2.910956859588623 seconds)\n",
      "| epoch 063:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 062 | valid on 'valid' subset | loss 2.057 | nll_loss 0.365 | ppl 1.29 | num_updates 95587 | best_loss 2.04018\n",
      "| epoch 063:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 062 | valid on 'valid' subset | loss 2.057 | nll_loss 0.365 | ppl 1.29 | num_updates 95587 | best_loss 2.04018\n",
      "| epoch 063 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 063 | loss 1.878 | nll_loss 0.319 | ppl 1.25 | wps 82379 | ups 7 | wpb 12087.674 | bsz 575.604 | num_updates 97128 | lr 0.000101468 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 14372 | train_wall 12139\n",
      "| epoch 063 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 063 | loss 1.878 | nll_loss 0.319 | ppl 1.25 | wps 82359 | ups 7 | wpb 12087.674 | bsz 575.604 | num_updates 97128 | lr 0.000101468 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 14372 | train_wall 12143\n",
      "| WARNING: overflow detected, setting loss scale to: 4.0\n",
      "| epoch 063 | loss 1.878 | nll_loss 0.319 | ppl 1.25 | wps 82354 | ups 7 | wpb 12087.674 | bsz 575.604 | num_updates 97128 | lr 0.000101468 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 14372 | train_wall 11934\n",
      "| epoch 064:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 063 | valid on 'valid' subset | loss 2.057 | nll_loss 0.367 | ppl 1.29 | num_updates 97128 | best_loss 2.04018\n",
      "| epoch 064:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 063 | valid on 'valid' subset | loss 2.057 | nll_loss 0.367 | ppl 1.29 | num_updates 97128 | best_loss 2.04018\n",
      "| epoch 064:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 063 | valid on 'valid' subset | loss 2.057 | nll_loss 0.367 | ppl 1.29 | num_updates 97128 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint63.pt (epoch 63 @ 97128 updates) (writing took 3.9020261764526367 seconds)\n",
      "| epoch 064 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 064 | loss 1.877 | nll_loss 0.317 | ppl 1.25 | wps 86874 | ups 7 | wpb 12087.679 | bsz 575.604 | num_updates 98670 | lr 0.000100672 | gnorm 0.192 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 14625 | train_wall 12345\n",
      "| epoch 064 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 064 | loss 1.877 | nll_loss 0.317 | ppl 1.25 | wps 86876 | ups 7 | wpb 12087.679 | bsz 575.604 | num_updates 98670 | lr 0.000100672 | gnorm 0.192 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 14625 | train_wall 12350\n",
      "| epoch 064 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 064 | loss 1.877 | nll_loss 0.317 | ppl 1.25 | wps 86872 | ups 7 | wpb 12087.679 | bsz 575.604 | num_updates 98670 | lr 0.000100672 | gnorm 0.192 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 14625 | train_wall 12140\n",
      "| epoch 065:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 064 | valid on 'valid' subset | loss 2.055 | nll_loss 0.366 | ppl 1.29 | num_updates 98670 | best_loss 2.04018\n",
      "| epoch 065:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 064 | valid on 'valid' subset | loss 2.055 | nll_loss 0.366 | ppl 1.29 | num_updates 98670 | best_loss 2.04018\n",
      "| epoch 065:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 064 | valid on 'valid' subset | loss 2.055 | nll_loss 0.366 | ppl 1.29 | num_updates 98670 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint64.pt (epoch 64 @ 98670 updates) (writing took 3.2241835594177246 seconds)\n",
      "| epoch 065 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 065 | loss 1.876 | nll_loss 0.317 | ppl 1.25 | wps 73615 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 100212 | lr 9.98942e-05 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 14904 | train_wall 12591\n",
      "| epoch 065 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 065 | loss 1.876 | nll_loss 0.317 | ppl 1.25 | wps 73611 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 100212 | lr 9.98942e-05 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 14905 | train_wall 12384\n",
      "| epoch 065 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 065 | loss 1.876 | nll_loss 0.317 | ppl 1.25 | wps 73614 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 100212 | lr 9.98942e-05 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 14905 | train_wall 12598\n",
      "| epoch 066:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 065 | valid on 'valid' subset | loss 2.057 | nll_loss 0.366 | ppl 1.29 | num_updates 100212 | best_loss 2.04018\n",
      "| epoch 066:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 065 | valid on 'valid' subset | loss 2.057 | nll_loss 0.366 | ppl 1.29 | num_updates 100212 | best_loss 2.04018\n",
      "| epoch 066:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 065 | valid on 'valid' subset | loss 2.057 | nll_loss 0.366 | ppl 1.29 | num_updates 100212 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint65.pt (epoch 65 @ 100212 updates) (writing took 3.7547454833984375 seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 066 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 066 | loss 1.875 | nll_loss 0.315 | ppl 1.24 | wps 93666 | ups 8 | wpb 12087.679 | bsz 575.604 | num_updates 101754 | lr 9.91344e-05 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 15144 | train_wall 12575\n",
      "| epoch 066 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 066 | loss 1.875 | nll_loss 0.315 | ppl 1.24 | wps 93648 | ups 8 | wpb 12087.679 | bsz 575.604 | num_updates 101754 | lr 9.91344e-05 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 15144 | train_wall 12790\n",
      "| epoch 066 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 066 | loss 1.875 | nll_loss 0.315 | ppl 1.24 | wps 93640 | ups 8 | wpb 12087.679 | bsz 575.604 | num_updates 101754 | lr 9.91344e-05 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 15144 | train_wall 12783\n",
      "| epoch 067:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 066 | valid on 'valid' subset | loss 2.057 | nll_loss 0.367 | ppl 1.29 | num_updates 101754 | best_loss 2.04018\n",
      "| epoch 067:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 066 | valid on 'valid' subset | loss 2.057 | nll_loss 0.367 | ppl 1.29 | num_updates 101754 | best_loss 2.04018\n",
      "| epoch 067:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 066 | valid on 'valid' subset | loss 2.057 | nll_loss 0.367 | ppl 1.29 | num_updates 101754 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint66.pt (epoch 66 @ 101754 updates) (writing took 3.494368314743042 seconds)\n",
      "| epoch 067 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 067 | loss 1.874 | nll_loss 0.314 | ppl 1.24 | wps 103929 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 103296 | lr 9.83917e-05 | gnorm 0.194 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 15350 | train_wall 12747\n",
      "| epoch 067 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 067 | loss 1.874 | nll_loss 0.314 | ppl 1.24 | wps 103922 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 103296 | lr 9.83917e-05 | gnorm 0.194 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 15350 | train_wall 12966\n",
      "| epoch 067 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 067 | loss 1.874 | nll_loss 0.314 | ppl 1.24 | wps 103920 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 103296 | lr 9.83917e-05 | gnorm 0.194 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 15349 | train_wall 12955\n",
      "| epoch 068:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 067 | valid on 'valid' subset | loss 2.059 | nll_loss 0.369 | ppl 1.29 | num_updates 103296 | best_loss 2.04018\n",
      "| epoch 068:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 067 | valid on 'valid' subset | loss 2.059 | nll_loss 0.369 | ppl 1.29 | num_updates 103296 | best_loss 2.04018\n",
      "| epoch 068:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 067 | valid on 'valid' subset | loss 2.059 | nll_loss 0.369 | ppl 1.29 | num_updates 103296 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint67.pt (epoch 67 @ 103296 updates) (writing took 3.360020637512207 seconds)\n",
      "| epoch 068 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 068 | loss 1.873 | nll_loss 0.313 | ppl 1.24 | wps 118742 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 104838 | lr 9.76654e-05 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 15530 | train_wall 13119\n",
      "| epoch 068 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 068 | loss 1.873 | nll_loss 0.313 | ppl 1.24 | wps 118732 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 104838 | lr 9.76654e-05 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 15530 | train_wall 13109\n",
      "| epoch 068 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 068 | loss 1.873 | nll_loss 0.313 | ppl 1.24 | wps 118723 | ups 10 | wpb 12087.679 | bsz 575.604 | num_updates 104838 | lr 9.76654e-05 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 15530 | train_wall 12897\n",
      "| epoch 069:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 068 | valid on 'valid' subset | loss 2.059 | nll_loss 0.368 | ppl 1.29 | num_updates 104838 | best_loss 2.04018\n",
      "| epoch 069:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 068 | valid on 'valid' subset | loss 2.059 | nll_loss 0.368 | ppl 1.29 | num_updates 104838 | best_loss 2.04018\n",
      "| epoch 069:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 068 | valid on 'valid' subset | loss 2.059 | nll_loss 0.368 | ppl 1.29 | num_updates 104838 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint68.pt (epoch 68 @ 104838 updates) (writing took 3.6251964569091797 seconds)\n",
      "| epoch 069 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| WARNING: overflow detected, setting loss scale to: 4.0\n",
      "| epoch 069 | loss 1.872 | nll_loss 0.312 | ppl 1.24 | wps 113344 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 106379 | lr 9.69554e-05 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 15718 | train_wall 13055\n",
      "| epoch 069 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 069 | loss 1.872 | nll_loss 0.312 | ppl 1.24 | wps 113347 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 106379 | lr 9.69554e-05 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 15718 | train_wall 13271\n",
      "| epoch 069 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 069 | loss 1.872 | nll_loss 0.312 | ppl 1.24 | wps 113293 | ups 9 | wpb 12087.674 | bsz 575.604 | num_updates 106379 | lr 9.69554e-05 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 15718 | train_wall 13280\n",
      "| epoch 070:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 069 | valid on 'valid' subset | loss 2.059 | nll_loss 0.369 | ppl 1.29 | num_updates 106379 | best_loss 2.04018\n",
      "| epoch 070:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 069 | valid on 'valid' subset | loss 2.059 | nll_loss 0.369 | ppl 1.29 | num_updates 106379 | best_loss 2.04018\n",
      "| epoch 070:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 069 | valid on 'valid' subset | loss 2.059 | nll_loss 0.369 | ppl 1.29 | num_updates 106379 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint69.pt (epoch 69 @ 106379 updates) (writing took 3.773021936416626 seconds)\n",
      "| epoch 070 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 070 | loss 1.871 | nll_loss 0.311 | ppl 1.24 | wps 103114 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 107921 | lr 9.62603e-05 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 15925 | train_wall 13457\n",
      "| epoch 070 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 070 | loss 1.871 | nll_loss 0.311 | ppl 1.24 | wps 103109 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 107921 | lr 9.62603e-05 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 15926 | train_wall 13227\n",
      "| epoch 070 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 070 | loss 1.871 | nll_loss 0.311 | ppl 1.24 | wps 103101 | ups 9 | wpb 12087.679 | bsz 575.604 | num_updates 107921 | lr 9.62603e-05 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 15925 | train_wall 13445\n",
      "| epoch 071:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 070 | valid on 'valid' subset | loss 2.060 | nll_loss 0.370 | ppl 1.29 | num_updates 107921 | best_loss 2.04018\n",
      "| epoch 071:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 070 | valid on 'valid' subset | loss 2.060 | nll_loss 0.370 | ppl 1.29 | num_updates 107921 | best_loss 2.04018\n",
      "| epoch 071:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 070 | valid on 'valid' subset | loss 2.060 | nll_loss 0.370 | ppl 1.29 | num_updates 107921 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint70.pt (epoch 70 @ 107921 updates) (writing took 3.6478114128112793 seconds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 071 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 071 | loss 1.870 | nll_loss 0.311 | ppl 1.24 | wps 71181 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 109463 | lr 9.55798e-05 | gnorm 0.197 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 16214 | train_wall 13481\n",
      "| epoch 071 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 071 | loss 1.870 | nll_loss 0.311 | ppl 1.24 | wps 71177 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 109463 | lr 9.55798e-05 | gnorm 0.197 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 16214 | train_wall 13714\n",
      "| epoch 071 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 071 | loss 1.870 | nll_loss 0.311 | ppl 1.24 | wps 71166 | ups 6 | wpb 12087.679 | bsz 575.604 | num_updates 109463 | lr 9.55798e-05 | gnorm 0.197 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 16214 | train_wall 13700\n",
      "| epoch 072:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 071 | valid on 'valid' subset | loss 2.060 | nll_loss 0.370 | ppl 1.29 | num_updates 109463 | best_loss 2.04018\n",
      "| epoch 072:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 071 | valid on 'valid' subset | loss 2.060 | nll_loss 0.370 | ppl 1.29 | num_updates 109463 | best_loss 2.04018\n",
      "| epoch 072:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 071 | valid on 'valid' subset | loss 2.060 | nll_loss 0.370 | ppl 1.29 | num_updates 109463 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint71.pt (epoch 71 @ 109463 updates) (writing took 3.534426212310791 seconds)\n",
      "| epoch 072 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 072 | loss 1.870 | nll_loss 0.310 | ppl 1.24 | wps 46076 | ups 4 | wpb 12087.679 | bsz 575.604 | num_updates 111005 | lr 9.49137e-05 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 16664 | train_wall 14099\n",
      "| epoch 072 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 072 | loss 1.870 | nll_loss 0.310 | ppl 1.24 | wps 46069 | ups 4 | wpb 12087.679 | bsz 575.604 | num_updates 111005 | lr 9.49137e-05 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 16664 | train_wall 13874\n",
      "| epoch 072 | valid on 'valid' subset:   0%|            | 0/386 [00:00<?, ?it/s]| epoch 072 | loss 1.870 | nll_loss 0.310 | ppl 1.24 | wps 46065 | ups 4 | wpb 12087.679 | bsz 575.604 | num_updates 111005 | lr 9.49137e-05 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 16664 | train_wall 14112\n",
      "| epoch 073:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 072 | valid on 'valid' subset | loss 2.061 | nll_loss 0.371 | ppl 1.29 | num_updates 111005 | best_loss 2.04018\n",
      "| epoch 073:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 072 | valid on 'valid' subset | loss 2.061 | nll_loss 0.371 | ppl 1.29 | num_updates 111005 | best_loss 2.04018\n",
      "| epoch 073:   0%|                                     | 0/1542 [00:00<?, ?it/s]| epoch 072 | valid on 'valid' subset | loss 2.061 | nll_loss 0.371 | ppl 1.29 | num_updates 111005 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/pretrained_nmt/checkpoint72.pt (epoch 72 @ 111005 updates) (writing took 3.7375411987304688 seconds)\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq/train.py\", line 362, in <module>\n",
      "    cli_main()\n",
      "  File \"/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq/train.py\", line 354, in cli_main\n",
      "    nprocs=args.distributed_world_size,\n",
      "  File \"/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\n",
      "    while not spawn_context.join():\n",
      "  File \"/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 77, in join\n",
      "    timeout=timeout,\n",
      "  File \"/data/xiaowentao/.anaconda3/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/data/xiaowentao/.anaconda3/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training && \\\n",
    "    chmod +x train_pre_nmt.sh && ./train_pre_nmt.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ source ../paths.sh\n",
      "++++ dirname ../paths.sh\n",
      "+++ cd ..\n",
      "+++ pwd\n",
      "++ BASE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb\n",
      "++ DATA_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data\n",
      "++ MODEL_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/models\n",
      "++ SCRIPTS_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts\n",
      "++ SOFTWARE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software\n",
      "++ TRAINING_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training\n",
      "+ src_ext=src\n",
      "+ trg_ext=trg\n",
      "+ train_data_prefix=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/train_no_bpe_no_ds.tok\n",
      "+ dev_data_prefix=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe_no_ds.tok\n",
      "+ FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt\n",
      "+ mkdir -p processed/\n",
      "+ cp -vf /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe_no_ds.tok.src processed/dev.src_bert_nmt\n",
      "'/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe_no_ds.tok.src' -> 'processed/dev.src_bert_nmt'\n",
      "+ cp -vf /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe_no_ds.tok.trg processed/dev.trg_bert_nmt\n",
      "'/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data/dev_no_bpe_no_ds.tok.trg' -> 'processed/dev.trg_bert_nmt'\n",
      "+ less processed/train.all.src\n",
      "+ less processed/train.all.trg\n",
      "+ python /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt/preprocess.py --source-lang src_bert_nmt --target-lang trg_bert_nmt --trainpref processed/train --validpref processed/dev --testpref processed/dev --destdir processed/bin_bert_nmt_no_ds --joined-dictionary --bert-model-name voidful/albert_chinese_tiny --workers 32\n",
      "Namespace(alignfile=None, bert_model_name='voidful/albert_chinese_tiny', cpu=False, criterion='cross_entropy', dataset_impl='cached', destdir='processed/bin_bert_nmt_no_ds', fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=True, log_format=None, log_interval=1000, lr_scheduler='fixed', memory_efficient_fp16=False, min_loss_scale=0.0001, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=False, optimizer='nag', padding_factor=8, seed=1, source_lang='src_bert_nmt', srcdict=None, target_lang='trg_bert_nmt', task='translation', tbmf_wrapper=False, tensorboard_logdir='', testpref='processed/dev', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, trainpref='processed/train', user_dir=None, validpref='processed/dev', workers=32)\n",
      "| [src_bert_nmt] Dictionary: 5543 types\n",
      "| [src_bert_nmt] processed/train.src_bert_nmt: 887581 sents, 18404200 tokens, 0.0% replaced by [UNK]\n",
      "| [src_bert_nmt] Dictionary: 5543 types\n",
      "| [src_bert_nmt] processed/dev.src_bert_nmt: 327403 sents, 6816512 tokens, 0.00139% replaced by [UNK]\n",
      "| [src_bert_nmt] Dictionary: 5543 types\n",
      "| [src_bert_nmt] processed/dev.src_bert_nmt: 327403 sents, 6816512 tokens, 0.00139% replaced by [UNK]\n",
      "| [trg_bert_nmt] Dictionary: 5543 types\n",
      "| [trg_bert_nmt] processed/train.trg_bert_nmt: 887581 sents, 18639201 tokens, 0.0% replaced by [UNK]\n",
      "| [trg_bert_nmt] Dictionary: 5543 types\n",
      "| [trg_bert_nmt] processed/dev.trg_bert_nmt: 327403 sents, 6875463 tokens, 0.00217% replaced by [UNK]\n",
      "| [trg_bert_nmt] Dictionary: 5543 types\n",
      "| [trg_bert_nmt] processed/dev.trg_bert_nmt: 327403 sents, 6875463 tokens, 0.00217% replaced by [UNK]\n",
      "| [src_bert_nmt] Dictionary: 21127 types\n",
      "| [src_bert_nmt] processed/train.bert.src_bert_nmt: 887581 sents, 18829055 tokens, 0.0% replaced by [UNK]\n",
      "| [src_bert_nmt] Dictionary: 21127 types\n",
      "| [src_bert_nmt] processed/dev.bert.src_bert_nmt: 327403 sents, 6984942 tokens, 0.0% replaced by [UNK]\n",
      "| [src_bert_nmt] Dictionary: 21127 types\n",
      "| [src_bert_nmt] processed/dev.bert.src_bert_nmt: 327403 sents, 6984942 tokens, 0.0% replaced by [UNK]\n",
      "| Wrote preprocessed data to processed/bin_bert_nmt_no_ds\n"
     ]
    }
   ],
   "source": [
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training && \\\n",
    "    chmod +x preprocess_bert_nmt_no_bpe_no_ds.sh && ./preprocess_bert_nmt_no_bpe_no_ds.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/fairseq/ && \\\n",
    "    env CUDA_VISIBLE_DEVICES='3' python generate.py \\\n",
    "    ../../training/processed/bin_bert_nmt_no_ds \\\n",
    "    --path ../../training/checkpoints/pretrained_nmt/checkpoint_best.pt \\\n",
    "    --beam 5 --batch-size 128 -s src_bert_nmt \\\n",
    "    -t trg_bert_nmt > ../../training/processed/dev.trg_bert_nmt.pred_pre_nmt_no_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
      "#val:327403\n",
      "cnt_em_crnn: 210386, cnt_em_transformer: 275691\n",
      "em_crnn: 0.6426, em_transformer: 0.8421\n",
      "dist_crnn: 94.0868, dist_cs2s: 94.2802\n"
     ]
    }
   ],
   "source": [
    "%cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
    "\n",
    "from Levenshtein import distance\n",
    "\n",
    "with open('dev.trg_bert_nmt.pred_pre_nmt_no_ds') as f:\n",
    "    lines = f.readlines()\n",
    "    dev_src = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[7:-2]) if i % 4 == 0]\n",
    "    dev_trg = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[7:-2]) if (i-1) % 4 == 0]\n",
    "    dev_pred = [''.join(l.strip().split()[2:]) for i, l in enumerate(\n",
    "        lines[7:-2]) if (i-2) % 4 == 0]\n",
    "assert len(dev_src) == len(dev_trg) == len(dev_pred), [\n",
    "    len(dev_src), len(dev_trg), len(dev_pred)]\n",
    "cnt_em_crnn = sum([p == g for p, g in zip(dev_src, dev_trg)])\n",
    "cnt_em_transformer = sum([p == g for p, g in zip(dev_pred, dev_trg)])\n",
    "dist_crnn = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_trg)]) / len(dev_src)\n",
    "dist_transformer = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_pred)]) / len(dev_src)\n",
    "print(f'#val:{len(dev_src)}')\n",
    "print(f'cnt_em_crnn: {cnt_em_crnn}, cnt_em_transformer: {cnt_em_transformer}')\n",
    "print('em_crnn: {:.4f}, em_transformer: {:.4f}'.format(\n",
    "    cnt_em_crnn / len(dev_src), cnt_em_transformer / len(dev_src)))\n",
    "print('dist_crnn: {:.4f}, dist_cs2s: {:.4f}'.format(\n",
    "    100 * (1 - dist_crnn), 100 * (1 - dist_transformer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ set -e\n",
      "+ source ../paths.sh\n",
      "++++ dirname ../paths.sh\n",
      "+++ cd ..\n",
      "+++ pwd\n",
      "++ BASE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb\n",
      "++ DATA_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data\n",
      "++ MODEL_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/models\n",
      "++ SCRIPTS_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts\n",
      "++ SOFTWARE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software\n",
      "++ TRAINING_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training\n",
      "+ FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt\n",
      "+ SEED=1\n",
      "+ DATA_BIN_DIR=processed/bin_bert_nmt\n",
      "+ src=src_bert_nmt\n",
      "+ tgt=trg_bert_nmt\n",
      "+ bedropout=0.1\n",
      "+ ARCH=transformer_s2_iwslt_de_en\n",
      "+ DATAPATH=processed/bin_bert_nmt\n",
      "+ SAVEDIR=checkpoints/bert_nmt_0.1\n",
      "+ mkdir -p checkpoints/bert_nmt_0.1\n",
      "+ '[' '!' -f checkpoints/bert_nmt_0.1/checkpoint_nmt.pt ']'\n",
      "+ cp -v checkpoints/pretrained_nmt/checkpoint_best.pt checkpoints/bert_nmt_0.1/checkpoint_nmt.pt\n",
      "'checkpoints/pretrained_nmt/checkpoint_best.pt' -> 'checkpoints/bert_nmt_0.1/checkpoint_nmt.pt'\n",
      "+ '[' '!' -f checkpoints/bert_nmt_0.1/checkpoint_last.pt ']'\n",
      "+ warmup='--warmup-from-nmt --reset-lr-scheduler'\n",
      "+ env PYTHONPATH=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt: CUDA_VISIBLE_DEVICES=1,2 python /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt/train.py processed/bin_bert_nmt -a transformer_s2_iwslt_de_en --optimizer adam --lr 0.0005 -s src_bert_nmt -t trg_bert_nmt --label-smoothing 0.1 --dropout 0.5 --max-tokens 8192 --min-lr 1e-09 --lr-scheduler inverse_sqrt --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --max-update 150000 --warmup-updates 8192 --warmup-init-lr 1e-07 --batch-size 512 --no-progress-bar --restore-file checkpoint_best.pt --bert-model-name voidful/albert_chinese_tiny --adam-betas '(0.9,0.98)' --save-dir checkpoints/bert_nmt_0.1 --share-all-embeddings --warmup-from-nmt --reset-lr-scheduler --fp16 --encoder-bert-dropout --encoder-bert-dropout-ratio 0.1\n",
      "+ tee -a checkpoints/bert_nmt_0.1/training.log\n",
      "| distributed init (rank 0): tcp://localhost:16679\n",
      "| distributed init (rank 1): tcp://localhost:16679\n",
      "| initialized host 199 as rank 1\n",
      "| initialized host 199 as rank 0\n",
      "Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_s2_iwslt_de_en', attention_dropout=0.0, bert_first=True, bert_gates=[1, 1, 1, 1, 1, 1], bert_model_name='voidful/albert_chinese_tiny', bert_output_layer=-1, bert_ratio=1.0, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='processed/bin_bert_nmt', dataset_impl='cached', ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layers=6, decoder_learned_pos=False, decoder_no_bert=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:16679', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=2, dropout=0.5, encoder_attention_heads=4, encoder_bert_dropout=True, encoder_bert_dropout_ratio=0.1, encoder_bert_mixup=False, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_ratio=1.0, find_unused_parameters=False, finetune_bert=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', mask_cls_sep=False, max_epoch=0, max_sentences=512, max_sentences_valid=512, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_update=150000, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_epoch_checkpoints=False, no_progress_bar=True, no_save=False, no_token_positional_embeddings=False, num_workers=0, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=True, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_best.pt', save_dir='checkpoints/bert_nmt_0.1', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='src_bert_nmt', target_lang='trg_bert_nmt', task='translation', tbmf_wrapper=False, tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', update_freq=[1], upsample_primary=1, user_dir=None, valid_subset='valid', validate_interval=1, warmup_from_nmt=True, warmup_init_lr=1e-07, warmup_nmt_file='checkpoint_nmt.pt', warmup_updates=8192, weight_decay=0.0001)\n",
      "| [src_bert_nmt] dictionary: 5544 types\n",
      "| [trg_bert_nmt] dictionary: 5544 types\n",
      "| processed/bin_bert_nmt valid src_bert_nmt-trg_bert_nmt 221931 examples\n",
      "bert_gates [True, True, True, True, True, True]\n",
      "TransformerS2Model(\n",
      "  (encoder): TransformerS2Encoder(\n",
      "    (embed_tokens): Embedding(5544, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed_tokens): Embedding(5544, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (bert_encoder): AlbertForMaskedLM(\n",
      "    (albert): AlbertModel(\n",
      "      (embeddings): AlbertEmbeddings(\n",
      "        (word_embeddings): Embedding(21128, 128, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 128)\n",
      "        (token_type_embeddings): Embedding(2, 128)\n",
      "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (encoder): AlbertTransformer(\n",
      "        (embedding_hidden_mapping_in): Linear(in_features=128, out_features=312, bias=True)\n",
      "        (albert_layer_groups): ModuleList(\n",
      "          (0): AlbertLayerGroup(\n",
      "            (albert_layers): ModuleList(\n",
      "              (0): AlbertLayer(\n",
      "                (full_layer_layer_norm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "                (attention): AlbertAttention(\n",
      "                  (query): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (key): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (value): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "                )\n",
      "                (ffn): Linear(in_features=312, out_features=1248, bias=True)\n",
      "                (ffn_output): Linear(in_features=1248, out_features=312, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): Linear(in_features=312, out_features=312, bias=True)\n",
      "      (pooler_activation): Tanh()\n",
      "    )\n",
      "    (predictions): AlbertMLMHead(\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dense): Linear(in_features=312, out_features=128, bias=True)\n",
      "      (decoder): Linear(in_features=128, out_features=21128, bias=True)\n",
      "    )\n",
      "  )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n",
      "| model transformer_s2_iwslt_de_en, criterion LabelSmoothedCrossEntropyCriterion\n",
      "| num. model params: 48673680 (num. trained: 44531712)\n",
      "| training on 2 GPUs\n",
      "| max tokens per GPU = 8192 and max sentences per GPU = 512\n",
      "Model will load checkpoint from checkpoints/bert_nmt_0.1/checkpoint_nmt.pt\n",
      "| loaded checkpoint checkpoints/bert_nmt_0.1/checkpoint_nmt.pt (epoch 31 @ 0 updates)\n",
      "| loading train data for epoch 31\n",
      "| processed/bin_bert_nmt train src_bert_nmt-trg_bert_nmt 887581 examples\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint31.pt (epoch 31 @ 0 updates) (writing took 0.17799663543701172 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 64.0\n",
      "| WARNING: overflow detected, setting loss scale to: 32.0\n",
      "| WARNING: overflow detected, setting loss scale to: 16.0\n",
      "| epoch 032:   1000 / 1205 loss=7.735, nll_loss=6.946, ppl=123.26, wps=57363, ups=4, wpb=15477.884, bsz=737.042, num_updates=998, lr=6.10009e-05, gnorm=0.757, clip=0.000, oom=0.000, loss_scale=16.000, wall=269, train_wall=5355\n",
      "| epoch 032 | loss 7.321 | nll_loss 6.467 | ppl 88.47 | wps 58434 | ups 4 | wpb 15467.968 | bsz 736.570 | num_updates 1202 | lr 7.34496e-05 | gnorm 0.739 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 318 | train_wall 5395\n",
      "| epoch 032 | valid on 'valid' subset | loss 4.094 | nll_loss 2.401 | ppl 5.28 | num_updates 1202 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint32.pt (epoch 32 @ 1202 updates) (writing took 3.070077657699585 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 8.0\n",
      "| WARNING: overflow detected, setting loss scale to: 4.0\n",
      "| epoch 033:   1000 / 1205 loss=2.940, nll_loss=1.419, ppl=2.67, wps=64084, ups=3, wpb=15472.417, bsz=736.782, num_updates=2201, lr=0.000134412, gnorm=0.671, clip=0.000, oom=0.000, loss_scale=4.000, wall=606, train_wall=5591\n",
      "| epoch 033 | loss 2.849 | nll_loss 1.320 | ppl 2.50 | wps 64076 | ups 4 | wpb 15467.958 | bsz 736.569 | num_updates 2405 | lr 0.00014686 | gnorm 0.635 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 655 | train_wall 5631\n",
      "| epoch 033 | valid on 'valid' subset | loss 2.570 | nll_loss 0.559 | ppl 1.47 | num_updates 2405 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint33.pt (epoch 33 @ 2405 updates) (writing took 2.9627742767333984 seconds)\n",
      "| epoch 034:   1000 / 1205 loss=2.277, nll_loss=0.712, ppl=1.64, wps=65711, ups=4, wpb=15474.063, bsz=736.860, num_updates=3406, lr=0.000207944, gnorm=0.408, clip=0.000, oom=0.000, loss_scale=4.000, wall=933, train_wall=5824\n",
      "| epoch 034 | loss 2.260 | nll_loss 0.695 | ppl 1.62 | wps 65833 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 3610 | lr 0.000220393 | gnorm 0.400 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 981 | train_wall 5863\n",
      "| epoch 034 | valid on 'valid' subset | loss 2.410 | nll_loss 0.472 | ppl 1.39 | num_updates 3610 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint34.pt (epoch 34 @ 3610 updates) (writing took 2.858826160430908 seconds)\n",
      "| epoch 035:   1000 / 1205 loss=2.164, nll_loss=0.602, ppl=1.52, wps=64906, ups=4, wpb=15471.042, bsz=736.716, num_updates=4611, lr=0.000281477, gnorm=0.330, clip=0.000, oom=0.000, loss_scale=4.000, wall=1263, train_wall=6058\n",
      "| epoch 035 | loss 2.178 | nll_loss 0.618 | ppl 1.53 | wps 64805 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 4815 | lr 0.000293926 | gnorm 0.327 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 1312 | train_wall 6099\n",
      "| epoch 035 | valid on 'valid' subset | loss 2.347 | nll_loss 0.437 | ppl 1.35 | num_updates 4815 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint35.pt (epoch 35 @ 4815 updates) (writing took 3.1570441722869873 seconds)\n",
      "| epoch 036:   1000 / 1205 loss=2.141, nll_loss=0.583, ppl=1.50, wps=64312, ups=4, wpb=15473.392, bsz=736.828, num_updates=5816, lr=0.000355009, gnorm=0.309, clip=0.000, oom=0.000, loss_scale=4.000, wall=1596, train_wall=6295\n",
      "| epoch 036 | loss 2.142 | nll_loss 0.585 | ppl 1.50 | wps 64335 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 6020 | lr 0.000367458 | gnorm 0.311 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 1645 | train_wall 6335\n",
      "| epoch 036 | valid on 'valid' subset | loss 2.280 | nll_loss 0.421 | ppl 1.34 | num_updates 6020 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint36.pt (epoch 36 @ 6020 updates) (writing took 3.00502347946167 seconds)\n",
      "| epoch 037:   1000 / 1205 loss=2.124, nll_loss=0.567, ppl=1.48, wps=65120, ups=4, wpb=15480.168, bsz=737.151, num_updates=7021, lr=0.000428542, gnorm=0.326, clip=0.000, oom=0.000, loss_scale=4.000, wall=1926, train_wall=6530\n",
      "| WARNING: overflow detected, setting loss scale to: 2.0\n",
      "| epoch 037 | loss 2.124 | nll_loss 0.567 | ppl 1.48 | wps 64952 | ups 4 | wpb 15467.948 | bsz 736.569 | num_updates 7224 | lr 0.00044093 | gnorm 0.324 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 1975 | train_wall 6570\n",
      "| epoch 037 | valid on 'valid' subset | loss 2.255 | nll_loss 0.409 | ppl 1.33 | num_updates 7224 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint37.pt (epoch 37 @ 7224 updates) (writing took 3.1318564414978027 seconds)\n",
      "| epoch 038:   1000 / 1205 loss=2.120, nll_loss=0.566, ppl=1.48, wps=64871, ups=4, wpb=15480.839, bsz=737.183, num_updates=8225, lr=0.000498996, gnorm=0.328, clip=0.000, oom=0.000, loss_scale=2.000, wall=2257, train_wall=6765\n",
      "| epoch 038 | loss 2.114 | nll_loss 0.559 | ppl 1.47 | wps 65065 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 8429 | lr 0.000492921 | gnorm 0.328 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 2305 | train_wall 6804\n",
      "| epoch 038 | valid on 'valid' subset | loss 2.256 | nll_loss 0.398 | ppl 1.32 | num_updates 8429 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint38.pt (epoch 38 @ 8429 updates) (writing took 3.0481021404266357 seconds)\n",
      "| epoch 039:   1000 / 1205 loss=2.112, nll_loss=0.559, ppl=1.47, wps=65325, ups=4, wpb=15474.063, bsz=736.860, num_updates=9430, lr=0.000466025, gnorm=0.331, clip=0.000, oom=0.000, loss_scale=2.000, wall=2584, train_wall=6998\n",
      "| epoch 039 | loss 2.103 | nll_loss 0.549 | ppl 1.46 | wps 65203 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 9634 | lr 0.000461064 | gnorm 0.339 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 2633 | train_wall 7038\n",
      "| epoch 039 | valid on 'valid' subset | loss 2.237 | nll_loss 0.393 | ppl 1.31 | num_updates 9634 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint39.pt (epoch 39 @ 9634 updates) (writing took 3.0602104663848877 seconds)\n",
      "| epoch 040:   1000 / 1205 loss=2.085, nll_loss=0.531, ppl=1.45, wps=64054, ups=4, wpb=15472.720, bsz=736.796, num_updates=10635, lr=0.00043883, gnorm=0.315, clip=0.000, oom=0.000, loss_scale=2.000, wall=2918, train_wall=7235\n",
      "| epoch 040 | loss 2.086 | nll_loss 0.532 | ppl 1.45 | wps 64105 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 10839 | lr 0.000434681 | gnorm 0.317 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 2967 | train_wall 7275\n",
      "| epoch 040 | valid on 'valid' subset | loss 2.219 | nll_loss 0.390 | ppl 1.31 | num_updates 10839 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint40.pt (epoch 40 @ 10839 updates) (writing took 3.0839245319366455 seconds)\n",
      "| epoch 041:   1000 / 1205 loss=2.085, nll_loss=0.534, ppl=1.45, wps=64479, ups=4, wpb=15474.734, bsz=736.892, num_updates=11840, lr=0.0004159, gnorm=0.301, clip=0.000, oom=0.000, loss_scale=2.000, wall=3250, train_wall=7471\n",
      "| epoch 041 | loss 2.074 | nll_loss 0.521 | ppl 1.43 | wps 64597 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 12044 | lr 0.000412363 | gnorm 0.297 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 3298 | train_wall 7511\n",
      "| epoch 041 | valid on 'valid' subset | loss 2.222 | nll_loss 0.381 | ppl 1.30 | num_updates 12044 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint41.pt (epoch 41 @ 12044 updates) (writing took 3.0450515747070312 seconds)\n",
      "| epoch 042:   1000 / 1205 loss=2.055, nll_loss=0.501, ppl=1.42, wps=64876, ups=4, wpb=15472.385, bsz=736.780, num_updates=13045, lr=0.000396226, gnorm=0.291, clip=0.000, oom=0.000, loss_scale=2.000, wall=3580, train_wall=7706\n",
      "| epoch 042 | loss 2.062 | nll_loss 0.509 | ppl 1.42 | wps 64798 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 13249 | lr 0.000393164 | gnorm 0.292 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 3629 | train_wall 7746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 042 | valid on 'valid' subset | loss 2.211 | nll_loss 0.383 | ppl 1.30 | num_updates 13249 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint42.pt (epoch 42 @ 13249 updates) (writing took 2.9619698524475098 seconds)\n",
      "| epoch 043:   1000 / 1205 loss=2.062, nll_loss=0.511, ppl=1.43, wps=65359, ups=4, wpb=15474.399, bsz=736.876, num_updates=14250, lr=0.000379103, gnorm=0.289, clip=0.000, oom=0.000, loss_scale=2.000, wall=3909, train_wall=7941\n",
      "| epoch 043 | loss 2.052 | nll_loss 0.499 | ppl 1.41 | wps 65347 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 14454 | lr 0.000376418 | gnorm 0.285 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 3958 | train_wall 7980\n",
      "| epoch 043 | valid on 'valid' subset | loss 2.193 | nll_loss 0.368 | ppl 1.29 | num_updates 14454 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint43.pt (epoch 43 @ 14454 updates) (writing took 3.231590986251831 seconds)\n",
      "| epoch 044:   1000 / 1205 loss=2.056, nll_loss=0.504, ppl=1.42, wps=65780, ups=4, wpb=15475.070, bsz=736.908, num_updates=15455, lr=0.000364024, gnorm=0.279, clip=0.000, oom=0.000, loss_scale=4.000, wall=4235, train_wall=8173\n",
      "| epoch 044 | loss 2.044 | nll_loss 0.491 | ppl 1.41 | wps 65488 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 15659 | lr 0.000361645 | gnorm 0.275 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 4284 | train_wall 8213\n",
      "| epoch 044 | valid on 'valid' subset | loss 2.181 | nll_loss 0.365 | ppl 1.29 | num_updates 15659 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint44.pt (epoch 44 @ 15659 updates) (writing took 3.216524839401245 seconds)\n",
      "| epoch 045:   1000 / 1205 loss=2.033, nll_loss=0.480, ppl=1.39, wps=64852, ups=4, wpb=15473.056, bsz=736.812, num_updates=16660, lr=0.000350613, gnorm=0.269, clip=0.000, oom=0.000, loss_scale=4.000, wall=4566, train_wall=8409\n",
      "| epoch 045 | loss 2.038 | nll_loss 0.486 | ppl 1.40 | wps 64721 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 16864 | lr 0.000348485 | gnorm 0.269 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 4615 | train_wall 8449\n",
      "| epoch 045 | valid on 'valid' subset | loss 2.186 | nll_loss 0.359 | ppl 1.28 | num_updates 16864 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint45.pt (epoch 45 @ 16864 updates) (writing took 3.2297418117523193 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 2.0\n",
      "| epoch 046:   1000 / 1205 loss=2.033, nll_loss=0.481, ppl=1.40, wps=64326, ups=4, wpb=15473.409, bsz=736.829, num_updates=17864, lr=0.000338591, gnorm=0.256, clip=0.000, oom=0.000, loss_scale=2.000, wall=4899, train_wall=8645\n",
      "| epoch 046 | loss 2.030 | nll_loss 0.478 | ppl 1.39 | wps 64328 | ups 4 | wpb 15467.948 | bsz 736.569 | num_updates 18068 | lr 0.000336674 | gnorm 0.258 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 4948 | train_wall 8685\n",
      "| epoch 046 | valid on 'valid' subset | loss 2.185 | nll_loss 0.358 | ppl 1.28 | num_updates 18068 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint46.pt (epoch 46 @ 18068 updates) (writing took 3.0759565830230713 seconds)\n",
      "| epoch 047:   1000 / 1205 loss=2.024, nll_loss=0.472, ppl=1.39, wps=64925, ups=4, wpb=15473.392, bsz=736.828, num_updates=19069, lr=0.000327719, gnorm=0.261, clip=0.000, oom=0.000, loss_scale=2.000, wall=5229, train_wall=8881\n",
      "| epoch 047 | loss 2.024 | nll_loss 0.472 | ppl 1.39 | wps 64940 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 19273 | lr 0.00032598 | gnorm 0.257 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 5278 | train_wall 8921\n",
      "| epoch 047 | valid on 'valid' subset | loss 2.180 | nll_loss 0.354 | ppl 1.28 | num_updates 19273 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint47.pt (epoch 47 @ 19273 updates) (writing took 3.03476619720459 seconds)\n",
      "| epoch 048:   1000 / 1205 loss=2.025, nll_loss=0.473, ppl=1.39, wps=65945, ups=4, wpb=15475.070, bsz=736.908, num_updates=20274, lr=0.00031783, gnorm=0.253, clip=0.000, oom=0.000, loss_scale=2.000, wall=5555, train_wall=9114\n",
      "| epoch 048 | loss 2.019 | nll_loss 0.467 | ppl 1.38 | wps 66032 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 20478 | lr 0.000316243 | gnorm 0.257 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 5602 | train_wall 9152\n",
      "| epoch 048 | valid on 'valid' subset | loss 2.172 | nll_loss 0.353 | ppl 1.28 | num_updates 20478 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint48.pt (epoch 48 @ 20478 updates) (writing took 3.056060552597046 seconds)\n",
      "| epoch 049:   1000 / 1205 loss=2.011, nll_loss=0.458, ppl=1.37, wps=64691, ups=4, wpb=15474.063, bsz=736.860, num_updates=21479, lr=0.000308786, gnorm=0.248, clip=0.000, oom=0.000, loss_scale=2.000, wall=5884, train_wall=9348\n",
      "| epoch 049 | loss 2.014 | nll_loss 0.461 | ppl 1.38 | wps 64676 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 21683 | lr 0.00030733 | gnorm 0.248 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 5933 | train_wall 9388\n",
      "| epoch 049 | valid on 'valid' subset | loss 2.175 | nll_loss 0.351 | ppl 1.28 | num_updates 21683 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint49.pt (epoch 49 @ 21683 updates) (writing took 3.0265097618103027 seconds)\n",
      "| epoch 050:   1000 / 1205 loss=1.991, nll_loss=0.436, ppl=1.35, wps=64423, ups=4, wpb=15470.035, bsz=736.668, num_updates=22684, lr=0.000300473, gnorm=0.253, clip=0.000, oom=0.000, loss_scale=2.000, wall=6216, train_wall=9584\n",
      "| epoch 050 | loss 2.011 | nll_loss 0.458 | ppl 1.37 | wps 64304 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 22888 | lr 0.000299131 | gnorm 0.255 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 6266 | train_wall 9624\n",
      "| epoch 050 | valid on 'valid' subset | loss 2.169 | nll_loss 0.347 | ppl 1.27 | num_updates 22888 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint50.pt (epoch 50 @ 22888 updates) (writing took 3.1230785846710205 seconds)\n",
      "| epoch 051:   1000 / 1205 loss=2.007, nll_loss=0.455, ppl=1.37, wps=64750, ups=4, wpb=15479.832, bsz=737.135, num_updates=23889, lr=0.000292797, gnorm=0.247, clip=0.000, oom=0.000, loss_scale=2.000, wall=6548, train_wall=9820\n",
      "| epoch 051 | loss 2.007 | nll_loss 0.454 | ppl 1.37 | wps 64651 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 24093 | lr 0.000291554 | gnorm 0.250 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 6597 | train_wall 9860\n",
      "| epoch 051 | valid on 'valid' subset | loss 2.166 | nll_loss 0.345 | ppl 1.27 | num_updates 24093 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint51.pt (epoch 51 @ 24093 updates) (writing took 3.060830593109131 seconds)\n",
      "| epoch 052:   1000 / 1205 loss=1.998, nll_loss=0.444, ppl=1.36, wps=64550, ups=4, wpb=15472.720, bsz=736.796, num_updates=25094, lr=0.00028568, gnorm=0.231, clip=0.000, oom=0.000, loss_scale=2.000, wall=6880, train_wall=10056\n",
      "| epoch 052 | loss 2.003 | nll_loss 0.450 | ppl 1.37 | wps 64824 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 25298 | lr 0.000284526 | gnorm 0.232 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 6928 | train_wall 10095\n",
      "| epoch 052 | valid on 'valid' subset | loss 2.159 | nll_loss 0.344 | ppl 1.27 | num_updates 25298 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint52.pt (epoch 52 @ 25298 updates) (writing took 2.9872708320617676 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 2.0\n",
      "| epoch 053:   1000 / 1205 loss=1.981, nll_loss=0.426, ppl=1.34, wps=65484, ups=4, wpb=15470.385, bsz=736.685, num_updates=26298, lr=0.000279064, gnorm=0.233, clip=0.000, oom=0.000, loss_scale=2.000, wall=7206, train_wall=10289\n",
      "| epoch 053 | loss 1.999 | nll_loss 0.446 | ppl 1.36 | wps 65421 | ups 4 | wpb 15467.948 | bsz 736.569 | num_updates 26502 | lr 0.000277988 | gnorm 0.234 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 7254 | train_wall 10328\n",
      "| epoch 053 | valid on 'valid' subset | loss 2.156 | nll_loss 0.342 | ppl 1.27 | num_updates 26502 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint53.pt (epoch 53 @ 26502 updates) (writing took 3.1162660121917725 seconds)\n",
      "| epoch 054:   1000 / 1205 loss=1.995, nll_loss=0.442, ppl=1.36, wps=64620, ups=4, wpb=15473.056, bsz=736.812, num_updates=27503, lr=0.000272882, gnorm=0.233, clip=0.000, oom=0.000, loss_scale=2.000, wall=7536, train_wall=10525\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 054 | loss 1.997 | nll_loss 0.444 | ppl 1.36 | wps 64599 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 27707 | lr 0.000271876 | gnorm 0.239 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 7585 | train_wall 10564\n",
      "| epoch 054 | valid on 'valid' subset | loss 2.161 | nll_loss 0.343 | ppl 1.27 | num_updates 27707 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint54.pt (epoch 54 @ 27707 updates) (writing took 3.1028738021850586 seconds)\n",
      "| epoch 055:   1000 / 1205 loss=1.995, nll_loss=0.443, ppl=1.36, wps=65459, ups=4, wpb=15473.392, bsz=736.828, num_updates=28708, lr=0.000267094, gnorm=0.237, clip=0.000, oom=0.000, loss_scale=2.000, wall=7866, train_wall=10759\n",
      "| epoch 055 | loss 1.995 | nll_loss 0.442 | ppl 1.36 | wps 65358 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 28912 | lr 0.00026615 | gnorm 0.235 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 7915 | train_wall 10799\n",
      "| epoch 055 | valid on 'valid' subset | loss 2.153 | nll_loss 0.340 | ppl 1.27 | num_updates 28912 | best_loss 2.04018\n",
      "| epoch 056 | loss 1.992 | nll_loss 0.440 | ppl 1.36 | wps 65103 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 30117 | lr 0.000260771 | gnorm 0.234 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 8243 | train_wall 11033\n",
      "| epoch 056 | valid on 'valid' subset | loss 2.155 | nll_loss 0.338 | ppl 1.26 | num_updates 30117 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint56.pt (epoch 56 @ 30117 updates) (writing took 3.223935604095459 seconds)\n",
      "| epoch 057:   1000 / 1205 loss=1.987, nll_loss=0.433, ppl=1.35, wps=65534, ups=4, wpb=15473.056, bsz=736.812, num_updates=31118, lr=0.000256542, gnorm=0.226, clip=0.000, oom=0.000, loss_scale=2.000, wall=8522, train_wall=11228\n",
      "| epoch 057 | loss 1.989 | nll_loss 0.436 | ppl 1.35 | wps 65637 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 31322 | lr 0.000255706 | gnorm 0.227 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 8570 | train_wall 11267\n",
      "| epoch 057 | valid on 'valid' subset | loss 2.155 | nll_loss 0.340 | ppl 1.27 | num_updates 31322 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint57.pt (epoch 57 @ 31322 updates) (writing took 3.0782666206359863 seconds)\n",
      "| epoch 058:   1000 / 1205 loss=1.986, nll_loss=0.433, ppl=1.35, wps=65595, ups=4, wpb=15473.392, bsz=736.828, num_updates=32323, lr=0.000251715, gnorm=0.226, clip=0.000, oom=0.000, loss_scale=2.000, wall=8848, train_wall=11461\n",
      "| epoch 058 | loss 1.986 | nll_loss 0.433 | ppl 1.35 | wps 65470 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 32527 | lr 0.000250924 | gnorm 0.227 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 8897 | train_wall 11501\n",
      "| epoch 058 | valid on 'valid' subset | loss 2.158 | nll_loss 0.339 | ppl 1.26 | num_updates 32527 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint58.pt (epoch 58 @ 32527 updates) (writing took 3.105429172515869 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 1.0\n",
      "| epoch 059:   1000 / 1205 loss=1.981, nll_loss=0.427, ppl=1.34, wps=64997, ups=4, wpb=15472.737, bsz=736.797, num_updates=33527, lr=0.000247154, gnorm=0.226, clip=0.000, oom=0.000, loss_scale=1.000, wall=9177, train_wall=11696\n",
      "| epoch 059 | loss 1.984 | nll_loss 0.431 | ppl 1.35 | wps 64975 | ups 4 | wpb 15468.227 | bsz 736.582 | num_updates 33731 | lr 0.000246405 | gnorm 0.227 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 9226 | train_wall 11737\n",
      "| epoch 059 | valid on 'valid' subset | loss 2.159 | nll_loss 0.342 | ppl 1.27 | num_updates 33731 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint59.pt (epoch 59 @ 33731 updates) (writing took 3.0114364624023438 seconds)\n",
      "| epoch 060:   1000 / 1205 loss=1.982, nll_loss=0.429, ppl=1.35, wps=64684, ups=4, wpb=15473.727, bsz=736.844, num_updates=34732, lr=0.000242829, gnorm=0.218, clip=0.000, oom=0.000, loss_scale=1.000, wall=9508, train_wall=11932\n",
      "| epoch 060 | loss 1.981 | nll_loss 0.427 | ppl 1.34 | wps 64694 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 34936 | lr 0.000242119 | gnorm 0.217 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 9557 | train_wall 11972\n",
      "| epoch 060 | valid on 'valid' subset | loss 2.160 | nll_loss 0.339 | ppl 1.27 | num_updates 34936 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint60.pt (epoch 60 @ 34936 updates) (writing took 3.1590933799743652 seconds)\n",
      "| epoch 061:   1000 / 1205 loss=1.978, nll_loss=0.425, ppl=1.34, wps=65488, ups=4, wpb=15473.392, bsz=736.828, num_updates=35937, lr=0.000238723, gnorm=0.226, clip=0.000, oom=0.000, loss_scale=1.000, wall=9838, train_wall=12167\n",
      "| epoch 061 | loss 1.981 | nll_loss 0.428 | ppl 1.35 | wps 65504 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 36141 | lr 0.000238048 | gnorm 0.227 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 9886 | train_wall 12206\n",
      "| epoch 061 | valid on 'valid' subset | loss 2.152 | nll_loss 0.337 | ppl 1.26 | num_updates 36141 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint61.pt (epoch 61 @ 36141 updates) (writing took 2.9509494304656982 seconds)\n",
      "| epoch 062:   1000 / 1205 loss=1.971, nll_loss=0.417, ppl=1.33, wps=65441, ups=4, wpb=15472.720, bsz=736.796, num_updates=37142, lr=0.000234819, gnorm=0.219, clip=0.000, oom=0.000, loss_scale=1.000, wall=10166, train_wall=12400\n",
      "| epoch 062 | loss 1.978 | nll_loss 0.425 | ppl 1.34 | wps 65410 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 37346 | lr 0.000234176 | gnorm 0.220 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 10214 | train_wall 12440\n",
      "| epoch 062 | valid on 'valid' subset | loss 2.149 | nll_loss 0.337 | ppl 1.26 | num_updates 37346 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint62.pt (epoch 62 @ 37346 updates) (writing took 3.3124661445617676 seconds)\n",
      "| epoch 063:   1000 / 1205 loss=1.983, nll_loss=0.430, ppl=1.35, wps=65604, ups=4, wpb=15481.846, bsz=737.231, num_updates=38347, lr=0.0002311, gnorm=0.218, clip=0.000, oom=0.000, loss_scale=1.000, wall=10492, train_wall=12633\n",
      "| epoch 063 | loss 1.974 | nll_loss 0.421 | ppl 1.34 | wps 65511 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 38551 | lr 0.000230487 | gnorm 0.216 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 10541 | train_wall 12673\n",
      "| epoch 063 | valid on 'valid' subset | loss 2.152 | nll_loss 0.337 | ppl 1.26 | num_updates 38551 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint63.pt (epoch 63 @ 38551 updates) (writing took 3.2294487953186035 seconds)\n",
      "| epoch 064:   1000 / 1205 loss=1.976, nll_loss=0.423, ppl=1.34, wps=64879, ups=4, wpb=15474.734, bsz=736.892, num_updates=39552, lr=0.000227552, gnorm=0.212, clip=0.000, oom=0.000, loss_scale=1.000, wall=10822, train_wall=12868\n",
      "| epoch 064 | loss 1.973 | nll_loss 0.420 | ppl 1.34 | wps 64844 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 39756 | lr 0.000226967 | gnorm 0.211 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 10870 | train_wall 12908\n",
      "| epoch 064 | valid on 'valid' subset | loss 2.149 | nll_loss 0.333 | ppl 1.26 | num_updates 39756 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint64.pt (epoch 64 @ 39756 updates) (writing took 3.185462236404419 seconds)\n",
      "| epoch 065:   1000 / 1205 loss=1.976, nll_loss=0.422, ppl=1.34, wps=64678, ups=4, wpb=15474.063, bsz=736.860, num_updates=40757, lr=0.000224163, gnorm=0.224, clip=0.000, oom=0.000, loss_scale=1.000, wall=11152, train_wall=13104\n",
      "| epoch 065 | loss 1.971 | nll_loss 0.418 | ppl 1.34 | wps 64778 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 40961 | lr 0.000223604 | gnorm 0.223 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 11200 | train_wall 13144\n",
      "| epoch 065 | valid on 'valid' subset | loss 2.152 | nll_loss 0.336 | ppl 1.26 | num_updates 40961 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint65.pt (epoch 65 @ 40961 updates) (writing took 3.0184779167175293 seconds)\n",
      "| epoch 066:   1000 / 1205 loss=1.966, nll_loss=0.412, ppl=1.33, wps=65406, ups=4, wpb=15472.049, bsz=736.764, num_updates=41962, lr=0.000220921, gnorm=0.204, clip=0.000, oom=0.000, loss_scale=2.000, wall=11480, train_wall=13338\n",
      "| epoch 066 | loss 1.970 | nll_loss 0.417 | ppl 1.33 | wps 65416 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 42166 | lr 0.000220386 | gnorm 0.204 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 11528 | train_wall 13378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 066 | valid on 'valid' subset | loss 2.148 | nll_loss 0.331 | ppl 1.26 | num_updates 42166 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint66.pt (epoch 66 @ 42166 updates) (writing took 3.1838581562042236 seconds)\n",
      "| epoch 067:   1000 / 1205 loss=1.968, nll_loss=0.414, ppl=1.33, wps=65070, ups=4, wpb=15474.063, bsz=736.860, num_updates=43167, lr=0.000217816, gnorm=0.211, clip=0.000, oom=0.000, loss_scale=2.000, wall=11808, train_wall=13572\n",
      "| epoch 067 | loss 1.968 | nll_loss 0.414 | ppl 1.33 | wps 65139 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 43371 | lr 0.000217303 | gnorm 0.210 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 11857 | train_wall 13612\n",
      "| epoch 067 | valid on 'valid' subset | loss 2.152 | nll_loss 0.333 | ppl 1.26 | num_updates 43371 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint67.pt (epoch 67 @ 43371 updates) (writing took 3.0098156929016113 seconds)\n",
      "| epoch 068:   1000 / 1205 loss=1.966, nll_loss=0.412, ppl=1.33, wps=64815, ups=4, wpb=15480.168, bsz=737.151, num_updates=44372, lr=0.000214838, gnorm=0.213, clip=0.000, oom=0.000, loss_scale=2.000, wall=12138, train_wall=13808\n",
      "| epoch 068 | loss 1.968 | nll_loss 0.414 | ppl 1.33 | wps 64725 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 44576 | lr 0.000214346 | gnorm 0.213 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 12187 | train_wall 13848\n",
      "| epoch 068 | valid on 'valid' subset | loss 2.152 | nll_loss 0.334 | ppl 1.26 | num_updates 44576 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint68.pt (epoch 68 @ 44576 updates) (writing took 3.1621859073638916 seconds)\n",
      "| epoch 069:   1000 / 1205 loss=1.973, nll_loss=0.421, ppl=1.34, wps=65081, ups=4, wpb=15475.406, bsz=736.924, num_updates=45577, lr=0.000211979, gnorm=0.210, clip=0.000, oom=0.000, loss_scale=2.000, wall=12468, train_wall=14043\n",
      "| epoch 069 | loss 1.965 | nll_loss 0.411 | ppl 1.33 | wps 65347 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 45781 | lr 0.000211506 | gnorm 0.210 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 12515 | train_wall 14082\n",
      "| epoch 069 | valid on 'valid' subset | loss 2.153 | nll_loss 0.332 | ppl 1.26 | num_updates 45781 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint69.pt (epoch 69 @ 45781 updates) (writing took 3.0938167572021484 seconds)\n",
      "| epoch 070:   1000 / 1205 loss=1.958, nll_loss=0.403, ppl=1.32, wps=65623, ups=4, wpb=15472.720, bsz=736.796, num_updates=46782, lr=0.000209231, gnorm=0.205, clip=0.000, oom=0.000, loss_scale=2.000, wall=12795, train_wall=14276\n",
      "| epoch 070 | loss 1.965 | nll_loss 0.411 | ppl 1.33 | wps 65638 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 46986 | lr 0.000208776 | gnorm 0.205 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 12843 | train_wall 14316\n",
      "| epoch 070 | valid on 'valid' subset | loss 2.150 | nll_loss 0.331 | ppl 1.26 | num_updates 46986 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint70.pt (epoch 70 @ 46986 updates) (writing took 2.9257876873016357 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 1.0\n",
      "| epoch 071:   1000 / 1205 loss=1.954, nll_loss=0.399, ppl=1.32, wps=65212, ups=4, wpb=15472.401, bsz=736.781, num_updates=47986, lr=0.000206589, gnorm=0.211, clip=0.000, oom=0.000, loss_scale=1.000, wall=13123, train_wall=14510\n",
      "| epoch 071 | loss 1.962 | nll_loss 0.408 | ppl 1.33 | wps 65312 | ups 4 | wpb 15467.948 | bsz 736.569 | num_updates 48190 | lr 0.000206152 | gnorm 0.210 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 13170 | train_wall 14549\n",
      "| epoch 071 | valid on 'valid' subset | loss 2.150 | nll_loss 0.334 | ppl 1.26 | num_updates 48190 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint71.pt (epoch 71 @ 48190 updates) (writing took 3.032637119293213 seconds)\n",
      "| epoch 072:   1000 / 1205 loss=1.960, nll_loss=0.406, ppl=1.33, wps=65503, ups=4, wpb=15473.727, bsz=736.844, num_updates=49191, lr=0.000204043, gnorm=0.215, clip=0.000, oom=0.000, loss_scale=1.000, wall=13449, train_wall=14743\n",
      "| epoch 072 | loss 1.961 | nll_loss 0.407 | ppl 1.33 | wps 65477 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 49395 | lr 0.000203621 | gnorm 0.213 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 13497 | train_wall 14783\n",
      "| epoch 072 | valid on 'valid' subset | loss 2.151 | nll_loss 0.335 | ppl 1.26 | num_updates 49395 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint72.pt (epoch 72 @ 49395 updates) (writing took 3.0720980167388916 seconds)\n",
      "| epoch 073:   1000 / 1205 loss=1.961, nll_loss=0.407, ppl=1.33, wps=65530, ups=4, wpb=15480.503, bsz=737.167, num_updates=50396, lr=0.000201589, gnorm=0.217, clip=0.000, oom=0.000, loss_scale=1.000, wall=13776, train_wall=14976\n",
      "| epoch 073 | loss 1.960 | nll_loss 0.405 | ppl 1.32 | wps 65488 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 50600 | lr 0.000201182 | gnorm 0.214 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 13824 | train_wall 15016\n",
      "| epoch 073 | valid on 'valid' subset | loss 2.149 | nll_loss 0.331 | ppl 1.26 | num_updates 50600 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint73.pt (epoch 73 @ 50600 updates) (writing took 3.3186256885528564 seconds)\n",
      "| epoch 074:   1000 / 1205 loss=1.972, nll_loss=0.420, ppl=1.34, wps=65848, ups=4, wpb=15475.741, bsz=736.940, num_updates=51601, lr=0.000199221, gnorm=0.225, clip=0.000, oom=0.000, loss_scale=1.000, wall=14102, train_wall=15210\n",
      "| epoch 074 | loss 1.958 | nll_loss 0.404 | ppl 1.32 | wps 65538 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 51805 | lr 0.000198829 | gnorm 0.222 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 14151 | train_wall 15250\n",
      "| epoch 074 | valid on 'valid' subset | loss 2.157 | nll_loss 0.338 | ppl 1.26 | num_updates 51805 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint74.pt (epoch 74 @ 51805 updates) (writing took 3.2201480865478516 seconds)\n",
      "| epoch 075:   1000 / 1205 loss=1.957, nll_loss=0.403, ppl=1.32, wps=64989, ups=4, wpb=15478.825, bsz=737.087, num_updates=52806, lr=0.000196935, gnorm=0.207, clip=0.000, oom=0.000, loss_scale=1.000, wall=14433, train_wall=15445\n",
      "| epoch 075 | loss 1.958 | nll_loss 0.404 | ppl 1.32 | wps 64991 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 53010 | lr 0.000196556 | gnorm 0.206 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 14481 | train_wall 15484\n",
      "| epoch 075 | valid on 'valid' subset | loss 2.148 | nll_loss 0.331 | ppl 1.26 | num_updates 53010 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint75.pt (epoch 75 @ 53010 updates) (writing took 3.1721882820129395 seconds)\n",
      "| epoch 076:   1000 / 1205 loss=1.956, nll_loss=0.402, ppl=1.32, wps=66102, ups=4, wpb=15473.392, bsz=736.828, num_updates=54011, lr=0.000194726, gnorm=0.215, clip=0.000, oom=0.000, loss_scale=1.000, wall=14758, train_wall=15676\n",
      "| epoch 076 | loss 1.955 | nll_loss 0.401 | ppl 1.32 | wps 65804 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 54215 | lr 0.000194359 | gnorm 0.212 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 14807 | train_wall 15716\n",
      "| epoch 076 | valid on 'valid' subset | loss 2.153 | nll_loss 0.333 | ppl 1.26 | num_updates 54215 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint76.pt (epoch 76 @ 54215 updates) (writing took 3.1151885986328125 seconds)\n",
      "| epoch 077:   1000 / 1205 loss=1.959, nll_loss=0.405, ppl=1.32, wps=65240, ups=4, wpb=15473.727, bsz=736.844, num_updates=55216, lr=0.000192589, gnorm=0.216, clip=0.000, oom=0.000, loss_scale=1.000, wall=15087, train_wall=15911\n",
      "| epoch 077 | loss 1.957 | nll_loss 0.403 | ppl 1.32 | wps 65379 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 55420 | lr 0.000192235 | gnorm 0.212 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 15135 | train_wall 15950\n",
      "| epoch 077 | valid on 'valid' subset | loss 2.153 | nll_loss 0.334 | ppl 1.26 | num_updates 55420 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint77.pt (epoch 77 @ 55420 updates) (writing took 3.1116933822631836 seconds)\n",
      "| epoch 078:   1000 / 1205 loss=1.955, nll_loss=0.401, ppl=1.32, wps=65745, ups=4, wpb=15473.056, bsz=736.812, num_updates=56421, lr=0.000190522, gnorm=0.203, clip=0.000, oom=0.000, loss_scale=2.000, wall=15414, train_wall=16143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 078 | loss 1.954 | nll_loss 0.399 | ppl 1.32 | wps 65647 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 56625 | lr 0.000190178 | gnorm 0.203 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 15463 | train_wall 16183\n",
      "| epoch 078 | valid on 'valid' subset | loss 2.146 | nll_loss 0.331 | ppl 1.26 | num_updates 56625 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint78.pt (epoch 78 @ 56625 updates) (writing took 3.0891809463500977 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 1.0\n",
      "| epoch 079:   1000 / 1205 loss=1.947, nll_loss=0.392, ppl=1.31, wps=64763, ups=4, wpb=15472.401, bsz=736.781, num_updates=57625, lr=0.000188521, gnorm=0.203, clip=0.000, oom=0.000, loss_scale=1.000, wall=15744, train_wall=16379\n",
      "| epoch 079 | loss 1.952 | nll_loss 0.397 | ppl 1.32 | wps 64686 | ups 4 | wpb 15467.948 | bsz 736.569 | num_updates 57829 | lr 0.000188188 | gnorm 0.206 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 15793 | train_wall 16418\n",
      "| epoch 079 | valid on 'valid' subset | loss 2.148 | nll_loss 0.335 | ppl 1.26 | num_updates 57829 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint79.pt (epoch 79 @ 57829 updates) (writing took 3.082519292831421 seconds)\n",
      "| epoch 080:   1000 / 1205 loss=1.957, nll_loss=0.403, ppl=1.32, wps=65884, ups=4, wpb=15474.063, bsz=736.860, num_updates=58830, lr=0.00018658, gnorm=0.203, clip=0.000, oom=0.000, loss_scale=1.000, wall=16071, train_wall=16612\n",
      "| epoch 080 | loss 1.953 | nll_loss 0.398 | ppl 1.32 | wps 65783 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 59034 | lr 0.000186258 | gnorm 0.202 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 16119 | train_wall 16652\n",
      "| epoch 080 | valid on 'valid' subset | loss 2.153 | nll_loss 0.333 | ppl 1.26 | num_updates 59034 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint80.pt (epoch 80 @ 59034 updates) (writing took 3.0544111728668213 seconds)\n",
      "| epoch 081:   1000 / 1205 loss=1.952, nll_loss=0.397, ppl=1.32, wps=65323, ups=4, wpb=15480.168, bsz=737.151, num_updates=60035, lr=0.000184698, gnorm=0.214, clip=0.000, oom=0.000, loss_scale=1.000, wall=16399, train_wall=16847\n",
      "| epoch 081 | loss 1.951 | nll_loss 0.396 | ppl 1.32 | wps 65184 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 60239 | lr 0.000184385 | gnorm 0.214 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 16448 | train_wall 16886\n",
      "| epoch 081 | valid on 'valid' subset | loss 2.148 | nll_loss 0.330 | ppl 1.26 | num_updates 60239 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint81.pt (epoch 81 @ 60239 updates) (writing took 3.0175254344940186 seconds)\n",
      "| epoch 082:   1000 / 1205 loss=1.952, nll_loss=0.397, ppl=1.32, wps=65036, ups=4, wpb=15474.063, bsz=736.860, num_updates=61240, lr=0.000182872, gnorm=0.201, clip=0.000, oom=0.000, loss_scale=1.000, wall=16728, train_wall=17082\n",
      "| epoch 082 | loss 1.949 | nll_loss 0.395 | ppl 1.31 | wps 64997 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 61444 | lr 0.000182568 | gnorm 0.202 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 16777 | train_wall 17122\n",
      "| epoch 082 | valid on 'valid' subset | loss 2.149 | nll_loss 0.332 | ppl 1.26 | num_updates 61444 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint82.pt (epoch 82 @ 61444 updates) (writing took 3.1679086685180664 seconds)\n",
      "| epoch 083:   1000 / 1205 loss=1.947, nll_loss=0.393, ppl=1.31, wps=61361, ups=3, wpb=15473.392, bsz=736.828, num_updates=62445, lr=0.000181099, gnorm=0.211, clip=0.000, oom=0.000, loss_scale=1.000, wall=17074, train_wall=17326\n",
      "| epoch 083 | loss 1.949 | nll_loss 0.395 | ppl 1.31 | wps 61977 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 62649 | lr 0.000180804 | gnorm 0.210 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 17123 | train_wall 17366\n",
      "| epoch 083 | valid on 'valid' subset | loss 2.145 | nll_loss 0.333 | ppl 1.26 | num_updates 62649 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint83.pt (epoch 83 @ 62649 updates) (writing took 3.1029245853424072 seconds)\n",
      "| epoch 084:   1000 / 1205 loss=1.955, nll_loss=0.401, ppl=1.32, wps=64950, ups=4, wpb=15481.175, bsz=737.199, num_updates=63650, lr=0.000179377, gnorm=0.216, clip=0.000, oom=0.000, loss_scale=1.000, wall=17404, train_wall=17561\n",
      "| epoch 084 | loss 1.948 | nll_loss 0.393 | ppl 1.31 | wps 64897 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 63854 | lr 0.00017909 | gnorm 0.212 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 17452 | train_wall 17601\n",
      "| epoch 084 | valid on 'valid' subset | loss 2.149 | nll_loss 0.331 | ppl 1.26 | num_updates 63854 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint84.pt (epoch 84 @ 63854 updates) (writing took 3.2188453674316406 seconds)\n",
      "| epoch 085:   1000 / 1205 loss=1.945, nll_loss=0.390, ppl=1.31, wps=65968, ups=4, wpb=15479.832, bsz=737.135, num_updates=64855, lr=0.000177702, gnorm=0.199, clip=0.000, oom=0.000, loss_scale=2.000, wall=17730, train_wall=17795\n",
      "| epoch 085 | loss 1.947 | nll_loss 0.392 | ppl 1.31 | wps 66099 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 65059 | lr 0.000177424 | gnorm 0.202 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 17777 | train_wall 17834\n",
      "| epoch 085 | valid on 'valid' subset | loss 2.147 | nll_loss 0.331 | ppl 1.26 | num_updates 65059 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint85.pt (epoch 85 @ 65059 updates) (writing took 3.925100564956665 seconds)\n",
      "| epoch 086:   1000 / 1205 loss=1.939, nll_loss=0.383, ppl=1.30, wps=58019, ups=3, wpb=15471.713, bsz=736.748, num_updates=66060, lr=0.000176074, gnorm=0.198, clip=0.000, oom=0.000, loss_scale=2.000, wall=18092, train_wall=18049\n",
      "| epoch 086 | loss 1.945 | nll_loss 0.391 | ppl 1.31 | wps 58132 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 66264 | lr 0.000175803 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 18145 | train_wall 18093\n",
      "| epoch 086 | valid on 'valid' subset | loss 2.149 | nll_loss 0.331 | ppl 1.26 | num_updates 66264 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint86.pt (epoch 86 @ 66264 updates) (writing took 4.072176456451416 seconds)\n",
      "| epoch 087:   1000 / 1205 loss=1.941, nll_loss=0.385, ppl=1.31, wps=58672, ups=3, wpb=15472.720, bsz=736.796, num_updates=67265, lr=0.00017449, gnorm=0.214, clip=0.000, oom=0.000, loss_scale=2.000, wall=18459, train_wall=18305\n",
      "| epoch 087 | loss 1.945 | nll_loss 0.390 | ppl 1.31 | wps 58950 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 67469 | lr 0.000174226 | gnorm 0.210 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 18511 | train_wall 18348\n",
      "| epoch 087 | valid on 'valid' subset | loss 2.148 | nll_loss 0.329 | ppl 1.26 | num_updates 67469 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint87.pt (epoch 87 @ 67469 updates) (writing took 4.0263214111328125 seconds)\n",
      "| epoch 088:   1000 / 1205 loss=1.942, nll_loss=0.386, ppl=1.31, wps=58635, ups=3, wpb=15479.497, bsz=737.119, num_updates=68470, lr=0.000172948, gnorm=0.205, clip=0.000, oom=0.000, loss_scale=2.000, wall=18827, train_wall=18561\n",
      "| epoch 088 | loss 1.944 | nll_loss 0.389 | ppl 1.31 | wps 58509 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 68674 | lr 0.000172691 | gnorm 0.205 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 18881 | train_wall 18605\n",
      "| epoch 088 | valid on 'valid' subset | loss 2.151 | nll_loss 0.332 | ppl 1.26 | num_updates 68674 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint88.pt (epoch 88 @ 68674 updates) (writing took 3.969541311264038 seconds)\n",
      "| epoch 089:   1000 / 1205 loss=1.948, nll_loss=0.393, ppl=1.31, wps=65411, ups=3, wpb=15473.727, bsz=736.844, num_updates=69675, lr=0.000171446, gnorm=0.208, clip=0.000, oom=0.000, loss_scale=2.000, wall=19169, train_wall=18801\n",
      "| epoch 089 | loss 1.944 | nll_loss 0.389 | ppl 1.31 | wps 65272 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 69879 | lr 0.000171195 | gnorm 0.206 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 19218 | train_wall 18842\n",
      "| epoch 089 | valid on 'valid' subset | loss 2.145 | nll_loss 0.331 | ppl 1.26 | num_updates 69879 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint89.pt (epoch 89 @ 69879 updates) (writing took 3.6579082012176514 seconds)\n",
      "| epoch 090:   1000 / 1205 loss=1.942, nll_loss=0.387, ppl=1.31, wps=58568, ups=3, wpb=15473.727, bsz=736.844, num_updates=70880, lr=0.000169982, gnorm=0.202, clip=0.000, oom=0.000, loss_scale=2.000, wall=19525, train_wall=19054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 090 | loss 1.943 | nll_loss 0.387 | ppl 1.31 | wps 58437 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 71084 | lr 0.000169738 | gnorm 0.200 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 19580 | train_wall 19099\n",
      "| epoch 090 | valid on 'valid' subset | loss 2.150 | nll_loss 0.332 | ppl 1.26 | num_updates 71084 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint90.pt (epoch 90 @ 71084 updates) (writing took 3.9884862899780273 seconds)\n",
      "| epoch 091:   1000 / 1205 loss=1.935, nll_loss=0.379, ppl=1.30, wps=57762, ups=3, wpb=15472.385, bsz=736.780, num_updates=72085, lr=0.000168555, gnorm=0.197, clip=0.000, oom=0.000, loss_scale=2.000, wall=19895, train_wall=19316\n",
      "| epoch 091 | loss 1.941 | nll_loss 0.386 | ppl 1.31 | wps 57572 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 72289 | lr 0.000168317 | gnorm 0.198 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 19951 | train_wall 19360\n",
      "| epoch 091 | valid on 'valid' subset | loss 2.150 | nll_loss 0.331 | ppl 1.26 | num_updates 72289 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint91.pt (epoch 91 @ 72289 updates) (writing took 3.9804861545562744 seconds)\n",
      "| epoch 092:   1000 / 1205 loss=1.941, nll_loss=0.386, ppl=1.31, wps=58038, ups=3, wpb=15475.070, bsz=736.908, num_updates=73290, lr=0.000167164, gnorm=0.195, clip=0.000, oom=0.000, loss_scale=4.000, wall=20266, train_wall=19573\n",
      "| epoch 092 | loss 1.940 | nll_loss 0.384 | ppl 1.31 | wps 58071 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 73494 | lr 0.000166932 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 20320 | train_wall 19616\n",
      "| epoch 092 | valid on 'valid' subset | loss 2.144 | nll_loss 0.330 | ppl 1.26 | num_updates 73494 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint92.pt (epoch 92 @ 73494 updates) (writing took 3.963587999343872 seconds)\n",
      "| epoch 093:   1000 / 1205 loss=1.935, nll_loss=0.379, ppl=1.30, wps=63346, ups=3, wpb=15472.720, bsz=736.796, num_updates=74495, lr=0.000165806, gnorm=0.202, clip=0.000, oom=0.000, loss_scale=4.000, wall=20613, train_wall=19817\n",
      "| epoch 093 | loss 1.940 | nll_loss 0.384 | ppl 1.31 | wps 63687 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 74699 | lr 0.00016558 | gnorm 0.201 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 20662 | train_wall 19857\n",
      "| epoch 093 | valid on 'valid' subset | loss 2.153 | nll_loss 0.333 | ppl 1.26 | num_updates 74699 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint93.pt (epoch 93 @ 74699 updates) (writing took 3.639946937561035 seconds)\n",
      "| epoch 094:   1000 / 1205 loss=1.941, nll_loss=0.386, ppl=1.31, wps=59686, ups=3, wpb=15479.832, bsz=737.135, num_updates=75700, lr=0.000164481, gnorm=0.211, clip=0.000, oom=0.000, loss_scale=4.000, wall=20964, train_wall=20067\n",
      "| epoch 094 | loss 1.940 | nll_loss 0.385 | ppl 1.31 | wps 59278 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 75904 | lr 0.00016426 | gnorm 0.210 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 21019 | train_wall 20110\n",
      "| epoch 094 | valid on 'valid' subset | loss 2.146 | nll_loss 0.330 | ppl 1.26 | num_updates 75904 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint94.pt (epoch 94 @ 75904 updates) (writing took 3.992720127105713 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 2.0\n",
      "| epoch 095:   1000 / 1205 loss=1.943, nll_loss=0.388, ppl=1.31, wps=57256, ups=3, wpb=15474.753, bsz=736.893, num_updates=76904, lr=0.000163189, gnorm=0.198, clip=0.000, oom=0.000, loss_scale=2.000, wall=21342, train_wall=20325\n",
      "| epoch 095 | loss 1.938 | nll_loss 0.382 | ppl 1.30 | wps 57246 | ups 3 | wpb 15467.948 | bsz 736.569 | num_updates 77108 | lr 0.000162973 | gnorm 0.199 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 21397 | train_wall 20370\n",
      "| epoch 095 | valid on 'valid' subset | loss 2.148 | nll_loss 0.333 | ppl 1.26 | num_updates 77108 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint95.pt (epoch 95 @ 77108 updates) (writing took 4.010393857955933 seconds)\n",
      "| epoch 096:   1000 / 1205 loss=1.946, nll_loss=0.392, ppl=1.31, wps=56790, ups=3, wpb=15475.070, bsz=736.908, num_updates=78109, lr=0.000161925, gnorm=0.208, clip=0.000, oom=0.000, loss_scale=2.000, wall=21723, train_wall=20587\n",
      "| epoch 096 | loss 1.937 | nll_loss 0.382 | ppl 1.30 | wps 57024 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 78313 | lr 0.000161714 | gnorm 0.206 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 21777 | train_wall 20631\n",
      "| epoch 096 | valid on 'valid' subset | loss 2.151 | nll_loss 0.332 | ppl 1.26 | num_updates 78313 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint96.pt (epoch 96 @ 78313 updates) (writing took 4.002050161361694 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 1.0\n",
      "| epoch 097:   1000 / 1205 loss=1.933, nll_loss=0.377, ppl=1.30, wps=60845, ups=3, wpb=15478.848, bsz=737.088, num_updates=79313, lr=0.000160691, gnorm=0.202, clip=0.000, oom=0.000, loss_scale=1.000, wall=22084, train_wall=20838\n",
      "| epoch 097 | loss 1.938 | nll_loss 0.382 | ppl 1.30 | wps 61595 | ups 3 | wpb 15468.227 | bsz 736.582 | num_updates 79517 | lr 0.000160485 | gnorm 0.205 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 22132 | train_wall 20878\n",
      "| epoch 097 | valid on 'valid' subset | loss 2.147 | nll_loss 0.328 | ppl 1.26 | num_updates 79517 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint97.pt (epoch 97 @ 79517 updates) (writing took 3.806627035140991 seconds)\n",
      "| epoch 098:   1000 / 1205 loss=1.941, nll_loss=0.386, ppl=1.31, wps=61838, ups=3, wpb=15474.734, bsz=736.892, num_updates=80518, lr=0.000159485, gnorm=0.195, clip=0.000, oom=0.000, loss_scale=1.000, wall=22426, train_wall=21083\n",
      "| epoch 098 | loss 1.936 | nll_loss 0.381 | ppl 1.30 | wps 61050 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 80722 | lr 0.000159283 | gnorm 0.198 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 22481 | train_wall 21127\n",
      "| epoch 098 | valid on 'valid' subset | loss 2.146 | nll_loss 0.329 | ppl 1.26 | num_updates 80722 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint98.pt (epoch 98 @ 80722 updates) (writing took 3.670187473297119 seconds)\n",
      "| epoch 099:   1000 / 1205 loss=1.930, nll_loss=0.374, ppl=1.30, wps=58542, ups=3, wpb=15471.713, bsz=736.748, num_updates=81723, lr=0.000158304, gnorm=0.204, clip=0.000, oom=0.000, loss_scale=1.000, wall=22799, train_wall=21344\n",
      "| epoch 099 | loss 1.936 | nll_loss 0.380 | ppl 1.30 | wps 58257 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 81927 | lr 0.000158107 | gnorm 0.204 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 22854 | train_wall 21389\n",
      "| epoch 099 | valid on 'valid' subset | loss 2.148 | nll_loss 0.332 | ppl 1.26 | num_updates 81927 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint99.pt (epoch 99 @ 81927 updates) (writing took 3.84432315826416 seconds)\n",
      "| epoch 100:   1000 / 1205 loss=1.937, nll_loss=0.381, ppl=1.30, wps=58985, ups=3, wpb=15480.503, bsz=737.167, num_updates=82928, lr=0.00015715, gnorm=0.196, clip=0.000, oom=0.000, loss_scale=1.000, wall=23165, train_wall=21601\n",
      "| epoch 100 | loss 1.935 | nll_loss 0.380 | ppl 1.30 | wps 59074 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 83132 | lr 0.000156957 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 23218 | train_wall 21644\n",
      "| epoch 100 | valid on 'valid' subset | loss 2.153 | nll_loss 0.331 | ppl 1.26 | num_updates 83132 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint100.pt (epoch 100 @ 83132 updates) (writing took 3.916836738586426 seconds)\n",
      "| epoch 101:   1000 / 1205 loss=1.938, nll_loss=0.383, ppl=1.30, wps=60916, ups=3, wpb=15474.734, bsz=736.892, num_updates=84133, lr=0.000156021, gnorm=0.200, clip=0.000, oom=0.000, loss_scale=1.000, wall=23521, train_wall=21850\n",
      "| epoch 101 | loss 1.934 | nll_loss 0.378 | ppl 1.30 | wps 61435 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 84337 | lr 0.000155832 | gnorm 0.199 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 23570 | train_wall 21890\n",
      "| epoch 101 | valid on 'valid' subset | loss 2.155 | nll_loss 0.334 | ppl 1.26 | num_updates 84337 | best_loss 2.04018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint101.pt (epoch 101 @ 84337 updates) (writing took 3.6303157806396484 seconds)\n",
      "| epoch 102:   1000 / 1205 loss=1.935, nll_loss=0.379, ppl=1.30, wps=62465, ups=3, wpb=15474.399, bsz=736.876, num_updates=85338, lr=0.000154915, gnorm=0.202, clip=0.000, oom=0.000, loss_scale=1.000, wall=23860, train_wall=22093\n",
      "| epoch 102 | loss 1.933 | nll_loss 0.377 | ppl 1.30 | wps 62023 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 85542 | lr 0.00015473 | gnorm 0.207 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 23913 | train_wall 22136\n",
      "| epoch 102 | valid on 'valid' subset | loss 2.157 | nll_loss 0.333 | ppl 1.26 | num_updates 85542 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint102.pt (epoch 102 @ 85542 updates) (writing took 3.942540168762207 seconds)\n",
      "| epoch 103:   1000 / 1205 loss=1.922, nll_loss=0.364, ppl=1.29, wps=60345, ups=3, wpb=15478.825, bsz=737.087, num_updates=86543, lr=0.000153833, gnorm=0.206, clip=0.000, oom=0.000, loss_scale=1.000, wall=24221, train_wall=22345\n",
      "| epoch 103 | loss 1.933 | nll_loss 0.377 | ppl 1.30 | wps 60179 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 86747 | lr 0.000153652 | gnorm 0.205 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 24274 | train_wall 22389\n",
      "| epoch 103 | valid on 'valid' subset | loss 2.148 | nll_loss 0.328 | ppl 1.26 | num_updates 86747 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint103.pt (epoch 103 @ 86747 updates) (writing took 3.909249782562256 seconds)\n",
      "| epoch 104:   1000 / 1205 loss=1.933, nll_loss=0.377, ppl=1.30, wps=58812, ups=3, wpb=15474.734, bsz=736.892, num_updates=87748, lr=0.000152773, gnorm=0.194, clip=0.000, oom=0.000, loss_scale=2.000, wall=24591, train_wall=22604\n",
      "| epoch 104 | loss 1.932 | nll_loss 0.376 | ppl 1.30 | wps 58787 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 87952 | lr 0.000152596 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 24645 | train_wall 22648\n",
      "| epoch 104 | valid on 'valid' subset | loss 2.151 | nll_loss 0.330 | ppl 1.26 | num_updates 87952 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint104.pt (epoch 104 @ 87952 updates) (writing took 3.6084818840026855 seconds)\n",
      "| epoch 105:   1000 / 1205 loss=1.939, nll_loss=0.384, ppl=1.30, wps=59442, ups=3, wpb=15475.070, bsz=736.908, num_updates=88953, lr=0.000151735, gnorm=0.204, clip=0.000, oom=0.000, loss_scale=2.000, wall=24955, train_wall=22858\n",
      "| epoch 105 | loss 1.931 | nll_loss 0.375 | ppl 1.30 | wps 60300 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 89157 | lr 0.000151561 | gnorm 0.200 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 25003 | train_wall 22898\n",
      "| epoch 105 | valid on 'valid' subset | loss 2.149 | nll_loss 0.330 | ppl 1.26 | num_updates 89157 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint105.pt (epoch 105 @ 89157 updates) (writing took 3.606868267059326 seconds)\n",
      "| epoch 106:   1000 / 1205 loss=1.929, nll_loss=0.373, ppl=1.29, wps=64233, ups=4, wpb=15473.727, bsz=736.844, num_updates=90158, lr=0.000150717, gnorm=0.199, clip=0.000, oom=0.000, loss_scale=2.000, wall=25287, train_wall=23095\n",
      "| epoch 106 | loss 1.930 | nll_loss 0.374 | ppl 1.30 | wps 63119 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 90362 | lr 0.000150547 | gnorm 0.200 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 25342 | train_wall 23138\n",
      "| epoch 106 | valid on 'valid' subset | loss 2.153 | nll_loss 0.332 | ppl 1.26 | num_updates 90362 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint106.pt (epoch 106 @ 90362 updates) (writing took 3.784520387649536 seconds)\n",
      "| epoch 107:   1000 / 1205 loss=1.938, nll_loss=0.383, ppl=1.30, wps=59087, ups=3, wpb=15474.734, bsz=736.892, num_updates=91363, lr=0.00014972, gnorm=0.205, clip=0.000, oom=0.000, loss_scale=2.000, wall=25651, train_wall=23349\n",
      "| epoch 107 | loss 1.930 | nll_loss 0.374 | ppl 1.30 | wps 59232 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 91567 | lr 0.000149553 | gnorm 0.200 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 25703 | train_wall 23393\n",
      "| epoch 107 | valid on 'valid' subset | loss 2.150 | nll_loss 0.331 | ppl 1.26 | num_updates 91567 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint107.pt (epoch 107 @ 91567 updates) (writing took 3.9305574893951416 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 1.0\n",
      "| epoch 108:   1000 / 1205 loss=1.928, nll_loss=0.372, ppl=1.29, wps=58911, ups=3, wpb=15472.065, bsz=736.765, num_updates=92567, lr=0.000148743, gnorm=0.200, clip=0.000, oom=0.000, loss_scale=1.000, wall=26014, train_wall=23605\n",
      "| epoch 108 | loss 1.930 | nll_loss 0.374 | ppl 1.30 | wps 59002 | ups 3 | wpb 15467.948 | bsz 736.569 | num_updates 92771 | lr 0.000148579 | gnorm 0.198 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 26067 | train_wall 23648\n",
      "| epoch 108 | valid on 'valid' subset | loss 2.152 | nll_loss 0.336 | ppl 1.26 | num_updates 92771 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint108.pt (epoch 108 @ 92771 updates) (writing took 3.8336737155914307 seconds)\n",
      "| epoch 109:   1000 / 1205 loss=1.937, nll_loss=0.382, ppl=1.30, wps=60050, ups=3, wpb=15474.399, bsz=736.876, num_updates=93772, lr=0.000147784, gnorm=0.199, clip=0.000, oom=0.000, loss_scale=1.000, wall=26377, train_wall=23857\n",
      "| epoch 109 | loss 1.929 | nll_loss 0.373 | ppl 1.29 | wps 60724 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 93976 | lr 0.000147624 | gnorm 0.201 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 26426 | train_wall 23898\n",
      "| epoch 109 | valid on 'valid' subset | loss 2.152 | nll_loss 0.331 | ppl 1.26 | num_updates 93976 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint109.pt (epoch 109 @ 93976 updates) (writing took 3.8167014122009277 seconds)\n",
      "| epoch 110:   1000 / 1205 loss=1.930, nll_loss=0.374, ppl=1.30, wps=65277, ups=4, wpb=15474.399, bsz=736.876, num_updates=94977, lr=0.000146844, gnorm=0.199, clip=0.000, oom=0.000, loss_scale=1.000, wall=26706, train_wall=24094\n",
      "| epoch 110 | loss 1.929 | nll_loss 0.373 | ppl 1.29 | wps 64165 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 95181 | lr 0.000146686 | gnorm 0.202 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 26760 | train_wall 24137\n",
      "| epoch 110 | valid on 'valid' subset | loss 2.158 | nll_loss 0.335 | ppl 1.26 | num_updates 95181 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint110.pt (epoch 110 @ 95181 updates) (writing took 3.9626362323760986 seconds)\n",
      "| epoch 111:   1000 / 1205 loss=1.912, nll_loss=0.353, ppl=1.28, wps=60016, ups=3, wpb=15471.042, bsz=736.716, num_updates=96182, lr=0.000145921, gnorm=0.190, clip=0.000, oom=0.000, loss_scale=1.000, wall=27071, train_wall=24346\n",
      "| epoch 111 | loss 1.928 | nll_loss 0.372 | ppl 1.29 | wps 59727 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 96386 | lr 0.000145767 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 27125 | train_wall 24390\n",
      "| epoch 111 | valid on 'valid' subset | loss 2.150 | nll_loss 0.330 | ppl 1.26 | num_updates 96386 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint111.pt (epoch 111 @ 96386 updates) (writing took 3.826537847518921 seconds)\n",
      "| epoch 112:   1000 / 1205 loss=1.921, nll_loss=0.364, ppl=1.29, wps=59102, ups=3, wpb=15472.385, bsz=736.780, num_updates=97387, lr=0.000145016, gnorm=0.197, clip=0.000, oom=0.000, loss_scale=1.000, wall=27437, train_wall=24602\n",
      "| epoch 112 | loss 1.927 | nll_loss 0.371 | ppl 1.29 | wps 59089 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 97591 | lr 0.000144864 | gnorm 0.202 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 27490 | train_wall 24645\n",
      "| epoch 112 | valid on 'valid' subset | loss 2.152 | nll_loss 0.332 | ppl 1.26 | num_updates 97591 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint112.pt (epoch 112 @ 97591 updates) (writing took 3.855522871017456 seconds)\n",
      "| epoch 113:   1000 / 1205 loss=1.921, nll_loss=0.364, ppl=1.29, wps=58209, ups=3, wpb=15473.056, bsz=736.812, num_updates=98592, lr=0.000144127, gnorm=0.209, clip=0.000, oom=0.000, loss_scale=1.000, wall=27805, train_wall=24859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 113 | loss 1.926 | nll_loss 0.370 | ppl 1.29 | wps 58428 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 98796 | lr 0.000143978 | gnorm 0.207 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 27858 | train_wall 24902\n",
      "| epoch 113 | valid on 'valid' subset | loss 2.153 | nll_loss 0.334 | ppl 1.26 | num_updates 98796 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint113.pt (epoch 113 @ 98796 updates) (writing took 3.6978278160095215 seconds)\n",
      "| epoch 114:   1000 / 1205 loss=1.927, nll_loss=0.371, ppl=1.29, wps=64481, ups=4, wpb=15473.727, bsz=736.844, num_updates=99797, lr=0.000143254, gnorm=0.207, clip=0.000, oom=0.000, loss_scale=1.000, wall=28141, train_wall=25099\n",
      "| epoch 114 | loss 1.926 | nll_loss 0.370 | ppl 1.29 | wps 64458 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 100001 | lr 0.000143108 | gnorm 0.209 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 28190 | train_wall 25139\n",
      "| epoch 114 | valid on 'valid' subset | loss 2.157 | nll_loss 0.334 | ppl 1.26 | num_updates 100001 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint114.pt (epoch 114 @ 100001 updates) (writing took 3.8847317695617676 seconds)\n",
      "| epoch 115:   1000 / 1205 loss=1.920, nll_loss=0.363, ppl=1.29, wps=59407, ups=3, wpb=15472.720, bsz=736.796, num_updates=101002, lr=0.000142397, gnorm=0.201, clip=0.000, oom=0.000, loss_scale=2.000, wall=28500, train_wall=25352\n",
      "| epoch 115 | loss 1.925 | nll_loss 0.369 | ppl 1.29 | wps 59768 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 101206 | lr 0.000142253 | gnorm 0.199 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 28551 | train_wall 25394\n",
      "| epoch 115 | valid on 'valid' subset | loss 2.156 | nll_loss 0.336 | ppl 1.26 | num_updates 101206 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint115.pt (epoch 115 @ 101206 updates) (writing took 3.9908392429351807 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 1.0\n",
      "| epoch 116:   1000 / 1205 loss=1.932, nll_loss=0.376, ppl=1.30, wps=58715, ups=3, wpb=15481.872, bsz=737.232, num_updates=102206, lr=0.000141556, gnorm=0.198, clip=0.000, oom=0.000, loss_scale=1.000, wall=28865, train_wall=25606\n",
      "| epoch 116 | loss 1.924 | nll_loss 0.368 | ppl 1.29 | wps 58737 | ups 3 | wpb 15468.227 | bsz 736.582 | num_updates 102410 | lr 0.000141414 | gnorm 0.198 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 28918 | train_wall 25650\n",
      "| epoch 116 | valid on 'valid' subset | loss 2.151 | nll_loss 0.332 | ppl 1.26 | num_updates 102410 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint116.pt (epoch 116 @ 102410 updates) (writing took 3.9331486225128174 seconds)\n",
      "| epoch 117:   1000 / 1205 loss=1.909, nll_loss=0.350, ppl=1.27, wps=59262, ups=3, wpb=15471.042, bsz=736.716, num_updates=103411, lr=0.000140728, gnorm=0.197, clip=0.000, oom=0.000, loss_scale=1.000, wall=29226, train_wall=25862\n",
      "| epoch 117 | loss 1.925 | nll_loss 0.368 | ppl 1.29 | wps 59208 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 103615 | lr 0.00014059 | gnorm 0.198 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 29280 | train_wall 25905\n",
      "| epoch 117 | valid on 'valid' subset | loss 2.153 | nll_loss 0.333 | ppl 1.26 | num_updates 103615 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint117.pt (epoch 117 @ 103615 updates) (writing took 3.7212884426116943 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 0.5\n",
      "| epoch 118:   1000 / 1205 loss=1.923, nll_loss=0.366, ppl=1.29, wps=64836, ups=4, wpb=15472.737, bsz=736.797, num_updates=104615, lr=0.000139916, gnorm=0.204, clip=0.000, oom=0.000, loss_scale=0.500, wall=29565, train_wall=26101\n",
      "| epoch 118 | loss 1.924 | nll_loss 0.368 | ppl 1.29 | wps 64653 | ups 4 | wpb 15468.227 | bsz 736.582 | num_updates 104819 | lr 0.00013978 | gnorm 0.203 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 29614 | train_wall 26141\n",
      "| epoch 118 | valid on 'valid' subset | loss 2.151 | nll_loss 0.333 | ppl 1.26 | num_updates 104819 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint118.pt (epoch 118 @ 104819 updates) (writing took 3.8249287605285645 seconds)\n",
      "| epoch 119:   1000 / 1205 loss=1.918, nll_loss=0.360, ppl=1.28, wps=60010, ups=3, wpb=15472.049, bsz=736.764, num_updates=105820, lr=0.000139117, gnorm=0.194, clip=0.000, oom=0.000, loss_scale=0.500, wall=29920, train_wall=26352\n",
      "| epoch 119 | loss 1.922 | nll_loss 0.365 | ppl 1.29 | wps 59933 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 106024 | lr 0.000138983 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 29973 | train_wall 26395\n",
      "| epoch 119 | valid on 'valid' subset | loss 2.156 | nll_loss 0.334 | ppl 1.26 | num_updates 106024 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint119.pt (epoch 119 @ 106024 updates) (writing took 3.8336405754089355 seconds)\n",
      "| epoch 120:   1000 / 1205 loss=1.926, nll_loss=0.369, ppl=1.29, wps=58652, ups=3, wpb=15473.727, bsz=736.844, num_updates=107025, lr=0.000138332, gnorm=0.201, clip=0.000, oom=0.000, loss_scale=0.500, wall=30285, train_wall=26608\n",
      "| epoch 120 | loss 1.923 | nll_loss 0.366 | ppl 1.29 | wps 59076 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 107229 | lr 0.0001382 | gnorm 0.198 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 30337 | train_wall 26649\n",
      "| epoch 120 | valid on 'valid' subset | loss 2.156 | nll_loss 0.333 | ppl 1.26 | num_updates 107229 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint120.pt (epoch 120 @ 107229 updates) (writing took 3.225221872329712 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 0.25\n",
      "| epoch 121:   1000 / 1205 loss=1.928, nll_loss=0.372, ppl=1.29, wps=58575, ups=3, wpb=15475.089, bsz=736.909, num_updates=108229, lr=0.00013756, gnorm=0.196, clip=0.000, oom=0.000, loss_scale=0.250, wall=30651, train_wall=26863\n",
      "| epoch 121 | loss 1.921 | nll_loss 0.365 | ppl 1.29 | wps 58535 | ups 3 | wpb 15467.948 | bsz 736.569 | num_updates 108433 | lr 0.000137431 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 30705 | train_wall 26906\n",
      "| epoch 121 | valid on 'valid' subset | loss 2.158 | nll_loss 0.334 | ppl 1.26 | num_updates 108433 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint121.pt (epoch 121 @ 108433 updates) (writing took 3.379361391067505 seconds)\n",
      "| epoch 122:   1000 / 1205 loss=1.919, nll_loss=0.362, ppl=1.29, wps=65236, ups=3, wpb=15479.832, bsz=737.135, num_updates=109434, lr=0.000136801, gnorm=0.191, clip=0.000, oom=0.000, loss_scale=0.250, wall=30991, train_wall=27104\n",
      "| epoch 122 | loss 1.921 | nll_loss 0.365 | ppl 1.29 | wps 65118 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 109638 | lr 0.000136674 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 31040 | train_wall 27144\n",
      "| epoch 122 | valid on 'valid' subset | loss 2.158 | nll_loss 0.335 | ppl 1.26 | num_updates 109638 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint122.pt (epoch 122 @ 109638 updates) (writing took 3.2857253551483154 seconds)\n",
      "| epoch 123:   1000 / 1205 loss=1.919, nll_loss=0.361, ppl=1.28, wps=59629, ups=3, wpb=15474.063, bsz=736.860, num_updates=110639, lr=0.000136054, gnorm=0.202, clip=0.000, oom=0.000, loss_scale=0.250, wall=31342, train_wall=27352\n",
      "| epoch 123 | loss 1.920 | nll_loss 0.363 | ppl 1.29 | wps 59378 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 110843 | lr 0.000135929 | gnorm 0.198 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 31396 | train_wall 27396\n",
      "| epoch 123 | valid on 'valid' subset | loss 2.158 | nll_loss 0.335 | ppl 1.26 | num_updates 110843 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint123.pt (epoch 123 @ 110843 updates) (writing took 3.3538966178894043 seconds)\n",
      "| epoch 124:   1000 / 1205 loss=1.918, nll_loss=0.361, ppl=1.28, wps=58754, ups=3, wpb=15473.056, bsz=736.812, num_updates=111844, lr=0.000135319, gnorm=0.202, clip=0.000, oom=0.000, loss_scale=0.250, wall=31709, train_wall=27608\n",
      "| epoch 124 | loss 1.921 | nll_loss 0.365 | ppl 1.29 | wps 58753 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 112048 | lr 0.000135196 | gnorm 0.199 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 31763 | train_wall 27651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 124 | valid on 'valid' subset | loss 2.154 | nll_loss 0.334 | ppl 1.26 | num_updates 112048 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint124.pt (epoch 124 @ 112048 updates) (writing took 3.39235520362854 seconds)\n",
      "| epoch 125:   1000 / 1205 loss=1.918, nll_loss=0.361, ppl=1.28, wps=60041, ups=3, wpb=15473.727, bsz=736.844, num_updates=113049, lr=0.000134596, gnorm=0.187, clip=0.000, oom=0.000, loss_scale=0.250, wall=32069, train_wall=27862\n",
      "| epoch 125 | loss 1.920 | nll_loss 0.363 | ppl 1.29 | wps 59978 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 113253 | lr 0.000134475 | gnorm 0.191 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 32122 | train_wall 27905\n",
      "| epoch 125 | valid on 'valid' subset | loss 2.156 | nll_loss 0.332 | ppl 1.26 | num_updates 113253 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint125.pt (epoch 125 @ 113253 updates) (writing took 3.2065305709838867 seconds)\n",
      "| epoch 126:   1000 / 1205 loss=1.921, nll_loss=0.364, ppl=1.29, wps=64335, ups=3, wpb=15474.063, bsz=736.860, num_updates=114254, lr=0.000133884, gnorm=0.206, clip=0.000, oom=0.000, loss_scale=0.250, wall=32410, train_wall=28103\n",
      "| epoch 126 | loss 1.920 | nll_loss 0.364 | ppl 1.29 | wps 64583 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 114458 | lr 0.000133765 | gnorm 0.206 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 32457 | train_wall 28142\n",
      "| epoch 126 | valid on 'valid' subset | loss 2.158 | nll_loss 0.338 | ppl 1.26 | num_updates 114458 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint126.pt (epoch 126 @ 114458 updates) (writing took 3.119145631790161 seconds)\n",
      "| epoch 127:   1000 / 1205 loss=1.919, nll_loss=0.362, ppl=1.28, wps=60973, ups=3, wpb=15473.392, bsz=736.828, num_updates=115459, lr=0.000133184, gnorm=0.201, clip=0.000, oom=0.000, loss_scale=0.250, wall=32759, train_wall=28348\n",
      "| epoch 127 | loss 1.920 | nll_loss 0.363 | ppl 1.29 | wps 60694 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 115663 | lr 0.000133066 | gnorm 0.197 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 32812 | train_wall 28392\n",
      "| epoch 127 | valid on 'valid' subset | loss 2.155 | nll_loss 0.334 | ppl 1.26 | num_updates 115663 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint127.pt (epoch 127 @ 115663 updates) (writing took 3.184220314025879 seconds)\n",
      "| epoch 128:   1000 / 1205 loss=1.922, nll_loss=0.365, ppl=1.29, wps=59675, ups=3, wpb=15474.063, bsz=736.860, num_updates=116664, lr=0.000132494, gnorm=0.201, clip=0.000, oom=0.000, loss_scale=0.500, wall=33119, train_wall=28601\n",
      "| epoch 128 | loss 1.918 | nll_loss 0.361 | ppl 1.28 | wps 59791 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 116868 | lr 0.000132378 | gnorm 0.197 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 33171 | train_wall 28644\n",
      "| epoch 128 | valid on 'valid' subset | loss 2.160 | nll_loss 0.337 | ppl 1.26 | num_updates 116868 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint128.pt (epoch 128 @ 116868 updates) (writing took 3.2939374446868896 seconds)\n",
      "| epoch 129:   1000 / 1205 loss=1.925, nll_loss=0.368, ppl=1.29, wps=59793, ups=3, wpb=15475.406, bsz=736.924, num_updates=117869, lr=0.000131815, gnorm=0.203, clip=0.000, oom=0.000, loss_scale=0.500, wall=33481, train_wall=28854\n",
      "| epoch 129 | loss 1.919 | nll_loss 0.362 | ppl 1.29 | wps 59295 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 118073 | lr 0.000131701 | gnorm 0.202 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 33536 | train_wall 28899\n",
      "| epoch 129 | valid on 'valid' subset | loss 2.158 | nll_loss 0.335 | ppl 1.26 | num_updates 118073 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint129.pt (epoch 129 @ 118073 updates) (writing took 4.2963707447052 seconds)\n",
      "| epoch 130:   1000 / 1205 loss=1.914, nll_loss=0.356, ppl=1.28, wps=60530, ups=3, wpb=15479.832, bsz=737.135, num_updates=119074, lr=0.000131146, gnorm=0.212, clip=0.000, oom=0.000, loss_scale=0.500, wall=33847, train_wall=29104\n",
      "| epoch 130 | loss 1.919 | nll_loss 0.362 | ppl 1.28 | wps 61285 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 119278 | lr 0.000131034 | gnorm 0.209 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 33895 | train_wall 29144\n",
      "| epoch 130 | valid on 'valid' subset | loss 2.159 | nll_loss 0.338 | ppl 1.26 | num_updates 119278 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint130.pt (epoch 130 @ 119278 updates) (writing took 3.211230993270874 seconds)\n",
      "| epoch 131:   1000 / 1205 loss=1.917, nll_loss=0.360, ppl=1.28, wps=61557, ups=3, wpb=15473.727, bsz=736.844, num_updates=120279, lr=0.000130488, gnorm=0.212, clip=0.000, oom=0.000, loss_scale=0.500, wall=34192, train_wall=29349\n",
      "| epoch 131 | loss 1.917 | nll_loss 0.359 | ppl 1.28 | wps 61214 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 120483 | lr 0.000130377 | gnorm 0.207 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 34245 | train_wall 29393\n",
      "| epoch 131 | valid on 'valid' subset | loss 2.159 | nll_loss 0.335 | ppl 1.26 | num_updates 120483 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint131.pt (epoch 131 @ 120483 updates) (writing took 3.2205915451049805 seconds)\n",
      "| epoch 132:   1000 / 1205 loss=1.911, nll_loss=0.353, ppl=1.28, wps=59097, ups=3, wpb=15472.385, bsz=736.780, num_updates=121484, lr=0.000129839, gnorm=0.192, clip=0.000, oom=0.000, loss_scale=0.500, wall=34552, train_wall=29605\n",
      "| epoch 132 | loss 1.917 | nll_loss 0.360 | ppl 1.28 | wps 58833 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 121688 | lr 0.00012973 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 34607 | train_wall 29649\n",
      "| epoch 132 | valid on 'valid' subset | loss 2.157 | nll_loss 0.336 | ppl 1.26 | num_updates 121688 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint132.pt (epoch 132 @ 121688 updates) (writing took 3.4066665172576904 seconds)\n",
      "| epoch 133:   1000 / 1205 loss=1.926, nll_loss=0.370, ppl=1.29, wps=58821, ups=3, wpb=15475.406, bsz=736.924, num_updates=122689, lr=0.0001292, gnorm=0.200, clip=0.000, oom=0.000, loss_scale=0.500, wall=34918, train_wall=29863\n",
      "| epoch 133 | loss 1.916 | nll_loss 0.359 | ppl 1.28 | wps 59107 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 122893 | lr 0.000129093 | gnorm 0.198 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 34970 | train_wall 29905\n",
      "| epoch 133 | valid on 'valid' subset | loss 2.158 | nll_loss 0.336 | ppl 1.26 | num_updates 122893 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint133.pt (epoch 133 @ 122893 updates) (writing took 3.2527880668640137 seconds)\n",
      "| epoch 134:   1000 / 1205 loss=1.919, nll_loss=0.362, ppl=1.29, wps=61168, ups=3, wpb=15474.063, bsz=736.860, num_updates=123894, lr=0.00012857, gnorm=0.199, clip=0.000, oom=0.000, loss_scale=0.500, wall=35275, train_wall=30113\n",
      "| epoch 134 | loss 1.917 | nll_loss 0.360 | ppl 1.28 | wps 61790 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 124098 | lr 0.000128464 | gnorm 0.201 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 35324 | train_wall 30153\n",
      "| epoch 134 | valid on 'valid' subset | loss 2.157 | nll_loss 0.336 | ppl 1.26 | num_updates 124098 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint134.pt (epoch 134 @ 124098 updates) (writing took 3.4198427200317383 seconds)\n",
      "| epoch 135:   1000 / 1205 loss=1.917, nll_loss=0.359, ppl=1.28, wps=62480, ups=3, wpb=15473.392, bsz=736.828, num_updates=125099, lr=0.000127949, gnorm=0.200, clip=0.000, oom=0.000, loss_scale=1.000, wall=35616, train_wall=30356\n",
      "| epoch 135 | loss 1.917 | nll_loss 0.360 | ppl 1.28 | wps 61834 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 125303 | lr 0.000127845 | gnorm 0.202 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 35669 | train_wall 30399\n",
      "| epoch 135 | valid on 'valid' subset | loss 2.163 | nll_loss 0.340 | ppl 1.27 | num_updates 125303 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.1/checkpoint135.pt (epoch 135 @ 125303 updates) (writing took 3.3063671588897705 seconds)\n",
      "| epoch 136:   1000 / 1205 loss=1.915, nll_loss=0.357, ppl=1.28, wps=59469, ups=3, wpb=15473.727, bsz=736.844, num_updates=126304, lr=0.000127338, gnorm=0.194, clip=0.000, oom=0.000, loss_scale=1.000, wall=35978, train_wall=30611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 136 | loss 1.915 | nll_loss 0.358 | ppl 1.28 | wps 59427 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 126508 | lr 0.000127235 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 36031 | train_wall 30654\n",
      "| epoch 136 | valid on 'valid' subset | loss 2.163 | nll_loss 0.338 | ppl 1.26 | num_updates 126508 | best_loss 2.04018\n"
     ]
    }
   ],
   "source": [
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training && \\\n",
    "    chmod +x train_bert_nmt_no_bpe.sh && ./train_bert_nmt_no_bpe.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ set -e\n",
      "+ source ../paths.sh\n",
      "++++ dirname ../paths.sh\n",
      "+++ cd ..\n",
      "+++ pwd\n",
      "++ BASE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb\n",
      "++ DATA_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/data\n",
      "++ MODEL_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/models\n",
      "++ SCRIPTS_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/scripts\n",
      "++ SOFTWARE_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software\n",
      "++ TRAINING_DIR=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training\n",
      "+ FAIRSEQPY=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt\n",
      "+ SEED=1\n",
      "+ DATA_BIN_DIR=processed/bin_bert_nmt\n",
      "+ src=src_bert_nmt\n",
      "+ tgt=trg_bert_nmt\n",
      "+ bedropout=0.25\n",
      "+ ARCH=transformer_s2_iwslt_de_en\n",
      "+ DATAPATH=processed/bin_bert_nmt\n",
      "+ SAVEDIR=checkpoints/bert_nmt_0.25\n",
      "+ mkdir -p checkpoints/bert_nmt_0.25\n",
      "+ '[' '!' -f checkpoints/bert_nmt_0.25/checkpoint_nmt.pt ']'\n",
      "+ cp -v checkpoints/pretrained_nmt/checkpoint_best.pt checkpoints/bert_nmt_0.25/checkpoint_nmt.pt\n",
      "'checkpoints/pretrained_nmt/checkpoint_best.pt' -> 'checkpoints/bert_nmt_0.25/checkpoint_nmt.pt'\n",
      "+ '[' '!' -f checkpoints/bert_nmt_0.25/checkpoint_last.pt ']'\n",
      "+ warmup='--warmup-from-nmt --reset-lr-scheduler'\n",
      "+ env PYTHONPATH=/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt: CUDA_VISIBLE_DEVICES=1,2 python /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt/train.py processed/bin_bert_nmt -a transformer_s2_iwslt_de_en --optimizer adam --lr 0.0005 -s src_bert_nmt -t trg_bert_nmt --label-smoothing 0.1 --dropout 0.5 --max-tokens 8192 --min-lr 1e-09 --lr-scheduler inverse_sqrt --weight-decay 0.0001 --criterion label_smoothed_cross_entropy --max-update 150000 --warmup-updates 8192 --warmup-init-lr 1e-07 --batch-size 512 --no-progress-bar --restore-file checkpoint_best.pt --bert-model-name voidful/albert_chinese_tiny --adam-betas '(0.9,0.98)' --save-dir checkpoints/bert_nmt_0.25 --share-all-embeddings --warmup-from-nmt --reset-lr-scheduler --fp16 --encoder-bert-dropout --encoder-bert-dropout-ratio 0.25\n",
      "+ tee -a checkpoints/bert_nmt_0.25/training.log\n",
      "| distributed init (rank 0): tcp://localhost:12156\n",
      "| distributed init (rank 1): tcp://localhost:12156\n",
      "| initialized host 199 as rank 1\n",
      "| initialized host 199 as rank 0\n",
      "Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9,0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer_s2_iwslt_de_en', attention_dropout=0.0, bert_first=True, bert_gates=[1, 1, 1, 1, 1, 1], bert_model_name='voidful/albert_chinese_tiny', bert_output_layer=-1, bert_ratio=1.0, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='processed/bin_bert_nmt', dataset_impl='cached', ddp_backend='c10d', decoder_attention_heads=4, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=1024, decoder_input_dim=512, decoder_layers=6, decoder_learned_pos=False, decoder_no_bert=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method='tcp://localhost:12156', distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=2, dropout=0.5, encoder_attention_heads=4, encoder_bert_dropout=True, encoder_bert_dropout_ratio=0.25, encoder_bert_mixup=False, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=1024, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, encoder_ratio=1.0, find_unused_parameters=False, finetune_bert=False, fix_batches_to_gpus=False, fp16=True, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', mask_cls_sep=False, max_epoch=0, max_sentences=512, max_sentences_valid=512, max_source_positions=1024, max_target_positions=1024, max_tokens=8192, max_update=150000, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_epoch_checkpoints=False, no_progress_bar=True, no_save=False, no_token_positional_embeddings=False, num_workers=0, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_dataloader=False, reset_lr_scheduler=True, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_best.pt', save_dir='checkpoints/bert_nmt_0.25', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=True, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='src_bert_nmt', target_lang='trg_bert_nmt', task='translation', tbmf_wrapper=False, tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', update_freq=[1], upsample_primary=1, user_dir=None, valid_subset='valid', validate_interval=1, warmup_from_nmt=True, warmup_init_lr=1e-07, warmup_nmt_file='checkpoint_nmt.pt', warmup_updates=8192, weight_decay=0.0001)\n",
      "| [src_bert_nmt] dictionary: 5544 types\n",
      "| [trg_bert_nmt] dictionary: 5544 types\n",
      "| processed/bin_bert_nmt valid src_bert_nmt-trg_bert_nmt 221931 examples\n",
      "bert_gates [True, True, True, True, True, True]\n",
      "TransformerS2Model(\n",
      "  (encoder): TransformerS2Encoder(\n",
      "    (embed_tokens): Embedding(5544, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerS2EncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embed_tokens): Embedding(5544, 512, padding_idx=1)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "    (layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=512, out_features=1024, bias=True)\n",
      "        (fc2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (bert_encoder): AlbertForMaskedLM(\n",
      "    (albert): AlbertModel(\n",
      "      (embeddings): AlbertEmbeddings(\n",
      "        (word_embeddings): Embedding(21128, 128, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 128)\n",
      "        (token_type_embeddings): Embedding(2, 128)\n",
      "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (encoder): AlbertTransformer(\n",
      "        (embedding_hidden_mapping_in): Linear(in_features=128, out_features=312, bias=True)\n",
      "        (albert_layer_groups): ModuleList(\n",
      "          (0): AlbertLayerGroup(\n",
      "            (albert_layers): ModuleList(\n",
      "              (0): AlbertLayer(\n",
      "                (full_layer_layer_norm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "                (attention): AlbertAttention(\n",
      "                  (query): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (key): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (value): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "                )\n",
      "                (ffn): Linear(in_features=312, out_features=1248, bias=True)\n",
      "                (ffn_output): Linear(in_features=1248, out_features=312, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): Linear(in_features=312, out_features=312, bias=True)\n",
      "      (pooler_activation): Tanh()\n",
      "    )\n",
      "    (predictions): AlbertMLMHead(\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dense): Linear(in_features=312, out_features=128, bias=True)\n",
      "      (decoder): Linear(in_features=128, out_features=21128, bias=True)\n",
      "    )\n",
      "  )\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ")\n",
      "| model transformer_s2_iwslt_de_en, criterion LabelSmoothedCrossEntropyCriterion\n",
      "| num. model params: 48673680 (num. trained: 44531712)\n",
      "| training on 2 GPUs\n",
      "| max tokens per GPU = 8192 and max sentences per GPU = 512\n",
      "Model will load checkpoint from checkpoints/bert_nmt_0.25/checkpoint_nmt.pt\n",
      "| loaded checkpoint checkpoints/bert_nmt_0.25/checkpoint_nmt.pt (epoch 31 @ 0 updates)\n",
      "| loading train data for epoch 31\n",
      "| processed/bin_bert_nmt train src_bert_nmt-trg_bert_nmt 887581 examples\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint31.pt (epoch 31 @ 0 updates) (writing took 0.1825413703918457 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 64.0\n",
      "| WARNING: overflow detected, setting loss scale to: 32.0\n",
      "| WARNING: overflow detected, setting loss scale to: 16.0\n",
      "| epoch 032:   1000 / 1205 loss=7.878, nll_loss=7.110, ppl=138.14, wps=56741, ups=4, wpb=15477.884, bsz=737.042, num_updates=998, lr=6.10009e-05, gnorm=0.786, clip=0.000, oom=0.000, loss_scale=16.000, wall=272, train_wall=5362\n",
      "| epoch 032 | loss 7.533 | nll_loss 6.711 | ppl 104.73 | wps 54574 | ups 4 | wpb 15467.968 | bsz 736.570 | num_updates 1202 | lr 7.34496e-05 | gnorm 0.760 | clip 0.000 | oom 0.000 | loss_scale 16.000 | wall 341 | train_wall 5419\n",
      "| epoch 032 | valid on 'valid' subset | loss 4.676 | nll_loss 3.110 | ppl 8.63 | num_updates 1202 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint32.pt (epoch 32 @ 1202 updates) (writing took 2.6564888954162598 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 8.0\n",
      "| epoch 033:   1000 / 1205 loss=3.221, nll_loss=1.734, ppl=3.33, wps=53751, ups=3, wpb=15472.401, bsz=736.781, num_updates=2202, lr=0.000134473, gnorm=0.722, clip=0.000, oom=0.000, loss_scale=8.000, wall=697, train_wall=5657\n",
      "| epoch 033 | loss 3.098 | nll_loss 1.598 | ppl 3.03 | wps 54609 | ups 3 | wpb 15467.948 | bsz 736.569 | num_updates 2406 | lr 0.000146921 | gnorm 0.689 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 750 | train_wall 5700\n",
      "| epoch 033 | valid on 'valid' subset | loss 2.640 | nll_loss 0.589 | ppl 1.50 | num_updates 2406 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint33.pt (epoch 33 @ 2406 updates) (writing took 2.2414114475250244 seconds)\n",
      "| epoch 034:   1000 / 1205 loss=2.333, nll_loss=0.769, ppl=1.70, wps=59433, ups=3, wpb=15474.063, bsz=736.860, num_updates=3407, lr=0.000208005, gnorm=0.443, clip=0.000, oom=0.000, loss_scale=8.000, wall=1060, train_wall=5912\n",
      "| epoch 034 | loss 2.313 | nll_loss 0.747 | ppl 1.68 | wps 59308 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 3611 | lr 0.000220454 | gnorm 0.436 | clip 0.000 | oom 0.000 | loss_scale 8.000 | wall 1114 | train_wall 5956\n",
      "| epoch 034 | valid on 'valid' subset | loss 2.432 | nll_loss 0.502 | ppl 1.42 | num_updates 3611 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint34.pt (epoch 34 @ 3611 updates) (writing took 2.148087501525879 seconds)\n",
      "| epoch 035:   1000 / 1205 loss=2.204, nll_loss=0.642, ppl=1.56, wps=58921, ups=3, wpb=15471.042, bsz=736.716, num_updates=4612, lr=0.000281538, gnorm=0.358, clip=0.000, oom=0.000, loss_scale=8.000, wall=1423, train_wall=6169\n",
      "| WARNING: overflow detected, setting loss scale to: 4.0\n",
      "| epoch 035 | loss 2.215 | nll_loss 0.655 | ppl 1.57 | wps 58837 | ups 3 | wpb 15467.948 | bsz 736.569 | num_updates 4815 | lr 0.000293926 | gnorm 0.352 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 1477 | train_wall 6212\n",
      "| epoch 035 | valid on 'valid' subset | loss 2.357 | nll_loss 0.457 | ppl 1.37 | num_updates 4815 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint35.pt (epoch 35 @ 4815 updates) (writing took 2.0876290798187256 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 2.0\n",
      "| epoch 036:   1000 / 1205 loss=2.169, nll_loss=0.612, ppl=1.53, wps=66085, ups=4, wpb=15473.409, bsz=736.829, num_updates=5815, lr=0.000354948, gnorm=0.334, clip=0.000, oom=0.000, loss_scale=2.000, wall=1755, train_wall=6405\n",
      "| epoch 036 | loss 2.171 | nll_loss 0.614 | ppl 1.53 | wps 65922 | ups 4 | wpb 15468.227 | bsz 736.582 | num_updates 6019 | lr 0.000367397 | gnorm 0.337 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 1803 | train_wall 6444\n",
      "| epoch 036 | valid on 'valid' subset | loss 2.310 | nll_loss 0.433 | ppl 1.35 | num_updates 6019 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint36.pt (epoch 36 @ 6019 updates) (writing took 2.3347854614257812 seconds)\n",
      "| epoch 037:   1000 / 1205 loss=2.159, nll_loss=0.604, ppl=1.52, wps=59157, ups=3, wpb=15480.168, bsz=737.151, num_updates=7020, lr=0.000428481, gnorm=0.351, clip=0.000, oom=0.000, loss_scale=2.000, wall=2112, train_wall=6658\n",
      "| epoch 037 | loss 2.161 | nll_loss 0.607 | ppl 1.52 | wps 59042 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 7224 | lr 0.00044093 | gnorm 0.351 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 2166 | train_wall 6702\n",
      "| epoch 037 | valid on 'valid' subset | loss 2.287 | nll_loss 0.430 | ppl 1.35 | num_updates 7224 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint37.pt (epoch 37 @ 7224 updates) (writing took 2.2174911499023438 seconds)\n",
      "| epoch 038:   1000 / 1205 loss=2.154, nll_loss=0.601, ppl=1.52, wps=57283, ups=3, wpb=15480.839, bsz=737.183, num_updates=8225, lr=0.000498996, gnorm=0.354, clip=0.000, oom=0.000, loss_scale=2.000, wall=2488, train_wall=6925\n",
      "| epoch 038 | loss 2.148 | nll_loss 0.594 | ppl 1.51 | wps 57280 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 8429 | lr 0.000492921 | gnorm 0.356 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 2542 | train_wall 6970\n",
      "| epoch 038 | valid on 'valid' subset | loss 2.265 | nll_loss 0.421 | ppl 1.34 | num_updates 8429 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint38.pt (epoch 38 @ 8429 updates) (writing took 2.3375051021575928 seconds)\n",
      "| epoch 039:   1000 / 1205 loss=2.141, nll_loss=0.589, ppl=1.50, wps=59162, ups=3, wpb=15474.063, bsz=736.860, num_updates=9430, lr=0.000466025, gnorm=0.341, clip=0.000, oom=0.000, loss_scale=2.000, wall=2852, train_wall=7185\n",
      "| epoch 039 | loss 2.131 | nll_loss 0.578 | ppl 1.49 | wps 58933 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 9634 | lr 0.000461064 | gnorm 0.350 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 2907 | train_wall 7229\n",
      "| epoch 039 | valid on 'valid' subset | loss 2.268 | nll_loss 0.407 | ppl 1.33 | num_updates 9634 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint39.pt (epoch 39 @ 9634 updates) (writing took 2.1624207496643066 seconds)\n",
      "| epoch 040:   1000 / 1205 loss=2.116, nll_loss=0.564, ppl=1.48, wps=64084, ups=3, wpb=15472.720, bsz=736.796, num_updates=10635, lr=0.00043883, gnorm=0.341, clip=0.000, oom=0.000, loss_scale=2.000, wall=3199, train_wall=7425\n",
      "| epoch 040 | loss 2.118 | nll_loss 0.567 | ppl 1.48 | wps 64534 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 10839 | lr 0.000434681 | gnorm 0.347 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 3246 | train_wall 7464\n",
      "| epoch 040 | valid on 'valid' subset | loss 2.243 | nll_loss 0.406 | ppl 1.33 | num_updates 10839 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint40.pt (epoch 40 @ 10839 updates) (writing took 2.1589086055755615 seconds)\n",
      "| epoch 041:   1000 / 1205 loss=2.114, nll_loss=0.565, ppl=1.48, wps=59271, ups=3, wpb=15474.734, bsz=736.892, num_updates=11840, lr=0.0004159, gnorm=0.315, clip=0.000, oom=0.000, loss_scale=2.000, wall=3549, train_wall=7678\n",
      "| epoch 041 | loss 2.103 | nll_loss 0.552 | ppl 1.47 | wps 59209 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 12044 | lr 0.000412363 | gnorm 0.309 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 3602 | train_wall 7722\n",
      "| epoch 041 | valid on 'valid' subset | loss 2.234 | nll_loss 0.400 | ppl 1.32 | num_updates 12044 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint41.pt (epoch 41 @ 12044 updates) (writing took 2.2751946449279785 seconds)\n",
      "| epoch 042:   1000 / 1205 loss=2.078, nll_loss=0.525, ppl=1.44, wps=58217, ups=3, wpb=15472.385, bsz=736.780, num_updates=13045, lr=0.000396226, gnorm=0.284, clip=0.000, oom=0.000, loss_scale=2.000, wall=3917, train_wall=7941\n",
      "| epoch 042 | loss 2.086 | nll_loss 0.535 | ppl 1.45 | wps 55882 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 13249 | lr 0.000393164 | gnorm 0.290 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 3984 | train_wall 7998\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 042 | valid on 'valid' subset | loss 2.230 | nll_loss 0.397 | ppl 1.32 | num_updates 13249 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint42.pt (epoch 42 @ 13249 updates) (writing took 2.337263584136963 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 2.0\n",
      "| WARNING: overflow detected, setting loss scale to: 1.0\n",
      "| epoch 043:   1000 / 1205 loss=2.085, nll_loss=0.535, ppl=1.45, wps=53475, ups=3, wpb=15473.763, bsz=736.846, num_updates=14248, lr=0.00037913, gnorm=0.283, clip=0.000, oom=0.000, loss_scale=1.000, wall=4332, train_wall=8239\n",
      "| epoch 043 | loss 2.076 | nll_loss 0.525 | ppl 1.44 | wps 54415 | ups 3 | wpb 15467.678 | bsz 736.556 | num_updates 14452 | lr 0.000376445 | gnorm 0.288 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 4385 | train_wall 8282\n",
      "| epoch 043 | valid on 'valid' subset | loss 2.200 | nll_loss 0.387 | ppl 1.31 | num_updates 14452 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint43.pt (epoch 43 @ 14452 updates) (writing took 2.371764898300171 seconds)\n",
      "| epoch 044:   1000 / 1205 loss=2.079, nll_loss=0.530, ppl=1.44, wps=55384, ups=3, wpb=15475.070, bsz=736.908, num_updates=15453, lr=0.000364048, gnorm=0.274, clip=0.000, oom=0.000, loss_scale=1.000, wall=4715, train_wall=8515\n",
      "| epoch 044 | loss 2.068 | nll_loss 0.517 | ppl 1.43 | wps 54347 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 15657 | lr 0.000361669 | gnorm 0.273 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 4778 | train_wall 8568\n",
      "| epoch 044 | valid on 'valid' subset | loss 2.208 | nll_loss 0.377 | ppl 1.30 | num_updates 15657 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint44.pt (epoch 44 @ 15657 updates) (writing took 2.2711362838745117 seconds)\n",
      "| epoch 045:   1000 / 1205 loss=2.055, nll_loss=0.503, ppl=1.42, wps=58733, ups=3, wpb=15473.056, bsz=736.812, num_updates=16658, lr=0.000350634, gnorm=0.256, clip=0.000, oom=0.000, loss_scale=1.000, wall=5090, train_wall=8786\n",
      "| epoch 045 | loss 2.062 | nll_loss 0.511 | ppl 1.43 | wps 58667 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 16862 | lr 0.000348506 | gnorm 0.262 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 5144 | train_wall 8830\n",
      "| epoch 045 | valid on 'valid' subset | loss 2.214 | nll_loss 0.376 | ppl 1.30 | num_updates 16862 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint45.pt (epoch 45 @ 16862 updates) (writing took 2.2752013206481934 seconds)\n",
      "| epoch 046:   1000 / 1205 loss=2.058, nll_loss=0.508, ppl=1.42, wps=51374, ups=3, wpb=15473.727, bsz=736.844, num_updates=17863, lr=0.000338601, gnorm=0.259, clip=0.000, oom=0.000, loss_scale=1.000, wall=5492, train_wall=9079\n",
      "| epoch 046 | loss 2.054 | nll_loss 0.503 | ppl 1.42 | wps 50046 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 18067 | lr 0.000336684 | gnorm 0.263 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 5563 | train_wall 9139\n",
      "| epoch 046 | valid on 'valid' subset | loss 2.200 | nll_loss 0.367 | ppl 1.29 | num_updates 18067 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint46.pt (epoch 46 @ 18067 updates) (writing took 3.029942512512207 seconds)\n",
      "| epoch 047:   1000 / 1205 loss=2.050, nll_loss=0.500, ppl=1.41, wps=59314, ups=3, wpb=15473.392, bsz=736.828, num_updates=19068, lr=0.000327727, gnorm=0.255, clip=0.000, oom=0.000, loss_scale=1.000, wall=5888, train_wall=9354\n",
      "| epoch 047 | loss 2.050 | nll_loss 0.499 | ppl 1.41 | wps 59152 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 19272 | lr 0.000325988 | gnorm 0.255 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 5942 | train_wall 9398\n",
      "| epoch 047 | valid on 'valid' subset | loss 2.194 | nll_loss 0.366 | ppl 1.29 | num_updates 19272 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint47.pt (epoch 47 @ 19272 updates) (writing took 2.472825527191162 seconds)\n",
      "| epoch 048:   1000 / 1205 loss=2.052, nll_loss=0.502, ppl=1.42, wps=55971, ups=3, wpb=15475.070, bsz=736.908, num_updates=20273, lr=0.000317838, gnorm=0.249, clip=0.000, oom=0.000, loss_scale=1.000, wall=6278, train_wall=9629\n",
      "| epoch 048 | loss 2.045 | nll_loss 0.495 | ppl 1.41 | wps 57427 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 20477 | lr 0.000316251 | gnorm 0.250 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 6326 | train_wall 9668\n",
      "| epoch 048 | valid on 'valid' subset | loss 2.201 | nll_loss 0.369 | ppl 1.29 | num_updates 20477 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint48.pt (epoch 48 @ 20477 updates) (writing took 2.9457857608795166 seconds)\n",
      "| epoch 049:   1000 / 1205 loss=2.037, nll_loss=0.486, ppl=1.40, wps=43596, ups=2, wpb=15474.063, bsz=736.860, num_updates=21478, lr=0.000308793, gnorm=0.239, clip=0.000, oom=0.000, loss_scale=1.000, wall=6734, train_wall=9972\n",
      "| epoch 049 | loss 2.038 | nll_loss 0.488 | ppl 1.40 | wps 43730 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 21682 | lr 0.000307337 | gnorm 0.237 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 6804 | train_wall 10032\n",
      "| epoch 049 | valid on 'valid' subset | loss 2.200 | nll_loss 0.364 | ppl 1.29 | num_updates 21682 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint49.pt (epoch 49 @ 21682 updates) (writing took 2.280566692352295 seconds)\n",
      "| epoch 050:   1000 / 1205 loss=2.014, nll_loss=0.460, ppl=1.38, wps=42990, ups=2, wpb=15470.035, bsz=736.668, num_updates=22683, lr=0.000300479, gnorm=0.236, clip=0.000, oom=0.000, loss_scale=2.000, wall=7212, train_wall=10339\n",
      "| epoch 050 | loss 2.034 | nll_loss 0.484 | ppl 1.40 | wps 42784 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 22887 | lr 0.000299137 | gnorm 0.239 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 7288 | train_wall 10403\n",
      "| epoch 050 | valid on 'valid' subset | loss 2.194 | nll_loss 0.364 | ppl 1.29 | num_updates 22887 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint50.pt (epoch 50 @ 22887 updates) (writing took 3.3131511211395264 seconds)\n",
      "| epoch 051:   1000 / 1205 loss=2.030, nll_loss=0.479, ppl=1.39, wps=57449, ups=3, wpb=15479.832, bsz=737.135, num_updates=23888, lr=0.000292803, gnorm=0.229, clip=0.000, oom=0.000, loss_scale=2.000, wall=7622, train_wall=10624\n",
      "| epoch 051 | loss 2.029 | nll_loss 0.478 | ppl 1.39 | wps 57720 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 24092 | lr 0.00029156 | gnorm 0.229 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 7675 | train_wall 10667\n",
      "| epoch 051 | valid on 'valid' subset | loss 2.191 | nll_loss 0.353 | ppl 1.28 | num_updates 24092 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint51.pt (epoch 51 @ 24092 updates) (writing took 2.0718257427215576 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 1.0\n",
      "| epoch 052:   1000 / 1205 loss=2.023, nll_loss=0.471, ppl=1.39, wps=63919, ups=4, wpb=15472.401, bsz=736.781, num_updates=25092, lr=0.000285692, gnorm=0.244, clip=0.000, oom=0.000, loss_scale=1.000, wall=7959, train_wall=10865\n",
      "| epoch 052 | loss 2.028 | nll_loss 0.478 | ppl 1.39 | wps 64229 | ups 4 | wpb 15467.948 | bsz 736.569 | num_updates 25296 | lr 0.000284537 | gnorm 0.245 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 8007 | train_wall 10905\n",
      "| epoch 052 | valid on 'valid' subset | loss 2.191 | nll_loss 0.357 | ppl 1.28 | num_updates 25296 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint52.pt (epoch 52 @ 25296 updates) (writing took 2.183154582977295 seconds)\n",
      "| epoch 053:   1000 / 1205 loss=2.007, nll_loss=0.454, ppl=1.37, wps=59381, ups=3, wpb=15470.706, bsz=736.700, num_updates=26297, lr=0.000279069, gnorm=0.226, clip=0.000, oom=0.000, loss_scale=1.000, wall=8317, train_wall=11120\n",
      "| epoch 053 | loss 2.024 | nll_loss 0.473 | ppl 1.39 | wps 59584 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 26501 | lr 0.000277993 | gnorm 0.225 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 8369 | train_wall 11163\n",
      "| epoch 053 | valid on 'valid' subset | loss 2.184 | nll_loss 0.352 | ppl 1.28 | num_updates 26501 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint53.pt (epoch 53 @ 26501 updates) (writing took 2.1188457012176514 seconds)\n",
      "| epoch 054:   1000 / 1205 loss=2.018, nll_loss=0.466, ppl=1.38, wps=58968, ups=3, wpb=15473.056, bsz=736.812, num_updates=27502, lr=0.000272887, gnorm=0.221, clip=0.000, oom=0.000, loss_scale=1.000, wall=8677, train_wall=11379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| WARNING: overflow detected, setting loss scale to: 0.5\n",
      "| epoch 054 | loss 2.018 | nll_loss 0.467 | ppl 1.38 | wps 58800 | ups 3 | wpb 15467.948 | bsz 736.569 | num_updates 27705 | lr 0.000271885 | gnorm 0.220 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 8731 | train_wall 11423\n",
      "| epoch 054 | valid on 'valid' subset | loss 2.184 | nll_loss 0.353 | ppl 1.28 | num_updates 27705 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint54.pt (epoch 54 @ 27705 updates) (writing took 2.398577928543091 seconds)\n",
      "| epoch 055:   1000 / 1205 loss=2.016, nll_loss=0.465, ppl=1.38, wps=58282, ups=3, wpb=15473.392, bsz=736.828, num_updates=28706, lr=0.000267103, gnorm=0.217, clip=0.000, oom=0.000, loss_scale=0.500, wall=9049, train_wall=11642\n",
      "| epoch 055 | loss 2.016 | nll_loss 0.465 | ppl 1.38 | wps 58389 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 28910 | lr 0.000266159 | gnorm 0.224 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 9102 | train_wall 11686\n",
      "| epoch 055 | valid on 'valid' subset | loss 2.181 | nll_loss 0.349 | ppl 1.27 | num_updates 28910 | best_loss 2.04018\n",
      "| epoch 056 | loss 2.014 | nll_loss 0.463 | ppl 1.38 | wps 64852 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 30115 | lr 0.00026078 | gnorm 0.223 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 9437 | train_wall 11922\n",
      "| epoch 056 | valid on 'valid' subset | loss 2.182 | nll_loss 0.351 | ppl 1.28 | num_updates 30115 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint56.pt (epoch 56 @ 30115 updates) (writing took 2.124802350997925 seconds)\n",
      "| epoch 057:   1000 / 1205 loss=2.008, nll_loss=0.456, ppl=1.37, wps=59464, ups=3, wpb=15473.056, bsz=736.812, num_updates=31116, lr=0.000256551, gnorm=0.217, clip=0.000, oom=0.000, loss_scale=0.500, wall=9744, train_wall=12138\n",
      "| epoch 057 | loss 2.012 | nll_loss 0.461 | ppl 1.38 | wps 59290 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 31320 | lr 0.000255714 | gnorm 0.217 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 9798 | train_wall 12182\n",
      "| epoch 057 | valid on 'valid' subset | loss 2.179 | nll_loss 0.348 | ppl 1.27 | num_updates 31320 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint57.pt (epoch 57 @ 31320 updates) (writing took 2.3398447036743164 seconds)\n",
      "| epoch 058:   1000 / 1205 loss=2.009, nll_loss=0.458, ppl=1.37, wps=59631, ups=3, wpb=15473.392, bsz=736.828, num_updates=32321, lr=0.000251723, gnorm=0.216, clip=0.000, oom=0.000, loss_scale=0.500, wall=10104, train_wall=12395\n",
      "| epoch 058 | loss 2.009 | nll_loss 0.458 | ppl 1.37 | wps 59812 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 32525 | lr 0.000250932 | gnorm 0.220 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 10155 | train_wall 12437\n",
      "| epoch 058 | valid on 'valid' subset | loss 2.172 | nll_loss 0.348 | ppl 1.27 | num_updates 32525 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint58.pt (epoch 58 @ 32525 updates) (writing took 2.3127670288085938 seconds)\n",
      "| epoch 059:   1000 / 1205 loss=2.001, nll_loss=0.450, ppl=1.37, wps=61186, ups=3, wpb=15472.720, bsz=736.796, num_updates=33526, lr=0.000247158, gnorm=0.212, clip=0.000, oom=0.000, loss_scale=0.500, wall=10454, train_wall=12641\n",
      "| epoch 059 | loss 2.006 | nll_loss 0.454 | ppl 1.37 | wps 60834 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 33730 | lr 0.000246409 | gnorm 0.216 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 10508 | train_wall 12685\n",
      "| epoch 059 | valid on 'valid' subset | loss 2.176 | nll_loss 0.345 | ppl 1.27 | num_updates 33730 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint59.pt (epoch 59 @ 33730 updates) (writing took 2.278092384338379 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 0.25\n",
      "| epoch 060:   1000 / 1205 loss=2.004, nll_loss=0.453, ppl=1.37, wps=60165, ups=3, wpb=15473.409, bsz=736.829, num_updates=34730, lr=0.000242836, gnorm=0.205, clip=0.000, oom=0.000, loss_scale=0.250, wall=10811, train_wall=12892\n",
      "| epoch 060 | loss 2.001 | nll_loss 0.450 | ppl 1.37 | wps 60097 | ups 3 | wpb 15467.948 | bsz 736.569 | num_updates 34934 | lr 0.000242126 | gnorm 0.204 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 10864 | train_wall 12934\n",
      "| epoch 060 | valid on 'valid' subset | loss 2.178 | nll_loss 0.343 | ppl 1.27 | num_updates 34934 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint60.pt (epoch 60 @ 34934 updates) (writing took 2.1071319580078125 seconds)\n",
      "| epoch 061:   1000 / 1205 loss=1.999, nll_loss=0.447, ppl=1.36, wps=63942, ups=3, wpb=15473.392, bsz=736.828, num_updates=35935, lr=0.00023873, gnorm=0.222, clip=0.000, oom=0.000, loss_scale=0.250, wall=11150, train_wall=13132\n",
      "| epoch 061 | loss 2.001 | nll_loss 0.450 | ppl 1.37 | wps 63881 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 36139 | lr 0.000238055 | gnorm 0.220 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 11199 | train_wall 13172\n",
      "| epoch 061 | valid on 'valid' subset | loss 2.174 | nll_loss 0.347 | ppl 1.27 | num_updates 36139 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint61.pt (epoch 61 @ 36139 updates) (writing took 2.2244045734405518 seconds)\n",
      "| epoch 062:   1000 / 1205 loss=1.991, nll_loss=0.439, ppl=1.36, wps=60521, ups=3, wpb=15472.720, bsz=736.796, num_updates=37140, lr=0.000234825, gnorm=0.215, clip=0.000, oom=0.000, loss_scale=0.250, wall=11503, train_wall=13379\n",
      "| epoch 062 | loss 2.000 | nll_loss 0.448 | ppl 1.36 | wps 60585 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 37344 | lr 0.000234183 | gnorm 0.216 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 11555 | train_wall 13421\n",
      "| epoch 062 | valid on 'valid' subset | loss 2.168 | nll_loss 0.339 | ppl 1.27 | num_updates 37344 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint62.pt (epoch 62 @ 37344 updates) (writing took 2.201096296310425 seconds)\n",
      "| epoch 063:   1000 / 1205 loss=2.004, nll_loss=0.453, ppl=1.37, wps=60314, ups=3, wpb=15481.846, bsz=737.231, num_updates=38345, lr=0.000231106, gnorm=0.210, clip=0.000, oom=0.000, loss_scale=0.250, wall=11860, train_wall=13628\n",
      "| epoch 063 | loss 1.995 | nll_loss 0.443 | ppl 1.36 | wps 60146 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 38549 | lr 0.000230493 | gnorm 0.208 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 11913 | train_wall 13671\n",
      "| epoch 063 | valid on 'valid' subset | loss 2.173 | nll_loss 0.339 | ppl 1.26 | num_updates 38549 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint63.pt (epoch 63 @ 38549 updates) (writing took 2.138805627822876 seconds)\n",
      "| epoch 064:   1000 / 1205 loss=1.997, nll_loss=0.445, ppl=1.36, wps=60749, ups=3, wpb=15474.734, bsz=736.892, num_updates=39550, lr=0.000227558, gnorm=0.205, clip=0.000, oom=0.000, loss_scale=0.250, wall=12214, train_wall=13877\n",
      "| epoch 064 | loss 1.993 | nll_loss 0.441 | ppl 1.36 | wps 60712 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 39754 | lr 0.000226973 | gnorm 0.204 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 12266 | train_wall 13920\n",
      "| epoch 064 | valid on 'valid' subset | loss 2.164 | nll_loss 0.337 | ppl 1.26 | num_updates 39754 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint64.pt (epoch 64 @ 39754 updates) (writing took 2.142831325531006 seconds)\n",
      "| epoch 065:   1000 / 1205 loss=1.997, nll_loss=0.445, ppl=1.36, wps=60487, ups=3, wpb=15474.063, bsz=736.860, num_updates=40755, lr=0.000224168, gnorm=0.210, clip=0.000, oom=0.000, loss_scale=0.250, wall=12567, train_wall=14127\n",
      "| epoch 065 | loss 1.993 | nll_loss 0.441 | ppl 1.36 | wps 60283 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 40959 | lr 0.00022361 | gnorm 0.210 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 12620 | train_wall 14170\n",
      "| epoch 065 | valid on 'valid' subset | loss 2.174 | nll_loss 0.341 | ppl 1.27 | num_updates 40959 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint65.pt (epoch 65 @ 40959 updates) (writing took 2.2061266899108887 seconds)\n",
      "| epoch 066:   1000 / 1205 loss=1.984, nll_loss=0.431, ppl=1.35, wps=63824, ups=3, wpb=15472.049, bsz=736.764, num_updates=41960, lr=0.000220926, gnorm=0.198, clip=0.000, oom=0.000, loss_scale=0.250, wall=12912, train_wall=14369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 066 | loss 1.990 | nll_loss 0.438 | ppl 1.35 | wps 64178 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 42164 | lr 0.000220391 | gnorm 0.202 | clip 0.000 | oom 0.000 | loss_scale 0.250 | wall 12959 | train_wall 14409\n",
      "| epoch 066 | valid on 'valid' subset | loss 2.168 | nll_loss 0.337 | ppl 1.26 | num_updates 42164 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint66.pt (epoch 66 @ 42164 updates) (writing took 2.205389976501465 seconds)\n",
      "| epoch 067:   1000 / 1205 loss=1.988, nll_loss=0.436, ppl=1.35, wps=62032, ups=3, wpb=15474.063, bsz=736.860, num_updates=43165, lr=0.000217821, gnorm=0.220, clip=0.000, oom=0.000, loss_scale=0.500, wall=13250, train_wall=14612\n",
      "| epoch 067 | loss 1.988 | nll_loss 0.436 | ppl 1.35 | wps 61729 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 43369 | lr 0.000217308 | gnorm 0.215 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 13303 | train_wall 14654\n",
      "| epoch 067 | valid on 'valid' subset | loss 2.169 | nll_loss 0.338 | ppl 1.26 | num_updates 43369 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint67.pt (epoch 67 @ 43369 updates) (writing took 2.164910316467285 seconds)\n",
      "| epoch 068:   1000 / 1205 loss=1.985, nll_loss=0.432, ppl=1.35, wps=60380, ups=3, wpb=15480.168, bsz=737.151, num_updates=44370, lr=0.000214843, gnorm=0.210, clip=0.000, oom=0.000, loss_scale=0.500, wall=13608, train_wall=14862\n",
      "| epoch 068 | loss 1.986 | nll_loss 0.434 | ppl 1.35 | wps 60268 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 44574 | lr 0.00021435 | gnorm 0.206 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 13661 | train_wall 14905\n",
      "| epoch 068 | valid on 'valid' subset | loss 2.169 | nll_loss 0.335 | ppl 1.26 | num_updates 44574 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint68.pt (epoch 68 @ 44574 updates) (writing took 2.848090410232544 seconds)\n",
      "| epoch 069:   1000 / 1205 loss=1.994, nll_loss=0.442, ppl=1.36, wps=60264, ups=3, wpb=15475.406, bsz=736.924, num_updates=45575, lr=0.000211983, gnorm=0.205, clip=0.000, oom=0.000, loss_scale=0.500, wall=13969, train_wall=15113\n",
      "| epoch 069 | loss 1.985 | nll_loss 0.432 | ppl 1.35 | wps 60423 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 45779 | lr 0.00021151 | gnorm 0.204 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 14021 | train_wall 15154\n",
      "| epoch 069 | valid on 'valid' subset | loss 2.167 | nll_loss 0.334 | ppl 1.26 | num_updates 45779 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint69.pt (epoch 69 @ 45779 updates) (writing took 2.32534122467041 seconds)\n",
      "| epoch 070:   1000 / 1205 loss=1.977, nll_loss=0.424, ppl=1.34, wps=61452, ups=3, wpb=15472.720, bsz=736.796, num_updates=46780, lr=0.000209235, gnorm=0.195, clip=0.000, oom=0.000, loss_scale=0.500, wall=14323, train_wall=15359\n",
      "| epoch 070 | loss 1.984 | nll_loss 0.432 | ppl 1.35 | wps 61307 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 46984 | lr 0.000208781 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 14375 | train_wall 15402\n",
      "| epoch 070 | valid on 'valid' subset | loss 2.164 | nll_loss 0.332 | ppl 1.26 | num_updates 46984 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint70.pt (epoch 70 @ 46984 updates) (writing took 2.286912679672241 seconds)\n",
      "| epoch 071:   1000 / 1205 loss=1.975, nll_loss=0.422, ppl=1.34, wps=61753, ups=3, wpb=15472.720, bsz=736.796, num_updates=47985, lr=0.000206591, gnorm=0.191, clip=0.000, oom=0.000, loss_scale=0.500, wall=14673, train_wall=15605\n",
      "| epoch 071 | loss 1.983 | nll_loss 0.431 | ppl 1.35 | wps 61978 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 48189 | lr 0.000206154 | gnorm 0.192 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 14723 | train_wall 15647\n",
      "| epoch 071 | valid on 'valid' subset | loss 2.162 | nll_loss 0.332 | ppl 1.26 | num_updates 48189 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint71.pt (epoch 71 @ 48189 updates) (writing took 2.1427698135375977 seconds)\n",
      "| epoch 072:   1000 / 1205 loss=1.981, nll_loss=0.428, ppl=1.35, wps=62576, ups=3, wpb=15473.727, bsz=736.844, num_updates=49190, lr=0.000204045, gnorm=0.200, clip=0.000, oom=0.000, loss_scale=0.500, wall=15016, train_wall=15847\n",
      "| epoch 072 | loss 1.980 | nll_loss 0.428 | ppl 1.35 | wps 61566 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 49394 | lr 0.000203623 | gnorm 0.197 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 15071 | train_wall 15892\n",
      "| epoch 072 | valid on 'valid' subset | loss 2.168 | nll_loss 0.335 | ppl 1.26 | num_updates 49394 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint72.pt (epoch 72 @ 49394 updates) (writing took 2.3549611568450928 seconds)\n",
      "| epoch 073:   1000 / 1205 loss=1.981, nll_loss=0.429, ppl=1.35, wps=57766, ups=3, wpb=15480.503, bsz=737.167, num_updates=50395, lr=0.000201591, gnorm=0.217, clip=0.000, oom=0.000, loss_scale=0.500, wall=15388, train_wall=16113\n",
      "| epoch 073 | loss 1.979 | nll_loss 0.427 | ppl 1.34 | wps 58070 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 50599 | lr 0.000201184 | gnorm 0.211 | clip 0.000 | oom 0.000 | loss_scale 0.500 | wall 15441 | train_wall 16156\n",
      "| epoch 073 | valid on 'valid' subset | loss 2.169 | nll_loss 0.336 | ppl 1.26 | num_updates 50599 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint73.pt (epoch 73 @ 50599 updates) (writing took 2.2463178634643555 seconds)\n",
      "| epoch 074:   1000 / 1205 loss=1.994, nll_loss=0.443, ppl=1.36, wps=57879, ups=3, wpb=15475.741, bsz=736.940, num_updates=51600, lr=0.000199223, gnorm=0.206, clip=0.000, oom=0.000, loss_scale=1.000, wall=15755, train_wall=16376\n",
      "| epoch 074 | loss 1.979 | nll_loss 0.426 | ppl 1.34 | wps 58116 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 51804 | lr 0.000198831 | gnorm 0.203 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 15809 | train_wall 16420\n",
      "| epoch 074 | valid on 'valid' subset | loss 2.169 | nll_loss 0.339 | ppl 1.26 | num_updates 51804 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint74.pt (epoch 74 @ 51804 updates) (writing took 3.2970175743103027 seconds)\n",
      "| epoch 075:   1000 / 1205 loss=1.974, nll_loss=0.421, ppl=1.34, wps=36092, ups=2, wpb=15478.825, bsz=737.087, num_updates=52805, lr=0.000196937, gnorm=0.198, clip=0.000, oom=0.000, loss_scale=1.000, wall=16286, train_wall=16793\n",
      "| epoch 075 | loss 1.976 | nll_loss 0.424 | ppl 1.34 | wps 35559 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 53009 | lr 0.000196558 | gnorm 0.198 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 16381 | train_wall 16878\n",
      "| epoch 075 | valid on 'valid' subset | loss 2.164 | nll_loss 0.336 | ppl 1.26 | num_updates 53009 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint75.pt (epoch 75 @ 53009 updates) (writing took 3.484171152114868 seconds)\n",
      "| epoch 076:   1000 / 1205 loss=1.976, nll_loss=0.423, ppl=1.34, wps=37785, ups=2, wpb=15473.392, bsz=736.828, num_updates=54010, lr=0.000194728, gnorm=0.197, clip=0.000, oom=0.000, loss_scale=1.000, wall=16862, train_wall=17235\n",
      "| epoch 076 | loss 1.975 | nll_loss 0.422 | ppl 1.34 | wps 36572 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 54214 | lr 0.000194361 | gnorm 0.195 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 16961 | train_wall 17323\n",
      "| epoch 076 | valid on 'valid' subset | loss 2.168 | nll_loss 0.337 | ppl 1.26 | num_updates 54214 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint76.pt (epoch 76 @ 54214 updates) (writing took 4.082276821136475 seconds)\n",
      "| epoch 077:   1000 / 1205 loss=1.975, nll_loss=0.423, ppl=1.34, wps=36823, ups=2, wpb=15473.727, bsz=736.844, num_updates=55215, lr=0.000192591, gnorm=0.195, clip=0.000, oom=0.000, loss_scale=1.000, wall=17463, train_wall=17686\n",
      "| epoch 077 | loss 1.973 | nll_loss 0.420 | ppl 1.34 | wps 37983 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 55419 | lr 0.000192236 | gnorm 0.194 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 17533 | train_wall 17745\n",
      "| epoch 077 | valid on 'valid' subset | loss 2.168 | nll_loss 0.333 | ppl 1.26 | num_updates 55419 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint77.pt (epoch 77 @ 55419 updates) (writing took 4.213014841079712 seconds)\n",
      "| epoch 078:   1000 / 1205 loss=1.974, nll_loss=0.421, ppl=1.34, wps=42299, ups=2, wpb=15473.056, bsz=736.812, num_updates=56420, lr=0.000190523, gnorm=0.192, clip=0.000, oom=0.000, loss_scale=1.000, wall=17959, train_wall=18058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 078 | loss 1.973 | nll_loss 0.420 | ppl 1.34 | wps 40763 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 56624 | lr 0.00019018 | gnorm 0.192 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 18050 | train_wall 18137\n",
      "| epoch 078 | valid on 'valid' subset | loss 2.164 | nll_loss 0.333 | ppl 1.26 | num_updates 56624 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint78.pt (epoch 78 @ 56624 updates) (writing took 3.7333972454071045 seconds)\n",
      "| epoch 079:   1000 / 1205 loss=1.966, nll_loss=0.412, ppl=1.33, wps=35006, ups=2, wpb=15472.720, bsz=736.796, num_updates=57625, lr=0.000188521, gnorm=0.194, clip=0.000, oom=0.000, loss_scale=1.000, wall=18564, train_wall=18522\n",
      "| epoch 079 | loss 1.971 | nll_loss 0.418 | ppl 1.34 | wps 36655 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 57829 | lr 0.000188188 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 18630 | train_wall 18578\n",
      "| epoch 079 | valid on 'valid' subset | loss 2.164 | nll_loss 0.329 | ppl 1.26 | num_updates 57829 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint79.pt (epoch 79 @ 57829 updates) (writing took 3.1914236545562744 seconds)\n",
      "| epoch 080:   1000 / 1205 loss=1.974, nll_loss=0.421, ppl=1.34, wps=49571, ups=3, wpb=15474.063, bsz=736.860, num_updates=58830, lr=0.00018658, gnorm=0.191, clip=0.000, oom=0.000, loss_scale=1.000, wall=18995, train_wall=18842\n",
      "| epoch 080 | loss 1.969 | nll_loss 0.415 | ppl 1.33 | wps 50867 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 59034 | lr 0.000186258 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 19049 | train_wall 18885\n",
      "| epoch 080 | valid on 'valid' subset | loss 2.163 | nll_loss 0.335 | ppl 1.26 | num_updates 59034 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint80.pt (epoch 80 @ 59034 updates) (writing took 2.6879899501800537 seconds)\n",
      "| epoch 081:   1000 / 1205 loss=1.970, nll_loss=0.417, ppl=1.34, wps=58976, ups=3, wpb=15480.168, bsz=737.151, num_updates=60035, lr=0.000184698, gnorm=0.184, clip=0.000, oom=0.000, loss_scale=2.000, wall=19360, train_wall=19100\n",
      "| epoch 081 | loss 1.969 | nll_loss 0.415 | ppl 1.33 | wps 58843 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 60239 | lr 0.000184385 | gnorm 0.187 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 19414 | train_wall 19144\n",
      "| epoch 081 | valid on 'valid' subset | loss 2.167 | nll_loss 0.329 | ppl 1.26 | num_updates 60239 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint81.pt (epoch 81 @ 60239 updates) (writing took 4.694702863693237 seconds)\n",
      "| epoch 082:   1000 / 1205 loss=1.971, nll_loss=0.418, ppl=1.34, wps=32697, ups=2, wpb=15474.063, bsz=736.860, num_updates=61240, lr=0.000182872, gnorm=0.197, clip=0.000, oom=0.000, loss_scale=2.000, wall=19952, train_wall=19561\n",
      "| epoch 082 | loss 1.968 | nll_loss 0.415 | ppl 1.33 | wps 32486 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 61444 | lr 0.000182568 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 20052 | train_wall 19649\n",
      "| epoch 082 | valid on 'valid' subset | loss 2.161 | nll_loss 0.329 | ppl 1.26 | num_updates 61444 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint82.pt (epoch 82 @ 61444 updates) (writing took 3.4471638202667236 seconds)\n",
      "| epoch 083:   1000 / 1205 loss=1.967, nll_loss=0.414, ppl=1.33, wps=31932, ups=2, wpb=15473.392, bsz=736.828, num_updates=62445, lr=0.000181099, gnorm=0.202, clip=0.000, oom=0.000, loss_scale=2.000, wall=20619, train_wall=20079\n",
      "| epoch 083 | loss 1.969 | nll_loss 0.416 | ppl 1.33 | wps 31761 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 62649 | lr 0.000180804 | gnorm 0.202 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 20721 | train_wall 20170\n",
      "| epoch 083 | valid on 'valid' subset | loss 2.161 | nll_loss 0.330 | ppl 1.26 | num_updates 62649 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint83.pt (epoch 83 @ 62649 updates) (writing took 3.5318641662597656 seconds)\n",
      "| epoch 084:   1000 / 1205 loss=1.975, nll_loss=0.422, ppl=1.34, wps=31260, ups=2, wpb=15481.175, bsz=737.199, num_updates=63650, lr=0.000179377, gnorm=0.190, clip=0.000, oom=0.000, loss_scale=2.000, wall=21293, train_wall=20608\n",
      "| epoch 084 | loss 1.967 | nll_loss 0.413 | ppl 1.33 | wps 31464 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 63854 | lr 0.00017909 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 21390 | train_wall 20693\n",
      "| epoch 084 | valid on 'valid' subset | loss 2.163 | nll_loss 0.330 | ppl 1.26 | num_updates 63854 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint84.pt (epoch 84 @ 63854 updates) (writing took 3.523024559020996 seconds)\n",
      "| epoch 085:   1000 / 1205 loss=1.963, nll_loss=0.410, ppl=1.33, wps=26453, ups=2, wpb=15479.832, bsz=737.135, num_updates=64855, lr=0.000177702, gnorm=0.188, clip=0.000, oom=0.000, loss_scale=2.000, wall=22056, train_wall=21219\n",
      "| epoch 085 | loss 1.965 | nll_loss 0.412 | ppl 1.33 | wps 25886 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 65059 | lr 0.000177424 | gnorm 0.187 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 22190 | train_wall 21340\n",
      "| epoch 085 | valid on 'valid' subset | loss 2.162 | nll_loss 0.330 | ppl 1.26 | num_updates 65059 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint85.pt (epoch 85 @ 65059 updates) (writing took 3.822547435760498 seconds)\n",
      "| epoch 086:   1000 / 1205 loss=1.957, nll_loss=0.403, ppl=1.32, wps=30050, ups=2, wpb=15471.713, bsz=736.748, num_updates=66060, lr=0.000176074, gnorm=0.191, clip=0.000, oom=0.000, loss_scale=2.000, wall=22808, train_wall=21798\n",
      "| epoch 086 | loss 1.965 | nll_loss 0.411 | ppl 1.33 | wps 30354 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 66264 | lr 0.000175803 | gnorm 0.191 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 22907 | train_wall 21887\n",
      "| epoch 086 | valid on 'valid' subset | loss 2.161 | nll_loss 0.330 | ppl 1.26 | num_updates 66264 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint86.pt (epoch 86 @ 66264 updates) (writing took 2.5784547328948975 seconds)\n",
      "| epoch 087:   1000 / 1205 loss=1.958, nll_loss=0.404, ppl=1.32, wps=36971, ups=2, wpb=15472.720, bsz=736.796, num_updates=67265, lr=0.00017449, gnorm=0.200, clip=0.000, oom=0.000, loss_scale=4.000, wall=23386, train_wall=22255\n",
      "| epoch 087 | loss 1.962 | nll_loss 0.409 | ppl 1.33 | wps 36461 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 67469 | lr 0.000174226 | gnorm 0.196 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 23478 | train_wall 22336\n",
      "| epoch 087 | valid on 'valid' subset | loss 2.163 | nll_loss 0.328 | ppl 1.26 | num_updates 67469 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint87.pt (epoch 87 @ 67469 updates) (writing took 3.4469072818756104 seconds)\n",
      "| epoch 088:   1000 / 1205 loss=1.961, nll_loss=0.407, ppl=1.33, wps=41263, ups=2, wpb=15479.497, bsz=737.119, num_updates=68470, lr=0.000172948, gnorm=0.188, clip=0.000, oom=0.000, loss_scale=4.000, wall=23918, train_wall=22661\n",
      "| epoch 088 | loss 1.963 | nll_loss 0.410 | ppl 1.33 | wps 41455 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 68674 | lr 0.000172691 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 23992 | train_wall 22725\n",
      "| epoch 088 | valid on 'valid' subset | loss 2.166 | nll_loss 0.331 | ppl 1.26 | num_updates 68674 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint88.pt (epoch 88 @ 68674 updates) (writing took 3.8152992725372314 seconds)\n",
      "| epoch 089:   1000 / 1205 loss=1.965, nll_loss=0.412, ppl=1.33, wps=41029, ups=2, wpb=15473.727, bsz=736.844, num_updates=69675, lr=0.000171446, gnorm=0.187, clip=0.000, oom=0.000, loss_scale=4.000, wall=24435, train_wall=23052\n",
      "| epoch 089 | loss 1.961 | nll_loss 0.407 | ppl 1.33 | wps 41052 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 69879 | lr 0.000171195 | gnorm 0.185 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 24512 | train_wall 23119\n",
      "| epoch 089 | valid on 'valid' subset | loss 2.158 | nll_loss 0.328 | ppl 1.26 | num_updates 69879 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint89.pt (epoch 89 @ 69879 updates) (writing took 3.6175129413604736 seconds)\n",
      "| epoch 090:   1000 / 1205 loss=1.961, nll_loss=0.408, ppl=1.33, wps=43109, ups=2, wpb=15473.727, bsz=736.844, num_updates=70880, lr=0.000169982, gnorm=0.192, clip=0.000, oom=0.000, loss_scale=4.000, wall=24924, train_wall=23431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch 090 | loss 1.961 | nll_loss 0.407 | ppl 1.33 | wps 42650 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 71084 | lr 0.000169738 | gnorm 0.191 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 25002 | train_wall 23499\n",
      "| epoch 090 | valid on 'valid' subset | loss 2.161 | nll_loss 0.328 | ppl 1.26 | num_updates 71084 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint90.pt (epoch 90 @ 71084 updates) (writing took 3.4028878211975098 seconds)\n",
      "| epoch 091:   1000 / 1205 loss=1.953, nll_loss=0.399, ppl=1.32, wps=50879, ups=3, wpb=15472.385, bsz=736.780, num_updates=72085, lr=0.000168555, gnorm=0.188, clip=0.000, oom=0.000, loss_scale=4.000, wall=25372, train_wall=23756\n",
      "| epoch 091 | loss 1.959 | nll_loss 0.405 | ppl 1.32 | wps 52061 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 72289 | lr 0.000168317 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 4.000 | wall 25425 | train_wall 23800\n",
      "| epoch 091 | valid on 'valid' subset | loss 2.161 | nll_loss 0.328 | ppl 1.25 | num_updates 72289 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint91.pt (epoch 91 @ 72289 updates) (writing took 2.6744496822357178 seconds)\n",
      "| WARNING: overflow detected, setting loss scale to: 2.0\n",
      "| epoch 092:   1000 / 1205 loss=1.959, nll_loss=0.405, ppl=1.32, wps=59673, ups=3, wpb=15474.753, bsz=736.893, num_updates=73289, lr=0.000167165, gnorm=0.186, clip=0.000, oom=0.000, loss_scale=2.000, wall=25734, train_wall=24011\n",
      "| epoch 092 | loss 1.958 | nll_loss 0.404 | ppl 1.32 | wps 59677 | ups 3 | wpb 15467.948 | bsz 736.569 | num_updates 73493 | lr 0.000166933 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 25787 | train_wall 24054\n",
      "| epoch 092 | valid on 'valid' subset | loss 2.160 | nll_loss 0.329 | ppl 1.26 | num_updates 73493 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint92.pt (epoch 92 @ 73493 updates) (writing took 2.422037363052368 seconds)\n",
      "| epoch 093:   1000 / 1205 loss=1.953, nll_loss=0.398, ppl=1.32, wps=60450, ups=3, wpb=15472.720, bsz=736.796, num_updates=74494, lr=0.000165808, gnorm=0.192, clip=0.000, oom=0.000, loss_scale=2.000, wall=26093, train_wall=24264\n",
      "| epoch 093 | loss 1.958 | nll_loss 0.404 | ppl 1.32 | wps 61231 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 74698 | lr 0.000165581 | gnorm 0.190 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 26141 | train_wall 24305\n",
      "| epoch 093 | valid on 'valid' subset | loss 2.161 | nll_loss 0.327 | ppl 1.25 | num_updates 74698 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint93.pt (epoch 93 @ 74698 updates) (writing took 2.4665207862854004 seconds)\n",
      "| epoch 094:   1000 / 1205 loss=1.958, nll_loss=0.405, ppl=1.32, wps=63394, ups=3, wpb=15479.832, bsz=737.135, num_updates=75699, lr=0.000164483, gnorm=0.195, clip=0.000, oom=0.000, loss_scale=2.000, wall=26431, train_wall=24506\n",
      "| epoch 094 | loss 1.958 | nll_loss 0.405 | ppl 1.32 | wps 62429 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 75903 | lr 0.000164261 | gnorm 0.200 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 26485 | train_wall 24550\n",
      "| epoch 094 | valid on 'valid' subset | loss 2.161 | nll_loss 0.328 | ppl 1.26 | num_updates 75903 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint94.pt (epoch 94 @ 75903 updates) (writing took 2.9183108806610107 seconds)\n",
      "| epoch 095:   1000 / 1205 loss=1.961, nll_loss=0.407, ppl=1.33, wps=45863, ups=3, wpb=15475.070, bsz=736.908, num_updates=76904, lr=0.000163189, gnorm=0.188, clip=0.000, oom=0.000, loss_scale=2.000, wall=26869, train_wall=24837\n",
      "| WARNING: overflow detected, setting loss scale to: 1.0\n",
      "| epoch 095 | loss 1.956 | nll_loss 0.402 | ppl 1.32 | wps 44789 | ups 3 | wpb 15468.506 | bsz 736.596 | num_updates 77107 | lr 0.000162974 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 26947 | train_wall 24904\n",
      "| epoch 095 | valid on 'valid' subset | loss 2.163 | nll_loss 0.328 | ppl 1.26 | num_updates 77107 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint95.pt (epoch 95 @ 77107 updates) (writing took 2.8142762184143066 seconds)\n",
      "| epoch 096:   1000 / 1205 loss=1.964, nll_loss=0.411, ppl=1.33, wps=48456, ups=3, wpb=15475.070, bsz=736.908, num_updates=78108, lr=0.000161926, gnorm=0.193, clip=0.000, oom=0.000, loss_scale=1.000, wall=27329, train_wall=25175\n",
      "| epoch 096 | loss 1.955 | nll_loss 0.401 | ppl 1.32 | wps 49723 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 78312 | lr 0.000161715 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 27384 | train_wall 25221\n",
      "| epoch 096 | valid on 'valid' subset | loss 2.166 | nll_loss 0.332 | ppl 1.26 | num_updates 78312 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint96.pt (epoch 96 @ 78312 updates) (writing took 2.4753403663635254 seconds)\n",
      "| epoch 097:   1000 / 1205 loss=1.950, nll_loss=0.395, ppl=1.32, wps=63200, ups=3, wpb=15478.825, bsz=737.087, num_updates=79313, lr=0.000160691, gnorm=0.187, clip=0.000, oom=0.000, loss_scale=1.000, wall=27678, train_wall=25422\n",
      "| epoch 097 | loss 1.955 | nll_loss 0.402 | ppl 1.32 | wps 63620 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 79517 | lr 0.000160485 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 27725 | train_wall 25462\n",
      "| epoch 097 | valid on 'valid' subset | loss 2.164 | nll_loss 0.330 | ppl 1.26 | num_updates 79517 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint97.pt (epoch 97 @ 79517 updates) (writing took 2.543107271194458 seconds)\n",
      "| epoch 098:   1000 / 1205 loss=1.959, nll_loss=0.406, ppl=1.32, wps=64585, ups=4, wpb=15474.734, bsz=736.892, num_updates=80518, lr=0.000159485, gnorm=0.191, clip=0.000, oom=0.000, loss_scale=1.000, wall=28011, train_wall=25662\n",
      "| epoch 098 | loss 1.955 | nll_loss 0.401 | ppl 1.32 | wps 64824 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 80722 | lr 0.000159283 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 28058 | train_wall 25702\n",
      "| epoch 098 | valid on 'valid' subset | loss 2.163 | nll_loss 0.328 | ppl 1.26 | num_updates 80722 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint98.pt (epoch 98 @ 80722 updates) (writing took 2.4547877311706543 seconds)\n",
      "| epoch 099:   1000 / 1205 loss=1.945, nll_loss=0.390, ppl=1.31, wps=64800, ups=4, wpb=15471.713, bsz=736.748, num_updates=81723, lr=0.000158304, gnorm=0.187, clip=0.000, oom=0.000, loss_scale=1.000, wall=28338, train_wall=25896\n",
      "| epoch 099 | loss 1.952 | nll_loss 0.398 | ppl 1.32 | wps 64506 | ups 4 | wpb 15468.217 | bsz 736.582 | num_updates 81927 | lr 0.000158107 | gnorm 0.188 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 28388 | train_wall 25937\n",
      "| epoch 099 | valid on 'valid' subset | loss 2.163 | nll_loss 0.327 | ppl 1.25 | num_updates 81927 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint99.pt (epoch 99 @ 81927 updates) (writing took 2.5380725860595703 seconds)\n",
      "| epoch 100:   1000 / 1205 loss=1.953, nll_loss=0.399, ppl=1.32, wps=53153, ups=3, wpb=15480.503, bsz=737.167, num_updates=82928, lr=0.00015715, gnorm=0.195, clip=0.000, oom=0.000, loss_scale=1.000, wall=28724, train_wall=26185\n",
      "| epoch 100 | loss 1.952 | nll_loss 0.398 | ppl 1.32 | wps 50974 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 83132 | lr 0.000156957 | gnorm 0.191 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 28798 | train_wall 26250\n",
      "| epoch 100 | valid on 'valid' subset | loss 2.161 | nll_loss 0.327 | ppl 1.25 | num_updates 83132 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint100.pt (epoch 100 @ 83132 updates) (writing took 2.556135892868042 seconds)\n",
      "| epoch 101:   1000 / 1205 loss=1.956, nll_loss=0.403, ppl=1.32, wps=41592, ups=2, wpb=15474.734, bsz=736.892, num_updates=84133, lr=0.000156021, gnorm=0.187, clip=0.000, oom=0.000, loss_scale=1.000, wall=29228, train_wall=26571\n",
      "| epoch 101 | loss 1.951 | nll_loss 0.397 | ppl 1.32 | wps 40757 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 84337 | lr 0.000155832 | gnorm 0.186 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 29313 | train_wall 26644\n",
      "| epoch 101 | valid on 'valid' subset | loss 2.164 | nll_loss 0.328 | ppl 1.25 | num_updates 84337 | best_loss 2.04018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint101.pt (epoch 101 @ 84337 updates) (writing took 2.4286229610443115 seconds)\n",
      "| epoch 102:   1000 / 1205 loss=1.952, nll_loss=0.398, ppl=1.32, wps=41903, ups=2, wpb=15474.399, bsz=736.876, num_updates=85338, lr=0.000154915, gnorm=0.191, clip=0.000, oom=0.000, loss_scale=2.000, wall=29733, train_wall=26966\n",
      "| epoch 102 | loss 1.951 | nll_loss 0.397 | ppl 1.32 | wps 41600 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 85542 | lr 0.00015473 | gnorm 0.190 | clip 0.000 | oom 0.000 | loss_scale 2.000 | wall 29812 | train_wall 27036\n",
      "| epoch 102 | valid on 'valid' subset | loss 2.162 | nll_loss 0.328 | ppl 1.25 | num_updates 85542 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint102.pt (epoch 102 @ 85542 updates) (writing took 2.5001649856567383 seconds)\n",
      "| epoch 103:   1000 / 1205 loss=1.940, nll_loss=0.384, ppl=1.31, wps=42642, ups=2, wpb=15478.825, bsz=737.087, num_updates=86543, lr=0.000153833, gnorm=0.194, clip=0.000, oom=0.000, loss_scale=2.000, wall=30235, train_wall=27353\n",
      "| WARNING: overflow detected, setting loss scale to: 1.0\n",
      "| epoch 103 | loss 1.951 | nll_loss 0.397 | ppl 1.32 | wps 42359 | ups 2 | wpb 15468.227 | bsz 736.582 | num_updates 86746 | lr 0.000153653 | gnorm 0.193 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 30311 | train_wall 27419\n",
      "| epoch 103 | valid on 'valid' subset | loss 2.162 | nll_loss 0.328 | ppl 1.26 | num_updates 86746 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint103.pt (epoch 103 @ 86746 updates) (writing took 2.418883800506592 seconds)\n",
      "| epoch 104:   1000 / 1205 loss=1.951, nll_loss=0.396, ppl=1.32, wps=42804, ups=2, wpb=15474.734, bsz=736.892, num_updates=87747, lr=0.000152774, gnorm=0.191, clip=0.000, oom=0.000, loss_scale=1.000, wall=30731, train_wall=27735\n",
      "| epoch 104 | loss 1.949 | nll_loss 0.395 | ppl 1.32 | wps 42689 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 87951 | lr 0.000152597 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 30806 | train_wall 27800\n",
      "| epoch 104 | valid on 'valid' subset | loss 2.164 | nll_loss 0.328 | ppl 1.25 | num_updates 87951 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint104.pt (epoch 104 @ 87951 updates) (writing took 2.423327922821045 seconds)\n",
      "| epoch 105:   1000 / 1205 loss=1.959, nll_loss=0.406, ppl=1.33, wps=42717, ups=2, wpb=15475.070, bsz=736.908, num_updates=88952, lr=0.000151735, gnorm=0.193, clip=0.000, oom=0.000, loss_scale=1.000, wall=31224, train_wall=28116\n",
      "| epoch 105 | loss 1.950 | nll_loss 0.396 | ppl 1.32 | wps 42236 | ups 2 | wpb 15468.217 | bsz 736.582 | num_updates 89156 | lr 0.000151562 | gnorm 0.192 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 31303 | train_wall 28186\n",
      "| epoch 105 | valid on 'valid' subset | loss 2.163 | nll_loss 0.332 | ppl 1.26 | num_updates 89156 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint105.pt (epoch 105 @ 89156 updates) (writing took 2.237250804901123 seconds)\n",
      "| epoch 106:   1000 / 1205 loss=1.947, nll_loss=0.393, ppl=1.31, wps=43272, ups=2, wpb=15473.727, bsz=736.844, num_updates=90157, lr=0.000150718, gnorm=0.189, clip=0.000, oom=0.000, loss_scale=1.000, wall=31712, train_wall=28498\n",
      "| epoch 106 | loss 1.948 | nll_loss 0.394 | ppl 1.31 | wps 45768 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 90361 | lr 0.000150548 | gnorm 0.189 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 31762 | train_wall 28539\n",
      "| epoch 106 | valid on 'valid' subset | loss 2.160 | nll_loss 0.327 | ppl 1.25 | num_updates 90361 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint106.pt (epoch 106 @ 90361 updates) (writing took 2.3038480281829834 seconds)\n",
      "| epoch 107:   1000 / 1205 loss=1.955, nll_loss=0.402, ppl=1.32, wps=46023, ups=3, wpb=15474.734, bsz=736.892, num_updates=91362, lr=0.000149721, gnorm=0.191, clip=0.000, oom=0.000, loss_scale=1.000, wall=32143, train_wall=28830\n",
      "| epoch 107 | loss 1.947 | nll_loss 0.393 | ppl 1.31 | wps 45953 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 91566 | lr 0.000149554 | gnorm 0.190 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 32213 | train_wall 28889\n",
      "| epoch 107 | valid on 'valid' subset | loss 2.158 | nll_loss 0.328 | ppl 1.26 | num_updates 91566 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint107.pt (epoch 107 @ 91566 updates) (writing took 2.375657558441162 seconds)\n",
      "| epoch 108:   1000 / 1205 loss=1.944, nll_loss=0.389, ppl=1.31, wps=59401, ups=3, wpb=15472.385, bsz=736.780, num_updates=92567, lr=0.000148743, gnorm=0.194, clip=0.000, oom=0.000, loss_scale=1.000, wall=32525, train_wall=29109\n",
      "| epoch 108 | loss 1.946 | nll_loss 0.392 | ppl 1.31 | wps 60608 | ups 3 | wpb 15468.217 | bsz 736.582 | num_updates 92771 | lr 0.000148579 | gnorm 0.192 | clip 0.000 | oom 0.000 | loss_scale 1.000 | wall 32572 | train_wall 29148\n",
      "| epoch 108 | valid on 'valid' subset | loss 2.162 | nll_loss 0.328 | ppl 1.26 | num_updates 92771 | best_loss 2.04018\n",
      "| saved checkpoint checkpoints/bert_nmt_0.25/checkpoint108.pt (epoch 108 @ 92771 updates) (writing took 2.187803268432617 seconds)\n",
      "| epoch 109:   1000 / 1205 loss=1.955, nll_loss=0.401, ppl=1.32, wps=66626, ups=4, wpb=15474.399, bsz=736.876, num_updates=93772, lr=0.000147784, gnorm=0.186, clip=0.000, oom=0.000, loss_scale=1.000, wall=32845, train_wall=29341\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt/train.py\", line 315, in <module>\n",
      "    cli_main()\n",
      "  File \"/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt/train.py\", line 307, in cli_main\n",
      "    nprocs=args.distributed_world_size,\n",
      "  File \"/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 171, in spawn\n",
      "    while not spawn_context.join():\n",
      "  File \"/data/xiaowentao/.anaconda3/lib/python3.7/site-packages/torch/multiprocessing/spawn.py\", line 77, in join\n",
      "    timeout=timeout,\n",
      "  File \"/data/xiaowentao/.anaconda3/lib/python3.7/multiprocessing/connection.py\", line 920, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/data/xiaowentao/.anaconda3/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training && \\\n",
    "    chmod +x train_bert_nmt_no_bpe.sh && ./train_bert_nmt_no_bpe.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt/ && \\\n",
    "    env CUDA_VISIBLE_DEVICES='1,2' python generate.py \\\n",
    "    ../../training/processed/bin_bert_nmt_no_ds \\\n",
    "    --bert-model-name voidful/albert_chinese_tiny \\\n",
    "    --path ../../training/checkpoints/bert_nmt_0.1/checkpoint_last.pt \\\n",
    "    --beam 5 --batch-size 256 -s src_bert_nmt \\\n",
    "    -t trg_bert_nmt > ../../training/processed/dev.trg_bert_nmt.pred_no_ds_0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
      "#val:327403\n",
      "cnt_em_crnn: 210386, cnt_em_transformer: 273296\n",
      "em_crnn: 0.6426, em_bert_nmt: 0.8347\n",
      "dist_crnn: 94.0868, dist_cs2s: 94.2958\n"
     ]
    }
   ],
   "source": [
    "# beam-5, ckpt-last, dropnet 0.1\n",
    "%cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
    "\n",
    "from Levenshtein import distance\n",
    "\n",
    "with open('dev.trg_bert_nmt.pred_no_ds_0.1') as f:\n",
    "    lines = f.readlines()\n",
    "    dev_src = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if i % 4 == 0]\n",
    "    dev_trg = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if (i-1) % 4 == 0]\n",
    "    dev_pred = [''.join(l.strip().split()[2:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if (i-2) % 4 == 0]\n",
    "assert len(dev_src) == len(dev_trg) == len(dev_pred), [\n",
    "    len(dev_src), len(dev_trg), len(dev_pred)]\n",
    "cnt_em_crnn = sum([p == g for p, g in zip(dev_src, dev_trg)])\n",
    "cnt_em_bert_nmt = sum([p == g for p, g in zip(dev_pred, dev_trg)])\n",
    "dist_crnn = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_trg)]) / len(dev_src)\n",
    "dist_bert_nmt = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_pred)]) / len(dev_src)\n",
    "print(f'#val:{len(dev_src)}')\n",
    "print(f'cnt_em_crnn: {cnt_em_crnn}, cnt_em_transformer: {cnt_em_bert_nmt}')\n",
    "print('em_crnn: {:.4f}, em_bert_nmt: {:.4f}'.format(\n",
    "    cnt_em_crnn / len(dev_src), cnt_em_bert_nmt / len(dev_src)))\n",
    "print('dist_crnn: {:.4f}, dist_cs2s: {:.4f}'.format(\n",
    "    100 * (1 - dist_crnn), 100 * (1 - dist_bert_nmt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt/ && \\\n",
    "    env CUDA_VISIBLE_DEVICES='1,2' python generate.py \\\n",
    "    ../../training/processed/bin_bert_nmt_no_ds \\\n",
    "    --bert-model-name voidful/albert_chinese_tiny \\\n",
    "    --path ../../training/checkpoints/bert_nmt_0.0/checkpoint_last.pt \\\n",
    "    --beam 5 --batch-size 256 -s src_bert_nmt \\\n",
    "    -t trg_bert_nmt > ../../training/processed/dev.trg_bert_nmt.pred_no_ds_0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
      "#val:327403\n",
      "cnt_em_crnn: 210386, cnt_em_transformer: 272413\n",
      "em_crnn: 0.6426, em_bert_nmt: 0.8320\n",
      "dist_crnn: 94.0868, dist_cs2s: 94.3315\n"
     ]
    }
   ],
   "source": [
    "# beam-5, ckpt-last, dropnet 0.0\n",
    "%cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
    "\n",
    "from Levenshtein import distance\n",
    "\n",
    "with open('dev.trg_bert_nmt.pred_no_ds_0.0') as f:\n",
    "    lines = f.readlines()\n",
    "    dev_src = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if i % 4 == 0]\n",
    "    dev_trg = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if (i-1) % 4 == 0]\n",
    "    dev_pred = [''.join(l.strip().split()[2:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if (i-2) % 4 == 0]\n",
    "assert len(dev_src) == len(dev_trg) == len(dev_pred), [\n",
    "    len(dev_src), len(dev_trg), len(dev_pred)]\n",
    "cnt_em_crnn = sum([p == g for p, g in zip(dev_src, dev_trg)])\n",
    "cnt_em_bert_nmt = sum([p == g for p, g in zip(dev_pred, dev_trg)])\n",
    "dist_crnn = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_trg)]) / len(dev_src)\n",
    "dist_bert_nmt = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_pred)]) / len(dev_src)\n",
    "print(f'#val:{len(dev_src)}')\n",
    "print(f'cnt_em_crnn: {cnt_em_crnn}, cnt_em_transformer: {cnt_em_bert_nmt}')\n",
    "print('em_crnn: {:.4f}, em_bert_nmt: {:.4f}'.format(\n",
    "    cnt_em_crnn / len(dev_src), cnt_em_bert_nmt / len(dev_src)))\n",
    "print('dist_crnn: {:.4f}, dist_cs2s: {:.4f}'.format(\n",
    "    100 * (1 - dist_crnn), 100 * (1 - dist_bert_nmt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "!cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/software/bert-nmt/ && \\\n",
    "    env CUDA_VISIBLE_DEVICES='1,2' python generate.py \\\n",
    "    ../../training/processed/bin_bert_nmt_no_ds \\\n",
    "    --bert-model-name voidful/albert_chinese_tiny \\\n",
    "    --path ../../training/checkpoints/bert_nmt_0.2/checkpoint_best.pt \\\n",
    "    --beam 5 --batch-size 256 -s src_bert_nmt \\\n",
    "    -t trg_bert_nmt > ../../training/processed/dev.trg_bert_nmt.pred_no_ds_0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
      "#val:327403\n",
      "cnt_em_crnn: 210386, cnt_em_bert_nmt: 277720\n",
      "em_crnn: 0.6426, em_bert_nmt: 0.8483\n",
      "dist_crnn: 94.0868, dist_cs2s: 94.3760\n"
     ]
    }
   ],
   "source": [
    "# beam-5, ckpt-best\n",
    "%cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
    "\n",
    "from Levenshtein import distance\n",
    "\n",
    "with open('dev.trg_bert_nmt.pred_no_ds_0.2') as f:\n",
    "    lines = f.readlines()\n",
    "    dev_src = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if i % 4 == 0]\n",
    "    dev_trg = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if (i-1) % 4 == 0]\n",
    "    dev_pred = [''.join(l.strip().split()[2:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if (i-2) % 4 == 0]\n",
    "assert len(dev_src) == len(dev_trg) == len(dev_pred), [\n",
    "    len(dev_src), len(dev_trg), len(dev_pred)]\n",
    "cnt_em_crnn = sum([p == g for p, g in zip(dev_src, dev_trg)])\n",
    "cnt_em_bert_nmt = sum([p == g for p, g in zip(dev_pred, dev_trg)])\n",
    "dist_crnn = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_trg)]) / len(dev_src)\n",
    "dist_bert_nmt = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_pred)]) / len(dev_src)\n",
    "print(f'#val:{len(dev_src)}')\n",
    "print(f'cnt_em_crnn: {cnt_em_crnn}, cnt_em_bert_nmt: {cnt_em_bert_nmt}')\n",
    "print('em_crnn: {:.4f}, em_bert_nmt: {:.4f}'.format(\n",
    "    cnt_em_crnn / len(dev_src), cnt_em_bert_nmt / len(dev_src)))\n",
    "print('dist_crnn: {:.4f}, dist_cs2s: {:.4f}'.format(\n",
    "    100 * (1 - dist_crnn), 100 * (1 - dist_bert_nmt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
      "#val:327403\n",
      "cnt_em_crnn: 210386, cnt_em_transformer: 275691\n",
      "em_crnn: 0.6426, em_bert_nmt: 0.8453\n",
      "dist_crnn: 94.0868, dist_cs2s: 94.3633\n"
     ]
    }
   ],
   "source": [
    "# beam-5, ckpt-77\n",
    "%cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
    "\n",
    "from Levenshtein import distance\n",
    "\n",
    "with open('dev.trg_bert_nmt.pred_no_ds') as f:\n",
    "    lines = f.readlines()\n",
    "    dev_src = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if i % 4 == 0]\n",
    "    dev_trg = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if (i-1) % 4 == 0]\n",
    "    dev_pred = [''.join(l.strip().split()[2:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if (i-2) % 4 == 0]\n",
    "assert len(dev_src) == len(dev_trg) == len(dev_pred), [\n",
    "    len(dev_src), len(dev_trg), len(dev_pred)]\n",
    "cnt_em_crnn = sum([p == g for p, g in zip(dev_src, dev_trg)])\n",
    "cnt_em_bert_nmt = sum([p == g for p, g in zip(dev_pred, dev_trg)])\n",
    "dist_crnn = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_trg)]) / len(dev_src)\n",
    "dist_bert_nmt = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_pred)]) / len(dev_src)\n",
    "print(f'#val:{len(dev_src)}')\n",
    "print(f'cnt_em_crnn: {cnt_em_crnn}, cnt_em_transformer: {cnt_em_transformer}')\n",
    "print('em_crnn: {:.4f}, em_bert_nmt: {:.4f}'.format(\n",
    "    cnt_em_crnn / len(dev_src), cnt_em_bert_nmt / len(dev_src)))\n",
    "print('dist_crnn: {:.4f}, dist_cs2s: {:.4f}'.format(\n",
    "    100 * (1 - dist_crnn), 100 * (1 - dist_bert_nmt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
      "#val:327403\n",
      "cnt_em_crnn: 210386, cnt_em_transformer: 275691\n",
      "em_crnn: 0.6426, em_bert_nmt: 0.8439\n",
      "dist_crnn: 94.0868, dist_cs2s: 94.3954\n"
     ]
    }
   ],
   "source": [
    "# beam-1, ckpt-66\n",
    "%cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
    "\n",
    "from Levenshtein import distance\n",
    "\n",
    "with open('dev.trg_bert_nmt.pred_no_ds') as f:\n",
    "    lines = f.readlines()\n",
    "    dev_src = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if i % 4 == 0]\n",
    "    dev_trg = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if (i-1) % 4 == 0]\n",
    "    dev_pred = [''.join(l.strip().split()[2:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if (i-2) % 4 == 0]\n",
    "assert len(dev_src) == len(dev_trg) == len(dev_pred), [\n",
    "    len(dev_src), len(dev_trg), len(dev_pred)]\n",
    "cnt_em_crnn = sum([p == g for p, g in zip(dev_src, dev_trg)])\n",
    "cnt_em_bert_nmt = sum([p == g for p, g in zip(dev_pred, dev_trg)])\n",
    "dist_crnn = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_trg)]) / len(dev_src)\n",
    "dist_bert_nmt = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_pred)]) / len(dev_src)\n",
    "print(f'#val:{len(dev_src)}')\n",
    "print(f'cnt_em_crnn: {cnt_em_crnn}, cnt_em_transformer: {cnt_em_transformer}')\n",
    "print('em_crnn: {:.4f}, em_bert_nmt: {:.4f}'.format(\n",
    "    cnt_em_crnn / len(dev_src), cnt_em_bert_nmt / len(dev_src)))\n",
    "print('dist_crnn: {:.4f}, dist_cs2s: {:.4f}'.format(\n",
    "    100 * (1 - dist_crnn), 100 * (1 - dist_bert_nmt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
      "#val:327403\n",
      "cnt_em_crnn: 210386, dist_bert_nmt: 0.05636737945027608\n",
      "em_crnn: 0.6426, em_bert_nmt: 0.8453\n",
      "dist_crnn: 94.0868, dist_cs2s: 94.3633\n"
     ]
    }
   ],
   "source": [
    "# beam-5, ckpt-103\n",
    "%cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
    "\n",
    "from Levenshtein import distance\n",
    "\n",
    "with open('dev.trg_bert_nmt.pred_no_ds') as f:\n",
    "    lines = f.readlines()\n",
    "    dev_src = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if i % 4 == 0]\n",
    "    dev_trg = [''.join(l.strip().split()[1:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if (i-1) % 4 == 0]\n",
    "    dev_pred = [''.join(l.strip().split()[2:]) for i, l in enumerate(\n",
    "        lines[6:-2]) if (i-2) % 4 == 0]\n",
    "assert len(dev_src) == len(dev_trg) == len(dev_pred), [\n",
    "    len(dev_src), len(dev_trg), len(dev_pred)]\n",
    "cnt_em_crnn = sum([p == g for p, g in zip(dev_src, dev_trg)])\n",
    "cnt_em_bert_nmt = sum([p == g for p, g in zip(dev_pred, dev_trg)])\n",
    "dist_crnn = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_trg)]) / len(dev_src)\n",
    "dist_bert_nmt = sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "    dev_src, dev_pred)]) / len(dev_src)\n",
    "print(f'#val:{len(dev_src)}')\n",
    "print(f'cnt_em_crnn: {cnt_em_crnn}, dist_bert_nmt: {dist_bert_nmt}')\n",
    "print('em_crnn: {:.4f}, em_bert_nmt: {:.4f}'.format(\n",
    "    cnt_em_crnn / len(dev_src), cnt_em_bert_nmt / len(dev_src)))\n",
    "print('dist_crnn: {:.4f}, dist_cs2s: {:.4f}'.format(\n",
    "    100 * (1 - dist_crnn), 100 * (1 - dist_bert_nmt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting multiprocessing_generator\n",
      "  Downloading multiprocessing-generator-0.2.tar.gz (3.9 kB)\n",
      "Building wheels for collected packages: multiprocessing-generator\n",
      "  Building wheel for multiprocessing-generator (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for multiprocessing-generator: filename=multiprocessing_generator-0.2-py3-none-any.whl size=3968 sha256=32bf751680b2956d6bfb539055bf08e87d6ab5242181a8c44390794472168149\n",
      "  Stored in directory: /amax/data/xiaowentao/.cache/pip/wheels/f9/e0/5e/93ca1c6aff0b8eb8de2b067792ccf7c05b365c883da93fc78c\n",
      "Successfully built multiprocessing-generator\n",
      "Installing collected packages: multiprocessing-generator\n",
      "Successfully installed multiprocessing-generator-0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install --user multiprocessing_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/pycorrector\n",
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
      "dev.bert.src_bert_nmt\t     dev.trg_bert_nmt.pred_no_ds_0.0\n",
      "dev.input.txt\t\t     dev.trg_bert_nmt.pred_no_ds_0.1\n",
      "dev.src\t\t\t     dev.trg_bert_nmt.pred_no_ds_0.2\n",
      "dev.src_bert_nmt\t     dev.trg_bert_nmt.pred_pre_nmt\n",
      "dev.src_no_ds\t\t     dev.trg_bert_nmt.pred_pre_nmt_no_ds\n",
      "dev.src.old\t\t     dev.trg_no_ds\n",
      "dev.trg\t\t\t     dev.trg_no_ds.pred\n",
      "dev.trg_bert_nmt\t     dev.trg_no_ds.pred_cs2s\n",
      "dev.trg_bert_nmt.pred\t     dev.trg.old\n",
      "dev.trg_bert_nmt.pred_no_ds  dev.trg.pred\n",
      "THUCNews_2gram.arps  THUCNews_3gram.klm   THUCNews_5gram.arps\n",
      "THUCNews_2gram.klm   THUCNews_4gram.arps  THUCNews_5gram.klm\n",
      "THUCNews_3gram.arps  THUCNews_4gram.klm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING 20200507 02:27:12 corrector:  67] file not exists:None\n",
      "[  DEBUG 20200507 02:27:16 detector:  87] Loaded language model: /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed/ngram_results/THUCNews_3gram.klm, spend: 0.012 s.\n",
      "[WARNING 20200507 02:27:18 detector: 122] file not found.None\n",
      "[WARNING 20200507 02:27:18 detector: 122] file not found.None\n",
      "[  DEBUG 20200507 02:27:19 detector: 106] Loaded dict file, spend: 2.922 s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "326.2827min in total, 325.2861min remain\n",
      "2000\n",
      "318.3508min in total, 316.4061min remain\n",
      "3000\n",
      "312.1954min in total, 309.3348min remain\n",
      "4000\n",
      "306.5259min in total, 302.7810min remain\n",
      "5000\n",
      "302.1310min in total, 297.5170min remain\n",
      "6000\n",
      "299.5375min in total, 294.0482min remain\n",
      "7000\n",
      "298.6584min in total, 292.2729min remain\n",
      "8000\n",
      "293.5135min in total, 286.3416min remain\n",
      "9000\n",
      "292.8918min in total, 284.8405min remain\n",
      "10000\n",
      "289.8355min in total, 280.9830min remain\n",
      "11000\n",
      "290.3688min in total, 280.6131min remain\n",
      "12000\n",
      "291.0842min in total, 280.4154min remain\n",
      "13000\n",
      "289.4251min in total, 277.9331min remain\n",
      "14000\n",
      "289.6837min in total, 277.2966min remain\n",
      "15000\n",
      "290.1159min in total, 276.8242min remain\n",
      "16000\n",
      "288.6823min in total, 274.5746min remain\n",
      "17000\n",
      "287.5425min in total, 272.6122min remain\n",
      "18000\n",
      "287.1887min in total, 271.3996min remain\n",
      "19000\n",
      "286.9720min in total, 270.3183min remain\n",
      "20000\n",
      "287.0579min in total, 269.5224min remain\n",
      "21000\n",
      "286.4210min in total, 268.0496min remain\n",
      "22000\n",
      "286.3330min in total, 267.0927min remain\n",
      "23000\n",
      "286.4971min in total, 266.3707min remain\n",
      "24000\n",
      "286.6253min in total, 265.6145min remain\n",
      "25000\n",
      "286.4249min in total, 264.5539min remain\n",
      "26000\n",
      "286.6719min in total, 263.9065min remain\n",
      "27000\n",
      "285.9996min in total, 262.4140min remain\n",
      "28000\n",
      "286.5503min in total, 262.0441min remain\n",
      "29000\n",
      "286.0669min in total, 260.7283min remain\n",
      "30000\n",
      "285.3521min in total, 259.2052min remain\n",
      "31000\n",
      "284.8942min in total, 257.9191min remain\n",
      "32000\n",
      "283.9889min in total, 256.2322min remain\n",
      "33000\n",
      "283.8880min in total, 255.2740min remain\n",
      "34000\n",
      "283.2818min in total, 253.8637min remain\n",
      "35000\n",
      "282.7922min in total, 252.5612min remain\n",
      "36000\n",
      "282.7292min in total, 251.6414min remain\n",
      "37000\n",
      "282.4063min in total, 250.4914min remain\n",
      "38000\n",
      "282.2462min in total, 249.4873min remain\n",
      "39000\n",
      "281.6403min in total, 248.0915min remain\n",
      "40000\n",
      "280.9870min in total, 246.6578min remain\n",
      "41000\n",
      "280.6259min in total, 245.4837min remain\n",
      "42000\n",
      "280.6867min in total, 244.6796min remain\n",
      "43000\n",
      "280.4857min in total, 243.6477min remain\n",
      "44000\n",
      "280.7156min in total, 242.9899min remain\n",
      "45000\n",
      "280.8514min in total, 242.2497min remain\n",
      "46000\n",
      "281.0964min in total, 241.6025min remain\n",
      "47000\n",
      "281.5956min in total, 241.1714min remain\n",
      "48000\n",
      "281.8685min in total, 240.5442min remain\n",
      "49000\n",
      "281.7488min in total, 239.5815min remain\n",
      "50000\n",
      "281.4528min in total, 238.4702min remain\n",
      "51000\n",
      "281.2777min in total, 237.4627min remain\n",
      "52000\n",
      "281.0588min in total, 236.4194min remain\n",
      "53000\n",
      "280.9159min in total, 235.4412min remain\n",
      "54000\n",
      "280.8053min in total, 234.4909min remain\n",
      "55000\n",
      "281.1300min in total, 233.9034min remain\n",
      "56000\n",
      "281.1693min in total, 233.0773min remain\n",
      "57000\n",
      "280.7578min in total, 231.8786min remain\n",
      "58000\n",
      "280.2798min in total, 230.6277min remain\n",
      "59000\n",
      "280.4843min in total, 229.9393min remain\n",
      "60000\n",
      "280.1817min in total, 228.8355min remain\n",
      "61000\n",
      "280.5015min in total, 228.2399min remain\n",
      "62000\n",
      "279.9235min in total, 226.9147min remain\n",
      "63000\n",
      "279.5052min in total, 225.7219min remain\n",
      "64000\n",
      "279.2514min in total, 224.6640min remain\n",
      "65000\n",
      "279.1552min in total, 223.7340min remain\n",
      "66000\n",
      "278.9517min in total, 222.7188min remain\n",
      "67000\n",
      "278.5288min in total, 221.5305min remain\n",
      "68000\n",
      "278.5420min in total, 220.6902min remain\n",
      "69000\n",
      "278.4814min in total, 219.7916min remain\n",
      "70000\n",
      "278.6656min in total, 219.0858min remain\n",
      "71000\n",
      "278.4178min in total, 218.0407min remain\n",
      "72000\n",
      "278.6406min in total, 217.3641min remain\n",
      "73000\n",
      "278.3426min in total, 216.2814min remain\n",
      "74000\n",
      "278.1989min in total, 215.3200min remain\n",
      "75000\n",
      "278.3791min in total, 214.6093min remain\n",
      "76000\n",
      "278.4973min in total, 213.8498min remain\n",
      "77000\n",
      "278.4937min in total, 212.9964min remain\n",
      "78000\n",
      "278.7276min in total, 212.3240min remain\n",
      "79000\n",
      "278.9611min in total, 211.6498min remain\n",
      "80000\n",
      "279.3115min in total, 211.0625min remain\n",
      "81000\n",
      "279.3980min in total, 210.2745min remain\n",
      "82000\n",
      "279.5703min in total, 209.5503min remain\n",
      "83000\n",
      "279.7672min in total, 208.8434min remain\n",
      "84000\n",
      "279.7847min in total, 208.0019min remain\n",
      "85000\n",
      "279.6438min in total, 207.0430min remain\n",
      "86000\n",
      "279.6194min in total, 206.1709min remain\n",
      "87000\n",
      "279.4718min in total, 205.2084min remain\n",
      "88000\n",
      "279.3772min in total, 204.2857min remain\n",
      "89000\n",
      "279.3179min in total, 203.3892min remain\n",
      "90000\n",
      "279.2009min in total, 202.4512min remain\n",
      "91000\n",
      "279.3305min in total, 201.6920min remain\n",
      "92000\n",
      "279.3409min in total, 200.8463min remain\n",
      "93000\n",
      "279.1946min in total, 199.8883min remain\n",
      "94000\n",
      "279.2312min in total, 199.0617min remain\n",
      "95000\n",
      "279.3621min in total, 198.3017min remain\n",
      "96000\n",
      "279.7658min in total, 197.7338min remain\n",
      "97000\n",
      "279.8734min in total, 196.9551min remain\n",
      "98000\n",
      "280.2102min in total, 196.3362min remain\n",
      "99000\n",
      "279.9752min in total, 195.3164min remain\n",
      "100000\n",
      "280.0488min in total, 194.5123min remain\n",
      "101000\n",
      "280.1640min in total, 193.7367min remain\n",
      "102000\n",
      "280.1606min in total, 192.8786min remain\n",
      "103000\n",
      "279.8858min in total, 191.8346min remain\n",
      "104000\n",
      "279.7655min in total, 190.8976min remain\n",
      "105000\n",
      "279.5610min in total, 189.9042min remain\n",
      "106000\n",
      "279.5382min in total, 189.0349min remain\n",
      "107000\n",
      "279.4441min in total, 188.1178min remain\n",
      "108000\n",
      "279.3690min in total, 187.2139min remain\n",
      "109000\n",
      "279.5203min in total, 186.4616min remain\n",
      "110000\n",
      "279.7405min in total, 185.7540min remain\n",
      "111000\n",
      "279.7162min in total, 184.8836min remain\n",
      "112000\n",
      "279.8073min in total, 184.0891min remain\n",
      "113000\n",
      "279.8223min in total, 183.2443min remain\n",
      "114000\n",
      "279.8386min in total, 182.4003min remain\n",
      "115000\n",
      "279.9509min in total, 181.6184min remain\n",
      "116000\n",
      "280.0640min in total, 180.8363min remain\n",
      "117000\n",
      "280.1814min in total, 180.0564min remain\n",
      "118000\n",
      "280.1672min in total, 179.1916min remain\n",
      "119000\n",
      "280.2341min in total, 178.3784min remain\n",
      "120000\n",
      "280.1060min in total, 177.4414min remain\n",
      "121000\n",
      "280.2355min in total, 176.6674min remain\n",
      "122000\n",
      "280.5278min in total, 175.9949min remain\n",
      "123000\n",
      "280.4884min in total, 175.1134min remain\n",
      "124000\n",
      "280.4376min in total, 174.2252min remain\n",
      "125000\n",
      "280.6018min in total, 173.4701min remain\n",
      "126000\n",
      "280.4910min in total, 172.5449min remain\n",
      "127000\n",
      "280.2839min in total, 171.5614min remain\n",
      "128000\n",
      "280.1118min in total, 170.6006min remain\n",
      "129000\n",
      "280.0854min in total, 169.7290min remain\n",
      "130000\n",
      "280.2031min in total, 168.9445min remain\n",
      "131000\n",
      "280.1634min in total, 168.0648min remain\n",
      "132000\n",
      "280.2104min in total, 167.2372min remain\n",
      "133000\n",
      "280.3644min in total, 166.4727min remain\n",
      "134000\n",
      "280.3104min in total, 165.5845min remain\n",
      "135000\n",
      "280.4185min in total, 164.7919min remain\n",
      "136000\n",
      "280.5760min in total, 164.0275min remain\n",
      "137000\n",
      "280.9283min in total, 163.3754min remain\n",
      "138000\n",
      "281.1149min in total, 162.6253min remain\n",
      "139000\n",
      "280.9275min in total, 161.6588min remain\n",
      "140000\n",
      "280.9995min in total, 160.8420min remain\n",
      "141000\n",
      "280.9589min in total, 159.9606min remain\n",
      "142000\n",
      "280.7933min in total, 159.0087min remain\n",
      "143000\n",
      "280.6956min in total, 158.0960min remain\n",
      "144000\n",
      "280.5670min in total, 157.1666min remain\n",
      "145000\n",
      "280.5230min in total, 156.2852min remain\n",
      "146000\n",
      "280.4698min in total, 155.3989min remain\n",
      "147000\n",
      "280.4296min in total, 154.5201min remain\n",
      "148000\n",
      "280.5669min in total, 153.7388min remain\n",
      "149000\n",
      "280.5901min in total, 152.8945min remain\n",
      "150000\n",
      "280.6156min in total, 152.0513min remain\n",
      "151000\n",
      "280.5602min in total, 151.1643min remain\n",
      "152000\n",
      "280.5730min in total, 150.3143min remain\n",
      "153000\n",
      "280.6120min in total, 149.4781min remain\n",
      "154000\n",
      "280.6401min in total, 148.6359min remain\n",
      "155000\n",
      "280.6925min in total, 147.8063min remain\n",
      "156000\n",
      "280.6626min in total, 146.9333min remain\n",
      "157000\n",
      "280.5282min in total, 146.0061min remain\n",
      "158000\n",
      "280.4917min in total, 145.1304min remain\n",
      "159000\n",
      "280.3493min in total, 144.2005min remain\n",
      "160000\n",
      "280.3393min in total, 143.3391min remain\n",
      "161000\n",
      "280.3403min in total, 142.4833min remain\n",
      "162000\n",
      "280.2833min in total, 141.5983min remain\n",
      "163000\n",
      "280.2841min in total, 140.7426min remain\n",
      "164000\n",
      "280.2522min in total, 139.8706min remain\n",
      "165000\n",
      "280.2630min in total, 139.0200min remain\n",
      "166000\n",
      "280.2595min in total, 138.1622min remain\n",
      "167000\n",
      "280.2408min in total, 137.2970min remain\n",
      "168000\n",
      "280.1963min in total, 136.4194min remain\n",
      "169000\n",
      "280.2434min in total, 135.5864min remain\n",
      "170000\n",
      "280.1669min in total, 134.6937min remain\n",
      "171000\n",
      "280.2077min in total, 133.8574min remain\n",
      "172000\n",
      "280.3055min in total, 133.0480min remain\n",
      "173000\n",
      "280.1962min in total, 132.1403min remain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174000\n",
      "280.1825min in total, 131.2781min remain\n",
      "175000\n",
      "280.0814min in total, 130.3753min remain\n",
      "176000\n",
      "279.9288min in total, 129.4492min remain\n",
      "177000\n",
      "279.8532min in total, 128.5595min remain\n",
      "178000\n",
      "279.7475min in total, 127.6565min remain\n",
      "179000\n",
      "279.7198min in total, 126.7895min remain\n",
      "180000\n",
      "279.7605min in total, 125.9534min remain\n",
      "181000\n",
      "279.6733min in total, 125.0600min remain\n",
      "182000\n",
      "279.6317min in total, 124.1873min remain\n",
      "183000\n",
      "279.6729min in total, 123.3514min remain\n",
      "184000\n",
      "279.7499min in total, 122.5309min remain\n",
      "185000\n",
      "279.7018min in total, 121.6555min remain\n",
      "186000\n",
      "279.7136min in total, 120.8063min remain\n",
      "187000\n",
      "279.7839min in total, 119.9821min remain\n",
      "188000\n",
      "279.6275min in total, 119.0610min remain\n",
      "189000\n",
      "279.6804min in total, 118.2292min remain\n",
      "190000\n",
      "279.6838min in total, 117.3764min remain\n",
      "191000\n",
      "279.6852min in total, 116.5228min remain\n",
      "192000\n",
      "279.7984min in total, 115.7153min remain\n",
      "193000\n",
      "279.8651min in total, 114.8881min remain\n",
      "194000\n",
      "279.8809min in total, 114.0397min remain\n",
      "195000\n",
      "279.8553min in total, 113.1745min remain\n",
      "196000\n",
      "279.9043min in total, 112.3394min remain\n",
      "197000\n",
      "279.8422min in total, 111.4598min remain\n",
      "198000\n",
      "279.9034min in total, 110.6292min remain\n",
      "199000\n",
      "279.8965min in total, 109.7716min remain\n",
      "200000\n",
      "280.0280min in total, 108.9679min remain\n",
      "201000\n",
      "280.0442min in total, 108.1188min remain\n",
      "202000\n",
      "280.0000min in total, 107.2466min remain\n",
      "203000\n",
      "280.0443min in total, 106.4081min remain\n",
      "204000\n",
      "280.0792min in total, 105.5660min remain\n",
      "205000\n",
      "280.0898min in total, 104.7145min remain\n",
      "206000\n",
      "280.1661min in total, 103.8873min remain\n",
      "207000\n",
      "280.1318min in total, 103.0189min remain\n",
      "208000\n",
      "280.0705min in total, 102.1410min remain\n",
      "209000\n",
      "280.1217min in total, 101.3040min remain\n",
      "210000\n",
      "280.1278min in total, 100.4507min remain\n",
      "211000\n",
      "280.0927min in total, 99.5826min remain\n",
      "212000\n",
      "280.0710min in total, 98.7194min remain\n",
      "213000\n",
      "280.0439min in total, 97.8545min remain\n",
      "214000\n",
      "280.0317min in total, 96.9949min remain\n",
      "215000\n",
      "279.9242min in total, 96.1027min remain\n",
      "216000\n",
      "279.8620min in total, 95.2266min remain\n",
      "217000\n",
      "279.8826min in total, 94.3787min remain\n",
      "218000\n",
      "279.8721min in total, 93.5204min remain\n",
      "219000\n",
      "279.9084min in total, 92.6776min remain\n",
      "220000\n",
      "279.9713min in total, 91.8433min remain\n",
      "221000\n",
      "279.9150min in total, 90.9698min remain\n",
      "222000\n",
      "279.7623min in total, 90.0657min remain\n",
      "223000\n",
      "279.7462min in total, 89.2061min remain\n",
      "224000\n",
      "279.7088min in total, 88.3398min remain\n",
      "225000\n",
      "279.6021min in total, 87.4522min remain\n",
      "226000\n",
      "279.5760min in total, 86.5901min remain\n",
      "227000\n",
      "279.5727min in total, 85.7351min remain\n",
      "228000\n",
      "279.5769min in total, 84.8825min remain\n",
      "229000\n",
      "279.6424min in total, 84.0482min remain\n",
      "230000\n",
      "279.6548min in total, 83.1978min remain\n",
      "231000\n",
      "279.6795min in total, 82.3509min remain\n",
      "232000\n",
      "279.5884min in total, 81.4701min remain\n",
      "233000\n",
      "279.5816min in total, 80.6142min remain\n",
      "234000\n",
      "279.6000min in total, 79.7655min remain\n",
      "235000\n",
      "279.6792min in total, 78.9339min remain\n",
      "236000\n",
      "279.7310min in total, 78.0941min remain\n",
      "237000\n",
      "279.8237min in total, 77.2653min remain\n",
      "238000\n",
      "279.9546min in total, 76.4464min remain\n",
      "239000\n",
      "279.9479min in total, 75.5895min remain\n",
      "240000\n",
      "279.9524min in total, 74.7357min remain\n",
      "241000\n",
      "279.9090min in total, 73.8691min remain\n",
      "242000\n",
      "279.8610min in total, 73.0017min remain\n",
      "243000\n",
      "279.8789min in total, 72.1515min remain\n",
      "244000\n",
      "279.8854min in total, 71.2983min remain\n",
      "245000\n",
      "279.8303min in total, 70.4296min remain\n",
      "246000\n",
      "279.8580min in total, 69.5818min remain\n",
      "247000\n",
      "279.9432min in total, 68.7479min remain\n",
      "248000\n",
      "279.9342min in total, 67.8907min remain\n",
      "249000\n",
      "279.9899min in total, 67.0490min remain\n",
      "250000\n",
      "280.1048min in total, 66.2210min remain\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing_generator import ParallelGenerator\n",
    "from time import time\n",
    "\n",
    "%cd /data/xiaowentao/chineseocr/pycorrector\n",
    "from pycorrector import Corrector\n",
    "%cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
    "!ls dev*\n",
    "prefix = '/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed'\n",
    "!ls {prefix}/ngram_results\n",
    "\n",
    "n_lm = 2\n",
    "lm_paths = [(prefix + '/ngram_results/' + 'THUCNews_{}gram.klm'.format(\n",
    "    n_lm)) for n_lm in range(2, 6)]\n",
    "\n",
    "# confusion sets: same_stroke_path, word_freq_path, custom_word_freq_path,\n",
    "# stopwords_path\n",
    "# model2 = Corrector(language_model_path=lm_paths[0], same_pinyin_path='None',\n",
    "#                   person_name_path='None', place_name_path='None')\n",
    "model3 = Corrector(language_model_path=lm_paths[1], same_pinyin_path='None',\n",
    "                   same_stroke_path=(prefix + '/custom_same_stroke.txt'),\n",
    "                   person_name_path='None', place_name_path='None')\n",
    "# model4 = Corrector(language_model_path=lm_paths[2], same_pinyin_path='None',\n",
    "#                   person_name_path='None', place_name_path='None')\n",
    "# model5 = Corrector(language_model_path=lm_paths[3], same_pinyin_path='None',\n",
    "#                   person_name_path='None', place_name_path='None')\n",
    "with open('dev.trg_no_ds') as f, open('dev.src_no_ds') as f2:\n",
    "    dev_trg = [''.join(l.strip().split()) for l in f.readlines()]\n",
    "    dev_src = [''.join(l.strip().split()) for l in f2.readlines()]\n",
    "# dev_pyc2 = []\n",
    "dev_pyc3 = []\n",
    "# dev_pyc4 = []\n",
    "# dev_pyc5 = []\n",
    "def my_generator(model):\n",
    "    cnt = -1\n",
    "    while True:\n",
    "        cnt += 1\n",
    "        yield model.correct(dev_src[cnt])[0]\n",
    "# s_t = time()\n",
    "# with ParallelGenerator(my_generator(model2), max_lookahead=200) as g:\n",
    "#     for idx, ele in enumerate(g):\n",
    "#         dev_pyc2.append(ele)\n",
    "#         if (idx + 1) % 1000 == 0:\n",
    "#             print(idx + 1)\n",
    "#             print((time() - s_t) / (idx + 1) * len(dev_src))\n",
    "s_t = time()\n",
    "try:\n",
    "    with ParallelGenerator(my_generator(model3), max_lookahead=200) as g:\n",
    "        for idx, ele in enumerate(g):\n",
    "            dev_pyc3.append(ele)\n",
    "            if (idx + 1) % 1000 == 0:\n",
    "                print(idx + 1)\n",
    "                cnt = idx + 1\n",
    "                total_est = (time() - s_t) / cnt * 327403\n",
    "                print('{:.4f}min in total, {:.4f}min remain'.format(\n",
    "                    total_est / 60, (total_est - (time() - s_t)) / 60))\n",
    "except:\n",
    "    pass\n",
    "# s_t = time()\n",
    "# try:\n",
    "#     with ParallelGenerator(my_generator(model4), max_lookahead=200) as g:\n",
    "#         for idx, ele in enumerate(g):\n",
    "#             dev_pyc4.append(ele)\n",
    "#             if (idx + 1) % 2000 == 0:\n",
    "#                 print(idx + 1)\n",
    "#                 print((time() - s_t) / (idx + 1) * len(dev_src))\n",
    "# except:\n",
    "#     pass\n",
    "# s_t = time()\n",
    "# try:\n",
    "#     with ParallelGenerator(my_generator(model5), max_lookahead=200) as g:\n",
    "#         for idx, ele in enumerate(g):\n",
    "#                 dev_pyc5.append(ele)\n",
    "#                 if (idx + 1) % 2000 == 0:\n",
    "#                     print(idx + 1)\n",
    "#                     print((time() - s_t) / (idx + 1) * len(dev_src))\n",
    "# except:\n",
    "#     pass\n",
    "assert len(dev_trg) == len(dev_pyc2) == len(dev_src)\n",
    "print('#dev: {}, dev_acc: {:.4f}, dist_dev: {:.4f}'.format(\n",
    "    len(dev_src),\n",
    "    sum([p == g for p, g in zip(dev_src, dev_trg)]) / len(dev_src),\n",
    "    100 * (1 - sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "        dev_src, dev_trg)]) / len(dev_trg))\n",
    "))\n",
    "print('dev_pyc2: {:.4f}, dist_pyc2: {:.4f}'.format(\n",
    "    sum([p == g for p, g in zip(dev_pyc2, dev_trg)]) / len(dev_src),\n",
    "    100 * (1 - sum([distance(p, g) / max(len(p), len(g)) for p, g in zip(\n",
    "        dev_pyc2, dev_trg)]) / len(dev_trg))\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/amax/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
      "1214984\n",
      "[[['罩', '单'], ['樊', '奖'], ['！', '单']], [['·', '，']], [['勤', '动']], [], [['·', '、'], ['·', '、']], [], [['6', '5'], ['服', '胀']], [], [['·', '。']], []]\n",
      "0.0317min in total, 0.0291min remain\n",
      "0.0233min in total, 0.0195min remain\n",
      "0.0204min in total, 0.0153min remain\n",
      "0.0196min in total, 0.0132min remain\n",
      "0.0190min in total, 0.0112min remain\n",
      "0.0223min in total, 0.0113min remain\n",
      "0.0217min in total, 0.0092min remain\n",
      "0.0210min in total, 0.0072min remain\n",
      "0.0203min in total, 0.0053min remain\n",
      "0.0200min in total, 0.0035min remain\n",
      "0.0195min in total, 0.0018min remain\n",
      "0.0191min in total, 0.0002min remain\n",
      "0.0188min in total, -0.0013min remain\n",
      "0.0185min in total, -0.0028min remain\n",
      "0.0184min in total, -0.0043min remain\n",
      "0.0182min in total, -0.0058min remain\n",
      "len confusion: 5050\n",
      "max confusion size: 3283\n"
     ]
    }
   ],
   "source": [
    "%cd /data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed\n",
    "from Levenshtein import distance, editops\n",
    "from functools import reduce\n",
    "from time import time\n",
    "\n",
    "with open('dev.src_no_ds') as f1, open('dev.trg_no_ds') as f2, \\\n",
    "     open('train.src') as f3, open('train.trg') as f4:\n",
    "        dev_src = [''.join(l.strip().split()) for l in f1.readlines()]\n",
    "        dev_trg = [''.join(l.strip().split()) for l in f2.readlines()]\n",
    "        train_src = [''.join(l.strip().split()) for l in f3.readlines()]\n",
    "        train_trg = [''.join(l.strip().split()) for l in f4.readlines()]\n",
    "        \n",
    "dev_ops = [[[\n",
    "    p[o[1]], g[o[2]]] for o in editops(p, g) if o[0] == 'replace'\n",
    "] for p, g in zip(dev_src, dev_trg)]\n",
    "train_ops = [[[\n",
    "    p[o[1]], g[o[2]]] for o in editops(p, g) if o[0] == 'replace'\n",
    "] for p, g in zip(train_src, train_trg)]\n",
    "dev_ops = dev_ops + train_ops\n",
    "print(len(dev_ops), flush=True)\n",
    "print(dev_ops[:10], flush=True)\n",
    "confusion = {}\n",
    "cnt = 0\n",
    "s_t = time()\n",
    "for op in dev_ops:\n",
    "    for o in op:\n",
    "        cnt += 1\n",
    "        if cnt % 100000 == 0:\n",
    "            total_est = (time() - s_t) / cnt * len(dev_ops)\n",
    "            print('{:.4f}min in total, {:.4f}min remain'.format(\n",
    "                total_est / 60, (total_est - (time() - s_t)) / 60))\n",
    "        if o[0] not in confusion:\n",
    "            confusion[o[0]] = set([o[1]])\n",
    "        else:\n",
    "            confusion[o[0]].add(o[1])\n",
    "print(f'len confusion: {len(confusion)}')\n",
    "print(f'max confusion size: {max([len(l) for l in confusion.values()])}')\n",
    "\n",
    "prefix = '/data/xiaowentao/chineseocr/NLPCC_2018_TASK2_GEC/CS2S+BPE+Emb/training/processed'\n",
    "with open(prefix + '/custom_same_stroke.txt', 'w') as f:\n",
    "    f.writelines([(('\\t'.join([k] + list(v))) + '\\n') for k, v in confusion.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6427124980528583\n",
      "0.6246674587587774\n"
     ]
    }
   ],
   "source": [
    "len(dev_pyc2), len(dev_src)\n",
    "print(sum([p == g for p, g in zip(dev_src, dev_trg)]) / len(dev_src))\n",
    "print(sum([p == g for p, g in zip(dev_pyc2, dev_trg)]) / len(dev_src))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon May  4 20:03:36 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 430.14       Driver Version: 430.14       CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:18:00.0  On |                  N/A |\n",
      "| 33%   59C    P2   229W / 250W |   8551MiB / 11019MiB |     90%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:3B:00.0  On |                  N/A |\n",
      "| 34%   59C    P2   176W / 250W |   3799MiB / 11019MiB |     68%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:86:00.0  On |                  N/A |\n",
      "| 32%   56C    P2   165W / 250W |   3799MiB / 11019MiB |     69%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:AF:00.0  On |                  N/A |\n",
      "| 34%   59C    P2   224W / 250W |   3799MiB / 11019MiB |     68%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0    100950      C   python                                      2617MiB |\n",
      "|    0    146379      C   python                                      1329MiB |\n",
      "|    0    206562      C   python                                      2735MiB |\n",
      "|    0    231292      C   python                                      1817MiB |\n",
      "|    0    243184      G   /usr/lib/xorg/Xorg                            39MiB |\n",
      "|    1    100950      C   python                                      2551MiB |\n",
      "|    1    146379      C   python                                      1221MiB |\n",
      "|    1    243184      G   /usr/lib/xorg/Xorg                            15MiB |\n",
      "|    2    100950      C   python                                      2551MiB |\n",
      "|    2    146379      C   python                                      1221MiB |\n",
      "|    2    243184      G   /usr/lib/xorg/Xorg                            15MiB |\n",
      "|    3    100950      C   python                                      2551MiB |\n",
      "|    3    146379      C   python                                      1221MiB |\n",
      "|    3    243184      G   /usr/lib/xorg/Xorg                            15MiB |\n",
      "+-----------------------------------------------------------------------------+\n",
      "              total        used        free      shared  buff/cache   available\n",
      "Mem:           125G         95G         14G        1.8G         14G         27G\n",
      "Swap:            0B          0B          0B\n",
      " 20:03:36 up 192 days,  2:55,  9 users,  load average: 18.09, 15.40, 12.34\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi && free -h && uptime"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
