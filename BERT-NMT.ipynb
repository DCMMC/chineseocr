{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTfused(\n",
      "  (bert_encoder): AlbertForMaskedLM(\n",
      "    (albert): AlbertModel(\n",
      "      (embeddings): AlbertEmbeddings(\n",
      "        (word_embeddings): Embedding(21128, 128, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 128)\n",
      "        (token_type_embeddings): Embedding(2, 128)\n",
      "        (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (encoder): AlbertTransformer(\n",
      "        (embedding_hidden_mapping_in): Linear(in_features=128, out_features=312, bias=True)\n",
      "        (albert_layer_groups): ModuleList(\n",
      "          (0): AlbertLayerGroup(\n",
      "            (albert_layers): ModuleList(\n",
      "              (0): AlbertLayer(\n",
      "                (full_layer_layer_norm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "                (attention): AlbertAttention(\n",
      "                  (query): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (key): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (value): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (dropout): Dropout(p=0.0, inplace=False)\n",
      "                  (dense): Linear(in_features=312, out_features=312, bias=True)\n",
      "                  (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
      "                )\n",
      "                (ffn): Linear(in_features=312, out_features=1248, bias=True)\n",
      "                (ffn_output): Linear(in_features=1248, out_features=312, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (pooler): Linear(in_features=312, out_features=312, bias=True)\n",
      "      (pooler_activation): Tanh()\n",
      "    )\n",
      "    (predictions): AlbertMLMHead(\n",
      "      (LayerNorm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "      (dense): Linear(in_features=312, out_features=128, bias=True)\n",
      "      (decoder): Linear(in_features=128, out_features=21128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (encoder): BERTfusedEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): BERTfusedEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (final_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): BERTfusedEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (final_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (project_in_dim): Linear(in_features=20, out_features=32, bias=False)\n",
      "  )\n",
      "  (decoder_emb): Embedding(50, 32, padding_idx=0)\n",
      "  (decoder): BERTfusedDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0): BERTfusedDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (final_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1): BERTfusedDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (bert_attn): MultiheadAttention(\n",
      "          (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=32, out_features=64, bias=True)\n",
      "        (fc2): Linear(in_features=64, out_features=32, bias=True)\n",
      "        (final_layer_norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (embed_layer): Embedding(50, 32, padding_idx=0)\n",
      "    (embed_positions): SinusoidalPositionalEmbedding()\n",
      "  )\n",
      ")\n",
      "Trainable parameters: 135168, total parameters: 4277136\n",
      "torch.Size([3, 20, 50])\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.onnx import operators\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from numpy.random import uniform\n",
    "from transformers import BertTokenizer, AlbertForMaskedLM\n",
    "\n",
    "\n",
    "# Helper funcs\n",
    "INCREMENTAL_STATE_INSTANCE_ID = defaultdict(lambda: 0)\n",
    "\n",
    "\n",
    "def fill_with_neg_inf(t):\n",
    "    \"\"\"FP16-compatible function that fills a tensor with -inf.\"\"\"\n",
    "    return t.float().fill_(float('-inf')).type_as(t)\n",
    "\n",
    "\n",
    "def _get_full_incremental_state_key(module_instance, key):\n",
    "    module_name = module_instance.__class__.__name__\n",
    "\n",
    "    # assign a unique ID to each module instance, so that incremental state is\n",
    "    # not shared across module instances\n",
    "    if not hasattr(module_instance, '_fairseq_instance_id'):\n",
    "        INCREMENTAL_STATE_INSTANCE_ID[module_name] += 1\n",
    "        module_instance._fairseq_instance_id = INCREMENTAL_STATE_INSTANCE_ID[module_name]\n",
    "\n",
    "    return '{}.{}.{}'.format(module_name, module_instance._fairseq_instance_id, key)\n",
    "\n",
    "\n",
    "def softmax(x, dim, onnx_trace=False):\n",
    "    if onnx_trace:\n",
    "        return F.softmax(x.float(), dim=dim)\n",
    "    else:\n",
    "        # noinspection PyTypeChecker\n",
    "        return F.softmax(x, dim=dim, dtype=torch.float32)\n",
    "\n",
    "\n",
    "def get_incremental_state(module, incremental_state, key):\n",
    "    \"\"\"Helper for getting incremental state for an nn.Module.\"\"\"\n",
    "    full_key = _get_full_incremental_state_key(module, key)\n",
    "    if incremental_state is None or full_key not in incremental_state:\n",
    "        return None\n",
    "    return incremental_state[full_key]\n",
    "\n",
    "\n",
    "def set_incremental_state(module, incremental_state, key, value):\n",
    "    \"\"\"Helper for setting incremental state for an nn.Module.\"\"\"\n",
    "    if incremental_state is not None:\n",
    "        full_key = _get_full_incremental_state_key(module, key)\n",
    "        incremental_state[full_key] = value\n",
    "\n",
    "\n",
    "def make_positions(tensor, padding_idx, onnx_trace=False):\n",
    "    \"\"\"Replace non-padding symbols with their position numbers.\n",
    "\n",
    "    Position numbers begin at padding_idx+1. Padding symbols are ignored.\n",
    "    \"\"\"\n",
    "    mask = tensor.ne(padding_idx).long()\n",
    "    return torch.cumsum(mask, dim=1) * mask + padding_idx\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention.\n",
    "\n",
    "    See \"Attention Is All You Need\" for more details.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, kdim=None, vdim=None, dropout=0., bias=True,\n",
    "                 add_bias_kv=False, add_zero_attn=False, self_attention=False,\n",
    "                 encoder_decoder_attention=False):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.kdim = kdim if kdim is not None else embed_dim\n",
    "        self.vdim = vdim if vdim is not None else embed_dim\n",
    "        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "        self.scaling = self.head_dim ** -0.5\n",
    "\n",
    "        self.self_attention = self_attention\n",
    "        self.encoder_decoder_attention = encoder_decoder_attention\n",
    "\n",
    "        assert not self.self_attention or self.qkv_same_dim, 'Self-attention requires query, key and '                                                              'value to be of the same size'\n",
    "\n",
    "\n",
    "        if self.qkv_same_dim:\n",
    "            self.in_proj_weight = nn.Parameter(torch.Tensor(3 * embed_dim, embed_dim))\n",
    "        else:\n",
    "            self.k_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.kdim))\n",
    "            self.v_proj_weight = nn.Parameter(torch.Tensor(embed_dim, self.vdim))\n",
    "            self.q_proj_weight = nn.Parameter(torch.Tensor(embed_dim, embed_dim))\n",
    "\n",
    "        if bias:\n",
    "            self.in_proj_bias = nn.Parameter(torch.Tensor(3 * embed_dim))\n",
    "        else:\n",
    "            self.register_parameter('in_proj_bias', None)\n",
    "\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
    "\n",
    "        if add_bias_kv:\n",
    "            self.bias_k = nn.Parameter(torch.Tensor(1, 1, embed_dim))\n",
    "            self.bias_v = nn.Parameter(torch.Tensor(1, 1, embed_dim))\n",
    "        else:\n",
    "            self.bias_k = self.bias_v = None\n",
    "\n",
    "        self.add_zero_attn = add_zero_attn\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "        self.onnx_trace = False\n",
    "\n",
    "    def prepare_for_onnx_export_(self):\n",
    "        self.onnx_trace = True\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.qkv_same_dim:\n",
    "            nn.init.xavier_uniform_(self.in_proj_weight)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.k_proj_weight)\n",
    "            nn.init.xavier_uniform_(self.v_proj_weight)\n",
    "            nn.init.xavier_uniform_(self.q_proj_weight)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
    "        if self.in_proj_bias is not None:\n",
    "            nn.init.constant_(self.in_proj_bias, 0.)\n",
    "            nn.init.constant_(self.out_proj.bias, 0.)\n",
    "        if self.bias_k is not None:\n",
    "            nn.init.xavier_normal_(self.bias_k)\n",
    "        if self.bias_v is not None:\n",
    "            nn.init.xavier_normal_(self.bias_v)\n",
    "\n",
    "    def forward(self, query, key, value, key_padding_mask=None, incremental_state=None,\n",
    "                need_weights=True, static_kv=False, attn_mask=None):\n",
    "        \"\"\"Input shape: Time x Batch x Channel\n",
    "\n",
    "        Timesteps can be masked by supplying a T x T mask in the\n",
    "        `attn_mask` argument. Padding elements can be excluded from\n",
    "        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n",
    "        batch x src_len, where padding elements are indicated by 1s.\n",
    "        \"\"\"\n",
    "\n",
    "        tgt_len, bsz, embed_dim = query.size()\n",
    "        assert embed_dim == self.embed_dim\n",
    "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
    "\n",
    "        if incremental_state is not None:\n",
    "            saved_state = self._get_input_buffer(incremental_state)\n",
    "            if 'prev_key' in saved_state:\n",
    "                # previous time steps are cached - no need to recompute\n",
    "                # key and value if they are static\n",
    "                if static_kv:\n",
    "                    assert self.encoder_decoder_attention and not self.self_attention\n",
    "                    key = value = None\n",
    "        else:\n",
    "            saved_state = None\n",
    "\n",
    "        if self.self_attention:\n",
    "            # self-attention\n",
    "            q, k, v = self.in_proj_qkv(query)\n",
    "        elif self.encoder_decoder_attention:\n",
    "            # encoder-decoder attention\n",
    "            q = self.in_proj_q(query)\n",
    "            if key is None:\n",
    "                assert value is None\n",
    "                k = v = None\n",
    "            else:\n",
    "                k = self.in_proj_k(key)\n",
    "                v = self.in_proj_v(key)\n",
    "\n",
    "        else:\n",
    "            q = self.in_proj_q(query)\n",
    "            k = self.in_proj_k(key)\n",
    "            v = self.in_proj_v(value)\n",
    "        q *= self.scaling\n",
    "\n",
    "        if self.bias_k is not None:\n",
    "            assert self.bias_v is not None\n",
    "            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n",
    "            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = torch.cat(\n",
    "                    [key_padding_mask, key_padding_mask.new_zeros(key_padding_mask.size(0), 1)],\n",
    "                    dim=1)\n",
    "\n",
    "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        if k is not None:\n",
    "            k = k.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "        if v is not None:\n",
    "            v = v.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
    "\n",
    "        if saved_state is not None:\n",
    "            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
    "            if 'prev_key' in saved_state:\n",
    "                prev_key = saved_state['prev_key'].view(bsz * self.num_heads, -1, self.head_dim)\n",
    "                if static_kv:\n",
    "                    k = prev_key\n",
    "                else:\n",
    "                    k = torch.cat((prev_key, k), dim=1)\n",
    "            if 'prev_value' in saved_state:\n",
    "                prev_value = saved_state['prev_value'].view(bsz * self.num_heads, -1, self.head_dim)\n",
    "                if static_kv:\n",
    "                    v = prev_value\n",
    "                else:\n",
    "                    v = torch.cat((prev_value, v), dim=1)\n",
    "            saved_state['prev_key'] = k.view(bsz, self.num_heads, -1, self.head_dim)\n",
    "            saved_state['prev_value'] = v.view(bsz, self.num_heads, -1, self.head_dim)\n",
    "\n",
    "            self._set_input_buffer(incremental_state, saved_state)\n",
    "\n",
    "        src_len = k.size(1)\n",
    "\n",
    "        # This is part of a workaround to get around fork/join parallelism\n",
    "        # not supporting Optional types.\n",
    "        if key_padding_mask is not None and key_padding_mask.shape == torch.Size([]):\n",
    "            key_padding_mask = None\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            assert key_padding_mask.size(0) == bsz\n",
    "            assert key_padding_mask.size(1) == src_len\n",
    "\n",
    "        if self.add_zero_attn:\n",
    "            src_len += 1\n",
    "            k = torch.cat([k, k.new_zeros((k.size(0), 1) + k.size()[2:])], dim=1)\n",
    "            v = torch.cat([v, v.new_zeros((v.size(0), 1) + v.size()[2:])], dim=1)\n",
    "            if attn_mask is not None:\n",
    "                attn_mask = torch.cat([attn_mask, attn_mask.new_zeros(attn_mask.size(0), 1)], dim=1)\n",
    "            if key_padding_mask is not None:\n",
    "                key_padding_mask = torch.cat(\n",
    "                    [key_padding_mask, torch.zeros(key_padding_mask.size(0), 1).type_as(key_padding_mask)],\n",
    "                    dim=1)\n",
    "\n",
    "        attn_weights = torch.bmm(q, k.transpose(1, 2))\n",
    "        assert list(attn_weights.size()) == [bsz * self.num_heads, tgt_len, src_len]\n",
    "\n",
    "        if attn_mask is not None:\n",
    "            attn_mask = attn_mask.unsqueeze(0)\n",
    "            if self.onnx_trace:\n",
    "                attn_mask = attn_mask.repeat(attn_weights.size(0), 1, 1)\n",
    "            attn_weights += attn_mask\n",
    "\n",
    "        if key_padding_mask is not None:\n",
    "            # don't attend to padding symbols\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            if self.onnx_trace:\n",
    "                attn_weights = torch.where(\n",
    "                    key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                    torch.Tensor([float(\"-Inf\")]),\n",
    "                    attn_weights.float()\n",
    "                ).type_as(attn_weights)\n",
    "            else:\n",
    "                attn_weights = attn_weights.masked_fill(\n",
    "                    key_padding_mask.unsqueeze(1).unsqueeze(2),\n",
    "                    float('-inf'),\n",
    "                )\n",
    "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
    "\n",
    "        attn_weights = softmax(\n",
    "            attn_weights, dim=-1, onnx_trace=self.onnx_trace,\n",
    "        ).type_as(attn_weights)\n",
    "        attn_weights = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "\n",
    "        attn = torch.bmm(attn_weights, v)\n",
    "        assert list(attn.size()) == [bsz * self.num_heads, tgt_len, self.head_dim]\n",
    "        if (self.onnx_trace and attn.size(1) == 1):\n",
    "            # when ONNX tracing a single decoder step (sequence length == 1)\n",
    "            # the transpose is a no-op copy before view, thus unnecessary\n",
    "            attn = attn.contiguous().view(tgt_len, bsz, embed_dim)\n",
    "        else:\n",
    "            attn = attn.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n",
    "        attn = self.out_proj(attn)\n",
    "\n",
    "        if need_weights:\n",
    "            # average attention weights over heads\n",
    "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
    "            attn_weights = attn_weights.sum(dim=1) / self.num_heads\n",
    "        else:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn, attn_weights\n",
    "\n",
    "    def in_proj_qkv(self, query):\n",
    "        return self._in_proj(query).chunk(3, dim=-1)\n",
    "\n",
    "    def in_proj_q(self, query):\n",
    "        if self.qkv_same_dim:\n",
    "            return self._in_proj(query, end=self.embed_dim)\n",
    "        else:\n",
    "            bias = self.in_proj_bias\n",
    "            if bias is not None:\n",
    "                bias = bias[:self.embed_dim]\n",
    "            return F.linear(query, self.q_proj_weight, bias)\n",
    "\n",
    "    def in_proj_k(self, key):\n",
    "        if self.qkv_same_dim:\n",
    "            return self._in_proj(key, start=self.embed_dim, end=2 * self.embed_dim)\n",
    "        else:\n",
    "            weight = self.k_proj_weight\n",
    "            bias = self.in_proj_bias\n",
    "            if bias is not None:\n",
    "                bias = bias[self.embed_dim:2 * self.embed_dim]\n",
    "            return F.linear(key, weight, bias)\n",
    "\n",
    "    def in_proj_v(self, value):\n",
    "        if self.qkv_same_dim:\n",
    "            return self._in_proj(value, start=2 * self.embed_dim)\n",
    "        else:\n",
    "            weight = self.v_proj_weight\n",
    "            bias = self.in_proj_bias\n",
    "            if bias is not None:\n",
    "                bias = bias[2 * self.embed_dim:]\n",
    "            return F.linear(value, weight, bias)\n",
    "\n",
    "    def _in_proj(self, input, start=0, end=None):\n",
    "        weight = self.in_proj_weight\n",
    "        bias = self.in_proj_bias\n",
    "        weight = weight[start:end, :]\n",
    "        if bias is not None:\n",
    "            bias = bias[start:end]\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "    def reorder_incremental_state(self, incremental_state, new_order):\n",
    "        \"\"\"Reorder buffered internal state (for incremental generation).\"\"\"\n",
    "        input_buffer = self._get_input_buffer(incremental_state)\n",
    "        if input_buffer is not None:\n",
    "            for k in input_buffer.keys():\n",
    "                input_buffer[k] = input_buffer[k].index_select(0, new_order)\n",
    "            self._set_input_buffer(incremental_state, input_buffer)\n",
    "\n",
    "    def _get_input_buffer(self, incremental_state):\n",
    "        return get_incremental_state(\n",
    "            self,\n",
    "            incremental_state,\n",
    "            'attn_state',\n",
    "        ) or {}\n",
    "\n",
    "    def _set_input_buffer(self, incremental_state, buffer):\n",
    "        set_incremental_state(\n",
    "            self,\n",
    "            incremental_state,\n",
    "            'attn_state',\n",
    "            buffer,\n",
    "        )\n",
    "\n",
    "\n",
    "def LayerNorm(normalized_shape, eps=1e-5, elementwise_affine=True, export=False):\n",
    "    if not export and torch.cuda.is_available():\n",
    "        try:\n",
    "            from apex.normalization import FusedLayerNorm\n",
    "            return FusedLayerNorm(normalized_shape, eps, elementwise_affine)\n",
    "        except ImportError:\n",
    "            pass\n",
    "    return torch.nn.LayerNorm(normalized_shape, eps, elementwise_affine)\n",
    "\n",
    "\n",
    "def Linear(in_features, out_features, bias=True):\n",
    "    m = nn.Linear(in_features, out_features, bias)\n",
    "    nn.init.xavier_uniform_(m.weight)\n",
    "    if bias:\n",
    "        nn.init.constant_(m.bias, 0.)\n",
    "    return m\n",
    "\n",
    "\n",
    "# Modules\n",
    "class LearnedPositionalEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    This module learns positional embeddings up to a fixed maximum size.\n",
    "    Padding ids are ignored by either offsetting based on padding_idx\n",
    "    or by setting padding_idx to None and ensuring that the appropriate\n",
    "    position ids are passed to the forward function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings: int,\n",
    "            embedding_dim: int,\n",
    "            padding_idx: int,\n",
    "    ):\n",
    "        super().__init__(num_embeddings, embedding_dim, padding_idx)\n",
    "        self.onnx_trace = False\n",
    "\n",
    "    def forward(self, input, incremental_state=None, positions=None):\n",
    "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
    "        assert (\n",
    "            (positions is None) or (self.padding_idx is None)\n",
    "        ), \"If positions is pre-computed then padding_idx should not be set.\"\n",
    "\n",
    "        if positions is None:\n",
    "            if incremental_state is not None:\n",
    "                # positions is the same for every token when decoding a single step\n",
    "                positions = input.data.new(1, 1).fill_(self.padding_idx + input.size(1))\n",
    "            else:\n",
    "                positions = make_positions(\n",
    "                    input.data, self.padding_idx, onnx_trace=self.onnx_trace,\n",
    "                )\n",
    "        return super().forward(positions)\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum number of supported positions.\"\"\"\n",
    "        if self.padding_idx is not None:\n",
    "            return self.num_embeddings - self.padding_idx - 1\n",
    "        else:\n",
    "            return self.num_embeddings\n",
    "\n",
    "\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    \"\"\"This module produces sinusoidal positional embeddings of any length.\n",
    "\n",
    "    Padding symbols are ignored.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim, padding_idx, init_size=1024):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.padding_idx = padding_idx\n",
    "        self.weights = SinusoidalPositionalEmbedding.get_embedding(\n",
    "            init_size,\n",
    "            embedding_dim,\n",
    "            padding_idx,\n",
    "        )\n",
    "        self.onnx_trace = False\n",
    "        self.register_buffer('_float_tensor', torch.FloatTensor(1))\n",
    "\n",
    "    def prepare_for_onnx_export_(self):\n",
    "        self.onnx_trace = True\n",
    "\n",
    "    @staticmethod\n",
    "    def get_embedding(num_embeddings, embedding_dim, padding_idx=None):\n",
    "        \"\"\"Build sinusoidal embeddings.\n",
    "\n",
    "        This matches the implementation in tensor2tensor, but differs slightly\n",
    "        from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "        \"\"\"\n",
    "        half_dim = embedding_dim // 2\n",
    "        emb = math.log(10000) / (half_dim - 1)\n",
    "        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)\n",
    "        emb = torch.arange(num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)\n",
    "        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).view(num_embeddings, -1)\n",
    "        if embedding_dim % 2 == 1:\n",
    "            # zero pad\n",
    "            emb = torch.cat([emb, torch.zeros(num_embeddings, 1)], dim=1)\n",
    "        if padding_idx is not None:\n",
    "            emb[padding_idx, :] = 0\n",
    "        return emb\n",
    "\n",
    "    def forward(self, input, incremental_state=None, timestep=None, **kwargs):\n",
    "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
    "        bsz, seq_len = torch.onnx.operators.shape_as_tensor(input)\n",
    "        max_pos = self.padding_idx + 1 + seq_len\n",
    "        if self.weights is None or max_pos > self.weights.size(0):\n",
    "            # recompute/expand embeddings if needed\n",
    "            self.weights = SinusoidalPositionalEmbedding.get_embedding(\n",
    "                max_pos,\n",
    "                self.embedding_dim,\n",
    "                self.padding_idx,\n",
    "            )\n",
    "        self.weights = self.weights.to(self._float_tensor)\n",
    "\n",
    "        if incremental_state is not None:\n",
    "            # positions is the same for every token when decoding a single step\n",
    "            pos = (timestep.int() + 1).long() if timestep is not None else seq_len\n",
    "            if self.onnx_trace:\n",
    "                return self.weights[self.padding_idx + pos, :].unsqueeze(1).repeat(bsz, 1, 1)\n",
    "            return self.weights[self.padding_idx + pos, :].expand(bsz, 1, -1)\n",
    "\n",
    "        positions = make_positions(input, self.padding_idx, onnx_trace=self.onnx_trace)\n",
    "        if self.onnx_trace:\n",
    "            flat_embeddings = self.weights.detach().index_select(0, positions.view(-1))\n",
    "            embedding_shape = torch.cat((bsz.view(1), seq_len.view(1), torch.LongTensor([-1])))\n",
    "            embeddings = torch.onnx.operators.reshape_from_tensor_shape(flat_embeddings, embedding_shape)\n",
    "            return embeddings\n",
    "        return self.weights.index_select(0, positions.view(-1)).view(bsz, seq_len, -1).detach()\n",
    "\n",
    "    def max_positions(self):\n",
    "        \"\"\"Maximum number of supported positions.\"\"\"\n",
    "        return int(1e5)  # an arbitrary large number\n",
    "\n",
    "\n",
    "def PositionalEmbedding(\n",
    "        num_embeddings: int,\n",
    "        embedding_dim: int,\n",
    "        padding_idx: int,\n",
    "        learned: bool = False,\n",
    "):\n",
    "    if learned:\n",
    "        # if padding_idx is specified then offset the embedding ids by\n",
    "        # this index and adjust num_embeddings appropriately\n",
    "        # TODO: The right place for this offset would be inside\n",
    "        # LearnedPositionalEmbedding. Move this there for a cleaner implementation.\n",
    "        if padding_idx is not None:\n",
    "            num_embeddings = num_embeddings + padding_idx + 1\n",
    "        m = LearnedPositionalEmbedding(num_embeddings, embedding_dim, padding_idx)\n",
    "        nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n",
    "        if padding_idx is not None:\n",
    "            nn.init.constant_(m.weight[padding_idx], 0)\n",
    "    else:\n",
    "        m = SinusoidalPositionalEmbedding(\n",
    "            embedding_dim, padding_idx, init_size=num_embeddings + padding_idx + 1,\n",
    "        )\n",
    "    return m\n",
    "\n",
    "\n",
    "# Layers\n",
    "class BERTfusedEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, encoder_ffn_embed_dim,\n",
    "                 attention_dropout, dropout, bert_out_dim,\n",
    "                 encoder_attention_heads,\n",
    "                 encoder_ratio=0.5, bert_ratio=0.5,\n",
    "                 bert_gate=True,\n",
    "                 normalize_before=False,\n",
    "                 bert_dropnet=False,\n",
    "                 bert_dropnet_rate=0.25,\n",
    "                 bert_mixup=False,\n",
    "                 activation_dropout=0.,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        (bert_ratio, encoder_ratio) and dropnet are alternative\n",
    "        \"\"\"\n",
    "        super(BERTfusedEncoderLayer, self).__init__()\n",
    "        self.activation_fn = F.relu\n",
    "        self.dropout = dropout\n",
    "        self.activation_dropout = activation_dropout\n",
    "        self.normalize_before = normalize_before\n",
    "        self.embed_dim = embed_dim\n",
    "        self.self_attn = MultiheadAttention(\n",
    "            embed_dim, encoder_attention_heads,\n",
    "            dropout=attention_dropout, self_attention=True)\n",
    "        self.bert_attn = MultiheadAttention(\n",
    "            embed_dim=embed_dim, num_heads=encoder_attention_heads,\n",
    "            kdim=bert_out_dim, vdim=bert_out_dim,\n",
    "            dropout=attention_dropout,\n",
    "        )\n",
    "        self.self_attn_layer_norm = LayerNorm(embed_dim)\n",
    "        self.fc1 = Linear(embed_dim, encoder_ffn_embed_dim)\n",
    "        self.fc2 = Linear(encoder_ffn_embed_dim, embed_dim)\n",
    "        self.final_layer_norm = LayerNorm(embed_dim)\n",
    "        # bert-fused\n",
    "        self.encoder_ratio = encoder_ratio\n",
    "        self.bert_ratio = bert_ratio\n",
    "        self.bert_dropnet = bert_dropnet\n",
    "        self.bert_dropnet_rate = bert_dropnet_rate\n",
    "        assert 0. <= self.bert_dropnet_rate <= 0.5\n",
    "        self.bert_mixup = bert_mixup\n",
    "        if not bert_gate:\n",
    "            self.bert_ratio = 0.\n",
    "            self.bert_dropnet = False\n",
    "            self.bert_mixup = False\n",
    "\n",
    "    def upgrade_state_dict_named(self, state_dict, name):\n",
    "        \"\"\"\n",
    "        Rename layer norm states from `...layer_norms.0.weight` to\n",
    "        `...self_attn_layer_norm.weight` and `...layer_norms.1.weight` to\n",
    "        `...final_layer_norm.weight`\n",
    "        \"\"\"\n",
    "        layer_norm_map = {\n",
    "            '0': 'self_attn_layer_norm',\n",
    "            '1': 'final_layer_norm'\n",
    "        }\n",
    "        for old, new in layer_norm_map.items():\n",
    "            for m in ('weight', 'bias'):\n",
    "                k = '{}.layer_norms.{}.{}'.format(name, old, m)\n",
    "                if k in state_dict:\n",
    "                    state_dict[\n",
    "                        '{}.{}.{}'.format(name, new, m)\n",
    "                    ] = state_dict[k]\n",
    "                    del state_dict[k]\n",
    "\n",
    "    def forward(self, x, encoder_padding_mask, bert_encoder_out,\n",
    "                bert_encoder_padding_mask):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n",
    "                `(batch, src_len)` where padding elements are indicated by ``1``.\n",
    "\n",
    "        Returns:\n",
    "            encoded output of shape `(batch, src_len, embed_dim)`\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.maybe_layer_norm(self.self_attn_layer_norm, x, before=True)\n",
    "        x1, _ = self.self_attn(\n",
    "            query=x, key=x, value=x, key_padding_mask=encoder_padding_mask)\n",
    "        x2, _ = self.bert_attn(\n",
    "            query=x, key=bert_encoder_out, value=bert_encoder_out,\n",
    "            key_padding_mask=bert_encoder_padding_mask)\n",
    "        x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
    "        x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
    "        # DCMMC: drop-net trick\n",
    "        ratios = self.get_ratio()\n",
    "        x = residual + ratios[0] * x1 + ratios[1] * x2\n",
    "        x = self.maybe_layer_norm(self.self_attn_layer_norm, x, after=True)\n",
    "\n",
    "        residual = x\n",
    "        x = self.maybe_layer_norm(self.final_layer_norm, x, before=True)\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = residual + x\n",
    "        x = self.maybe_layer_norm(self.final_layer_norm, x, after=True)\n",
    "        return x\n",
    "\n",
    "    def get_ratio(self):\n",
    "        if self.bert_dropnet:\n",
    "            frand = float(uniform(0, 1))\n",
    "            if self.bert_mixup and self.training:\n",
    "                return [frand, 1 - frand]\n",
    "            # dropnet trick\n",
    "            if frand < self.bert_dropnet_rate and self.training:\n",
    "                return [1, 0]\n",
    "            elif frand > 1 - self.bert_dropnet_rate and self.training:\n",
    "                return [0, 1]\n",
    "            else:\n",
    "                return [0.5, 0.5]\n",
    "        else:\n",
    "            return [self.encoder_ratio, self.bert_ratio]\n",
    "\n",
    "    def maybe_layer_norm(self, layer_norm, x, before=False, after=False):\n",
    "        assert before ^ after\n",
    "        if after ^ self.normalize_before:\n",
    "            return layer_norm(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "\n",
    "class BERTfusedEncoder(nn.Module):\n",
    "    def __init__(self, dropout, encoder_layer, embed_dim,\n",
    "                 input_dim,\n",
    "                 bert_out_dim, encoder_ffn_embed_dim,\n",
    "                 encoder_attention_heads, attention_dropout,\n",
    "                 encoder_normalize_before=False,\n",
    "                 bert_dropnet=False,\n",
    "                 bert_dropnet_rate=0.25,\n",
    "                 bert_mixup=False,\n",
    "                 **kwargs):\n",
    "        super(BERTfusedEncoder, self).__init__()\n",
    "        # what if add position embed?\n",
    "        # self.embed_positions\n",
    "        self.dropout = dropout\n",
    "        self.bert_gates = [1, ] * encoder_layer\n",
    "        self.layers = nn.ModuleList([])\n",
    "        self.layers.extend([\n",
    "            BERTfusedEncoderLayer(\n",
    "                embed_dim=embed_dim,\n",
    "                encoder_ffn_embed_dim=encoder_ffn_embed_dim,\n",
    "                attention_dropout=attention_dropout,\n",
    "                dropout=dropout,\n",
    "                bert_out_dim=bert_out_dim,\n",
    "                encoder_attention_heads=encoder_attention_heads,\n",
    "                bert_gate=self.bert_gates[i],\n",
    "                normalize_before=encoder_normalize_before,\n",
    "                bert_dropnet=bert_dropnet,\n",
    "                bert_dropnet_rate=bert_dropnet_rate,\n",
    "                bert_mixup=bert_mixup,\n",
    "            )\n",
    "            for i in range(encoder_layer)\n",
    "        ])\n",
    "        self.project_in_dim = Linear(input_dim, embed_dim, bias=False) \\\n",
    "            if embed_dim != input_dim else None\n",
    "        if encoder_normalize_before:\n",
    "            self.layer_norm = LayerNorm(embed_dim)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "\n",
    "    def forward(self, source, src_lengths, encoder_padding_mask,\n",
    "                bert_encoder_out):\n",
    "        if self.project_in_dim is not None:\n",
    "            source = self.project_in_dim(source)\n",
    "        x = F.dropout(source, p=self.dropout, training=self.training)\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "        # encoder_padding_mask from CRNN\n",
    "        for layer in self.layers:\n",
    "            x = layer(\n",
    "                x, encoder_padding_mask,\n",
    "                bert_encoder_out['bert_encoder_out'],\n",
    "                bert_encoder_out['bert_encoder_padding_mask'])\n",
    "        if self.layer_norm:\n",
    "            x = self.layer_norm(x)\n",
    "        return {\n",
    "            # T x B x C\n",
    "            'encoder_out': x,\n",
    "            # B x T\n",
    "            'encoder_padding_mask': encoder_padding_mask\n",
    "        }\n",
    "\n",
    "\n",
    "class BERTfusedDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, decoder_ffn_embed_dim,\n",
    "                 attention_dropout, dropout, bert_out_dim,\n",
    "                 decoder_attention_heads,\n",
    "                 encoder_ratio=0.5, bert_ratio=0.5,\n",
    "                 normalize_before=False,\n",
    "                 bert_dropnet=False,\n",
    "                 bert_dropnet_rate=0.25,\n",
    "                 bert_mixup=False,\n",
    "                 no_encoder_attn=False, add_bias_kv=False,\n",
    "                 add_zero_attn=False, bert_gate=True,\n",
    "                 char_inputs=False,\n",
    "                 activation_dropout=0.,\n",
    "                ):\n",
    "        super(BERTfusedDecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiheadAttention(\n",
    "            embed_dim=embed_dim,\n",
    "            num_heads=decoder_attention_heads,\n",
    "            dropout=attention_dropout,\n",
    "            add_bias_kv=add_bias_kv,\n",
    "            add_zero_attn=add_zero_attn,\n",
    "            self_attention=True\n",
    "        )\n",
    "        self.dropout = dropout\n",
    "        self.activation_dropout = activation_dropout\n",
    "        self.activation_fn = F.relu\n",
    "        self.normalize_before = normalize_before\n",
    "        self.embed_dim = embed_dim\n",
    "        # dont know whats this\n",
    "        export = char_inputs\n",
    "        self.self_attn_layer_norm = LayerNorm(embed_dim, export=export)\n",
    "        if no_encoder_attn:\n",
    "            self.encoder_attn = None\n",
    "            self.encoder_attn_layer_norm = None\n",
    "        else:\n",
    "            self.encoder_attn = MultiheadAttention(\n",
    "                embed_dim, decoder_attention_heads,\n",
    "                dropout=attention_dropout, encoder_decoder_attention=True\n",
    "            )\n",
    "            self.bert_attn = MultiheadAttention(\n",
    "                self.embed_dim, decoder_attention_heads,\n",
    "                kdim=bert_out_dim, vdim=bert_out_dim,\n",
    "                dropout=attention_dropout, encoder_decoder_attention=True\n",
    "            )\n",
    "            self.encoder_attn_layer_norm = LayerNorm(embed_dim, export=export)\n",
    "        self.fc1 = Linear(self.embed_dim, decoder_ffn_embed_dim)\n",
    "        self.fc2 = Linear(decoder_ffn_embed_dim, self.embed_dim)\n",
    "        self.final_layer_norm = LayerNorm(self.embed_dim, export=export)\n",
    "        self.need_attn = True\n",
    "        self.onnx_trace = False\n",
    "        self.encoder_ratio = encoder_ratio\n",
    "        self.bert_ratio = bert_ratio\n",
    "        self.bert_dropnet = bert_dropnet\n",
    "        self.bert_dropnet_rate = bert_dropnet_rate\n",
    "        assert 0 <= self.bert_dropnet_rate <= 0.5\n",
    "        self.bert_mixup = bert_mixup\n",
    "        if not bert_gate:\n",
    "            self.bert_ratio = 0.\n",
    "            self.bert_dropnet = False\n",
    "            self.bert_mixup = False\n",
    "\n",
    "    def prepare_for_onnx_export_(self):\n",
    "        self.onnx_trace = True\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x,\n",
    "        encoder_out=None,\n",
    "        encoder_padding_mask=None,\n",
    "        bert_encoder_out=None,\n",
    "        bert_encoder_padding_mask=None,\n",
    "        incremental_state=None,\n",
    "        prev_self_attn_state=None,\n",
    "        prev_attn_state=None,\n",
    "        self_attn_mask=None,\n",
    "        self_attn_padding_mask=None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
    "            encoder_padding_mask (ByteTensor): binary ByteTensor of shape\n",
    "                `(batch, src_len)` where padding elements are indicated by ``True``.\n",
    "\n",
    "        Returns:\n",
    "            encoded output of shape `(batch, src_len, embed_dim)`\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        x = self.maybe_layer_norm(self.self_attn_layer_norm, x, before=True)\n",
    "        if prev_self_attn_state is not None:\n",
    "            if incremental_state is None:\n",
    "                incremental_state = {}\n",
    "            prev_key, prev_value = prev_self_attn_state\n",
    "            saved_state = {\"prev_key\": prev_key, \"prev_value\": prev_value}\n",
    "            self.self_attn._set_input_buffer(incremental_state, saved_state)\n",
    "        x, attn = self.self_attn(\n",
    "            query=x,\n",
    "            key=x,\n",
    "            value=x,\n",
    "            key_padding_mask=self_attn_padding_mask,\n",
    "            incremental_state=incremental_state,\n",
    "            need_weights=False,\n",
    "            attn_mask=self_attn_mask,\n",
    "        )\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = residual + x\n",
    "        x = self.maybe_layer_norm(self.self_attn_layer_norm, x, after=True)\n",
    "\n",
    "        if self.encoder_attn is not None:\n",
    "            residual = x\n",
    "            x = self.maybe_layer_norm(self.encoder_attn_layer_norm, x, before=True)\n",
    "            if prev_attn_state is not None:\n",
    "                if incremental_state is None:\n",
    "                    incremental_state = {}\n",
    "                prev_key, prev_value = prev_attn_state\n",
    "                saved_state = {\"prev_key\": prev_key, \"prev_value\": prev_value}\n",
    "                self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n",
    "            x1, attn = self.encoder_attn(\n",
    "                query=x,\n",
    "                key=encoder_out,\n",
    "                value=encoder_out,\n",
    "                key_padding_mask=encoder_padding_mask,\n",
    "                incremental_state=incremental_state,\n",
    "                static_kv=True,\n",
    "                need_weights=(not self.training and self.need_attn),\n",
    "            )\n",
    "            x2, _ = self.bert_attn(\n",
    "                query=x,\n",
    "                key=bert_encoder_out,\n",
    "                value=bert_encoder_out,\n",
    "                key_padding_mask=bert_encoder_padding_mask,\n",
    "                incremental_state=incremental_state,\n",
    "                static_kv=True,\n",
    "                need_weights=(not self.training and self.need_attn),\n",
    "            )\n",
    "            x1 = F.dropout(x1, p=self.dropout, training=self.training)\n",
    "            x2 = F.dropout(x2, p=self.dropout, training=self.training)\n",
    "            ratios = self.get_ratio()\n",
    "            x = residual + ratios[0] * x1 + ratios[1] * x2\n",
    "            x = self.maybe_layer_norm(self.encoder_attn_layer_norm, x, after=True)\n",
    "\n",
    "        residual = x\n",
    "        x = self.maybe_layer_norm(self.final_layer_norm, x, before=True)\n",
    "        x = self.activation_fn(self.fc1(x))\n",
    "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = residual + x\n",
    "        x = self.maybe_layer_norm(self.final_layer_norm, x, after=True)\n",
    "        if self.onnx_trace and incremental_state is not None:\n",
    "            saved_state = self.self_attn._get_input_buffer(incremental_state)\n",
    "            self_attn_state = saved_state[\"prev_key\"], saved_state[\"prev_value\"]\n",
    "            return x, attn, self_attn_state\n",
    "        return x, attn\n",
    "\n",
    "    def get_ratio(self):\n",
    "        if self.bert_dropnet:\n",
    "            frand = float(uniform(0, 1))\n",
    "            if self.bert_mixup and self.training:\n",
    "                return [frand, 1 - frand]\n",
    "            if frand < self.bert_dropnet_rate and self.training:\n",
    "                return [1, 0]\n",
    "            elif frand > 1 - self.bert_dropnet_rate and self.training:\n",
    "                return [0, 1]\n",
    "            else:\n",
    "                return [0.5, 0.5]\n",
    "        else:\n",
    "            return [self.encoder_ratio, self.bert_ratio]\n",
    "\n",
    "    def maybe_layer_norm(self, layer_norm, x, before=False, after=False):\n",
    "        assert before ^ after\n",
    "        if after ^ self.normalize_before:\n",
    "            return layer_norm(x)\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def make_generation_fast_(self, need_attn=False, **kwargs):\n",
    "        self.need_attn = need_attn\n",
    "\n",
    "\n",
    "class BERTfusedDecoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 num_tgt_alphabet,\n",
    "                 dropout, decoder_layer, embed_dim,\n",
    "                 bert_out_dim, decoder_ffn_embed_dim,\n",
    "                 decoder_attention_heads, attention_dropout,\n",
    "                 embed_layer,\n",
    "                 normalize_before=False,\n",
    "                 bert_dropnet=False,\n",
    "                 bert_dropnet_rate=0.25,\n",
    "                 bert_mixup=False,\n",
    "                 max_target_positions=128,\n",
    "                 no_token_positional_embeddings=False,\n",
    "                 decoder_learned_pos=False,\n",
    "                 decoder_no_bert=False,\n",
    "                 no_encoder_attn=False):\n",
    "        super(BERTfusedDecoder, self).__init__()\n",
    "        bert_gates = [1, ] * decoder_layer\n",
    "        self.layers = nn.ModuleList([])\n",
    "        # TODO\n",
    "        # if decoder_no_bert:\n",
    "        self.layers.extend([\n",
    "            BERTfusedDecoderLayer(\n",
    "                embed_dim=embed_dim,\n",
    "                decoder_ffn_embed_dim=decoder_ffn_embed_dim,\n",
    "                attention_dropout=attention_dropout,\n",
    "                dropout=dropout,\n",
    "                bert_out_dim=bert_out_dim,\n",
    "                decoder_attention_heads=decoder_attention_heads,\n",
    "                normalize_before=normalize_before,\n",
    "                bert_dropnet=bert_dropnet,\n",
    "                bert_dropnet_rate=bert_dropnet_rate,\n",
    "                bert_mixup=bert_mixup,\n",
    "                bert_gate=bert_gates[i])\n",
    "            for i in range(decoder_layer)\n",
    "        ])\n",
    "        self.dropout = dropout\n",
    "        self.adaptive_softmax = None\n",
    "        self.embed_layer = embed_layer\n",
    "        self.project_in_dim = Linear(embed_layer.embedding_dim, embed_dim, bias=False) \\\n",
    "            if embed_dim != embed_layer.embedding_dim else None\n",
    "        self.embed_scale = math.sqrt(embed_dim)\n",
    "        out_embed_dim = self.embed_layer.embedding_dim\n",
    "        padding_idx = self.embed_layer.padding_idx\n",
    "        self.embed_positions = PositionalEmbedding(\n",
    "            max_target_positions, embed_dim, padding_idx,\n",
    "            learned=decoder_learned_pos,\n",
    "        ) if not no_token_positional_embeddings else None\n",
    "        self.project_out_dim = Linear(embed_dim, out_embed_dim, bias=False) \\\n",
    "            if embed_dim != out_embed_dim else None\n",
    "        self.embed_out = nn.Parameter(torch.Tensor(num_tgt_alphabet, out_embed_dim))\n",
    "        nn.init.normal_(self.embed_out, mean=0, std=out_embed_dim ** -0.5)\n",
    "        if normalize_before:\n",
    "            self.layer_norm = LayerNorm(embed_dim)\n",
    "        else:\n",
    "            self.layer_norm = None\n",
    "\n",
    "    def forward(self, prev_output_tokens, encoder_out=None, bert_encoder_out=None,\n",
    "                incremental_state=None, **unused):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prev_output_tokens (LongTensor): previous decoder outputs of shape\n",
    "                `(batch, tgt_len)`, for input feeding/teacher forcing\n",
    "            encoder_out (Tensor, optional): output from the encoder, used for\n",
    "                encoder-side attention\n",
    "            incremental_state (dict): dictionary used for storing state during\n",
    "                :ref:`Incremental decoding`\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - the decoder's output of shape `(batch, tgt_len, vocab)`\n",
    "                - a dictionary with any model-specific outputs\n",
    "        \"\"\"\n",
    "        x, extra = self.extract_features(prev_output_tokens, encoder_out, bert_encoder_out,\n",
    "                                         incremental_state)\n",
    "        x = self.output_layer(x)\n",
    "        return x, extra\n",
    "\n",
    "    def extract_features(self, prev_output_tokens, encoder_out=None, bert_encoder_out=None,\n",
    "                         incremental_state=None, **unused):\n",
    "        \"\"\"\n",
    "        Similar to *forward* but only return features.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - the decoder's features of shape `(batch, tgt_len, embed_dim)`\n",
    "                - a dictionary with any model-specific outputs\n",
    "        \"\"\"\n",
    "        # embed positions\n",
    "        positions = self.embed_positions(\n",
    "            prev_output_tokens,\n",
    "            incremental_state=incremental_state,\n",
    "        ) if self.embed_positions is not None else None\n",
    "        if incremental_state is not None:\n",
    "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
    "            if positions is not None:\n",
    "                positions = positions[:, -1:]\n",
    "        # embed tokens and positions\n",
    "        x = self.embed_scale * self.embed_layer(prev_output_tokens)\n",
    "        if self.project_in_dim is not None:\n",
    "            x = self.project_in_dim(x)\n",
    "        if positions is not None:\n",
    "            x += positions\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        # B x T x C -> T x B x C\n",
    "        x = x.transpose(0, 1)\n",
    "        attn = None\n",
    "        inner_states = [x]\n",
    "        # decoder layers\n",
    "        for layer in self.layers:\n",
    "            x, attn = layer(\n",
    "                x,\n",
    "                encoder_out['encoder_out'] if encoder_out is not None else None,\n",
    "                encoder_out['encoder_padding_mask'] if encoder_out is not None else None,\n",
    "                bert_encoder_out['bert_encoder_out'],\n",
    "                bert_encoder_out['bert_encoder_padding_mask'],\n",
    "                incremental_state,\n",
    "                self_attn_mask=self.buffered_future_mask(x) if incremental_state is None else None,\n",
    "            )\n",
    "            inner_states.append(x)\n",
    "        if self.layer_norm:\n",
    "            x = self.layer_norm(x)\n",
    "        # T x B x C -> B x T x C\n",
    "        x = x.transpose(0, 1)\n",
    "        if self.project_out_dim is not None:\n",
    "            x = self.project_out_dim(x)\n",
    "        return x, {'attn': attn, 'inner_states': inner_states}\n",
    "\n",
    "    def buffered_future_mask(self, tensor):\n",
    "        dim = tensor.size(0)\n",
    "        if not hasattr(self, '_future_mask') or self._future_mask is None or \\\n",
    "                self._future_mask.device != tensor.device:\n",
    "            self._future_mask = torch.triu(fill_with_neg_inf(tensor.new(dim, dim)), 1)\n",
    "        if self._future_mask.size(0) < dim:\n",
    "            self._future_mask = torch.triu(fill_with_neg_inf(self._future_mask.resize_(\n",
    "                dim, dim)), 1)\n",
    "        return self._future_mask[:dim, :dim]\n",
    "\n",
    "    def output_layer(self, features, **kwargs):\n",
    "        \"\"\"Project features to the vocabulary size.\"\"\"\n",
    "        return F.linear(features, self.embed_out)\n",
    "\n",
    "\n",
    "class BERTfused(nn.Module):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        bert_model_name (str): default is 4-layer ALBERT\n",
    "    \"\"\"\n",
    "    def __init__(self, num_tgt_alphabet,\n",
    "                 input_dim,\n",
    "                 bert_model_name='voidful/albert_chinese_tiny',\n",
    "                 bert_output_layer=-1,\n",
    "                 bert_dropnet_rate=0.5,\n",
    "                 bert_dropnet=True,\n",
    "                 bert_mixup=False,\n",
    "                 dropout=0.3,\n",
    "                 attention_dropout=0.,\n",
    "                 normalize_before=False,\n",
    "                 decoder_no_bert=False,\n",
    "                 no_encoder_attn=False,\n",
    "                 encoder_layer=6, decoder_layer=6,\n",
    "                 encoder_embed_dim=512, encoder_ffn_embed_dim=1024,\n",
    "                 encoder_attention_heads=4, decoder_attention_heads=4,\n",
    "                 decoder_embed_dim=512, decoder_ffn_embed_dim=1024,\n",
    "                 **kwargs):\n",
    "        super(BERTfused, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.bert_encoder = AlbertForMaskedLM.from_pretrained(\n",
    "            bert_model_name,\n",
    "            output_hidden_states=True,\n",
    "            output_attentions=True)\n",
    "        for param in self.bert_encoder.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.bert_out_dim = self.bert_encoder.config.hidden_size\n",
    "        self.bert_output_layer = bert_output_layer\n",
    "        self.encoder = BERTfusedEncoder(\n",
    "            input_dim=input_dim,\n",
    "            dropout=self.dropout,\n",
    "            encoder_layer=encoder_layer,\n",
    "            embed_dim=encoder_embed_dim,\n",
    "            bert_out_dim=self.bert_out_dim,\n",
    "            encoder_ffn_embed_dim=encoder_ffn_embed_dim,\n",
    "            attention_dropout=attention_dropout,\n",
    "            encoder_attention_heads=encoder_attention_heads,\n",
    "            encoder_normalize_before=normalize_before,\n",
    "            bert_dropnet=bert_dropnet,\n",
    "            bert_dropnet_rate=bert_dropnet_rate,\n",
    "            bert_mixup=bert_mixup,\n",
    "        )\n",
    "        self.decoder_emb = self.build_embedding(num_tgt_alphabet, decoder_embed_dim)\n",
    "        self.decoder = BERTfusedDecoder(\n",
    "            num_tgt_alphabet=num_tgt_alphabet,\n",
    "            embed_layer=self.decoder_emb,\n",
    "            embed_dim=decoder_embed_dim,\n",
    "            decoder_layer=decoder_layer,\n",
    "            dropout=dropout,\n",
    "            bert_out_dim=self.bert_out_dim,\n",
    "            decoder_ffn_embed_dim=decoder_ffn_embed_dim,\n",
    "            attention_dropout=attention_dropout,\n",
    "            decoder_attention_heads=decoder_attention_heads,\n",
    "            normalize_before=normalize_before,\n",
    "            bert_dropnet=bert_dropnet,\n",
    "            bert_dropnet_rate=bert_dropnet_rate,\n",
    "            bert_mixup=bert_mixup,\n",
    "            decoder_no_bert=decoder_no_bert,\n",
    "            no_encoder_attn=no_encoder_attn,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def build_embedding(num_emb, embed_dim, padding_idx=0):\n",
    "        emb = nn.Embedding(num_emb, embed_dim, padding_idx=padding_idx)\n",
    "        nn.init.normal_(emb.weight, mean=0, std=embed_dim ** -0.5)\n",
    "        nn.init.constant_(emb.weight[padding_idx], 0)\n",
    "        return emb\n",
    "\n",
    "    def forward(self, source, src_lengths, prev_output_tokens, bert_input,\n",
    "                encoder_padding_mask,\n",
    "                **kwargs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source (LongTensor): tokens in the source language of shape\n",
    "                `(batch, src_len, hidden_size)`\n",
    "            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n",
    "            prev_output_tokens (LongTensor): previous decoder outputs of shape\n",
    "                `(batch, tgt_len)`, for input feeding/teacher forcing\n",
    "            bert_input (list of str): output string of CRNN\n",
    "        \"\"\"\n",
    "        bert_input = self.bert_tokenizer.batch_encode_plus(bert_input)\n",
    "        # gpu context?\n",
    "        # In huggingface transformers, padding mask is 0 instead of 1\n",
    "        bert_encoder_padding_mask = (torch.tensor(bert_input['attention_mask']) == 0)\n",
    "        bert_encoder_out =  self.bert_encoder(\n",
    "            torch.tensor(bert_input['input_ids']),\n",
    "            attention_mask=bert_encoder_padding_mask)[-2]\n",
    "        bert_encoder_out = bert_encoder_out[self.bert_output_layer]\n",
    "        bert_encoder_out = {\n",
    "            # => (T, B, C)\n",
    "            'bert_encoder_out': bert_encoder_out.permute(1,0,2).contiguous(),\n",
    "            'bert_encoder_padding_mask': bert_encoder_padding_mask\n",
    "        }\n",
    "        encoder_out = self.encoder(\n",
    "            source, src_lengths=src_lengths,\n",
    "            encoder_padding_mask=encoder_padding_mask,\n",
    "            bert_encoder_out=bert_encoder_out)\n",
    "        decoder_out = self.decoder(\n",
    "            prev_output_tokens, encoder_out=encoder_out,\n",
    "            bert_encoder_out=bert_encoder_out, **kwargs)\n",
    "        return decoder_out\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = BERTfused(num_tgt_alphabet=50, input_dim=20,\n",
    "                      encoder_layer=2, decoder_layer=2,\n",
    "                      encoder_embed_dim=32, encoder_ffn_embed_dim=64,\n",
    "                      encoder_attention_heads=2, decoder_attention_heads=2,\n",
    "                      decoder_embed_dim=32, decoder_ffn_embed_dim=64\n",
    "                      )\n",
    "    print(model)\n",
    "    print('Trainable parameters: {}, total parameters: {}'.format(\n",
    "        sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "        sum(p.numel() for p in model.parameters())\n",
    "    ))\n",
    "    # (batch_size, seq_length, hidden_size)\n",
    "    source = torch.rand(3, 20, 20)\n",
    "    src_lengths = torch.Tensor([18, 17, 16])\n",
    "    encoder_padding_mask = (torch.zeros([3, 20]) == 1)\n",
    "    prev_output_tokens = torch.randint(1, 48, size=(3, 20), dtype=torch.long)\n",
    "    bert_input = ['1', '22', '333']\n",
    "    y_hat, _ = model(source, src_lengths, prev_output_tokens, bert_input, encoder_padding_mask)\n",
    "    print(y_hat.shape)\n",
    "    assert list(y_hat.shape) == [3, 20, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 3844, 6407, 1079, 2159, 102, 0, 0], [101, 1369, 1912, 671, 702, 3844, 6407, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "AlbertConfig {\n",
      "  \"_num_labels\": 2,\n",
      "  \"architectures\": null,\n",
      "  \"attention_probs_dropout_prob\": 0.0,\n",
      "  \"bos_token_id\": 2,\n",
      "  \"classifier_dropout_prob\": 0.1,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"down_scale_factor\": 1,\n",
      "  \"early_stopping\": false,\n",
      "  \"embedding_size\": 128,\n",
      "  \"eos_token_id\": 3,\n",
      "  \"finetuning_task\": null,\n",
      "  \"gap_size\": 0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.0,\n",
      "  \"hidden_size\": 312,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"inner_group_num\": 1,\n",
      "  \"intermediate_size\": 1248,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"albert\",\n",
      "  \"net_structure_type\": 0,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_hidden_groups\": 1,\n",
      "  \"num_hidden_layers\": 4,\n",
      "  \"num_memory_blocks\": 0,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": true,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 21128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, AlbertForMaskedLM\n",
    "bert_model_name = 'voidful/albert_chinese_tiny'\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "print(bert_tokenizer.batch_encode_plus(['', ''],\n",
    "                        pad_to_max_length=True,\n",
    "                        add_special_tokens=True))\n",
    "model = AlbertForMaskedLM.from_pretrained(bert_model_name, force_download=False,\n",
    "                                  output_hidden_states=True,\n",
    "                                  output_attentions=True)\n",
    "print(model.config)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  1 20:46:55 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 430.14       Driver Version: 430.14       CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 208...  Off  | 00000000:18:00.0  On |                  N/A |\r\n",
      "| 28%   51C    P2   192W / 250W |   3183MiB / 11019MiB |     99%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   1  GeForce RTX 208...  Off  | 00000000:3B:00.0  On |                  N/A |\r\n",
      "| 26%   48C    P2   109W / 250W |   2911MiB / 11019MiB |     53%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   2  GeForce RTX 208...  Off  | 00000000:86:00.0  On |                  N/A |\r\n",
      "| 22%   29C    P8    21W / 250W |     27MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "|   3  GeForce RTX 208...  Off  | 00000000:AF:00.0  On |                  N/A |\r\n",
      "| 22%   26C    P8    16W / 250W |     27MiB / 11019MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0     25702      C   python                                      1315MiB |\r\n",
      "|    0    231292      C   python                                      1817MiB |\r\n",
      "|    0    243184      G   /usr/lib/xorg/Xorg                            39MiB |\r\n",
      "|    1    243184      G   /usr/lib/xorg/Xorg                            15MiB |\r\n",
      "|    1    251941      C   python                                      1199MiB |\r\n",
      "|    1    254774      C   python                                      1685MiB |\r\n",
      "|    2    243184      G   /usr/lib/xorg/Xorg                            15MiB |\r\n",
      "|    3    243184      G   /usr/lib/xorg/Xorg                            15MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
